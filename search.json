[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics for Sense-Making",
    "section": "",
    "text": "Preface\nIND5003 is a foundation course for students in a Master’s program for working adults, at the National University of Singapore. The title of the course is\n\n\nData Analytics for Sense-Making\n\n\nThe content of the course aims to bring students up to speed in a variety of analytic techniques that will be useful for them in later courses of the program. You can read some reviews of the course on NUSmods.\nIn designing this course, we tried to cover the techniques from a couple of different angles:\n\nFrom the viewpoint of the types of data encountered, e.g. time series, unstructured text data, image data, and so on.\nFrom the viewpoint of the types of questions that could be asked of the data, e.g. are we attempting to plan for unseen scenarios (simulation), are we interrogating the data for hidden structure (unsupervised learning), and so on.\n\nThe entire course uses the Python programming language. By the end of the course, the aim was for students to become familiar with Python for data analysis.\nIf you are a fellow instructor and you find something useful in this textbook, please do let me know at vik.gopal@nus.edu.sg. If you need more details about anything, do feel free to write as well.\nSo long, and thanks for reading!\nVik\nhttps://blog.nus.edu.sg/stavg",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction to Python",
    "section": "",
    "text": "1.1 Introduction\nPython is a general-purpose programming language. It is a higher-level language than C, C++ and Java in the sense that a Python program does not have to be compiled before execution.\nIt was originally conceived back in the 1980s by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands. The language is named after a BBC TV show (Guido’s favorite program) “Monty Python’s Flying Circus”.\nPython reached version 1.0 in January 1994. Python 2.0 was released on October 16, 2000. Python 3.0, which is backwards-incompatible with earlier versions, was released on 3 December 2008.\nPython is a very flexible language; it is simple to learn yet is fast enough to be used in production. Over the past ten years, more and more comprehensive data science toolkits (e.g. scikit-learn, NTLK, tensorflow, keras) have been written in Python and are now the standard frameworks for those models.\nPython is an open-source software. It is free to use and extend.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#installing-python-and-jupyter-lab",
    "href": "01-intro.html#installing-python-and-jupyter-lab",
    "title": "1  Introduction to Python",
    "section": "1.2 Installing Python and Jupyter Lab",
    "text": "1.2 Installing Python and Jupyter Lab\nTo install Python, navigate to the official Python download page to obtain the appropriate installer for your operating system.\n\n\n\n\n\n\nImportant\n\n\n\nFor our class, please ensure that you are using Python 3.10.12.\n\n\nThe next step is to create a virtual environment for this course. Virtual environments are specific to Python. They allow you to retain multiple versions of Python, and of packages, on the same computer. Go through the videos on Canvas relevant to your operating system to create a virtual environment and install Jupyter Lab on your machine.\nJupyter notebooks are great for interactive work with Python, but more advanced users may prefer a full-fledged IDE. If you are an advanced user, and are comfortable with an IDE of your own choice (e.g. Spyder or VSCode), feel free to continue using that to run the codes for this course.\n\n\n\n\n\n\nImportant\n\n\n\nEven if you are using Anaconda/Spyder/VSCode, you still need to create a virtual environment.\n\n\nJupyter notebooks consist of cells, which can be of three main types:\n\ncode cells,\noutput cells, and\nmarkdown cells.\n\n\n\n\n\n\n\nFigure 1.1: Jupyter Lab\n\n\n\nIn Figure 1.1, the red box labelled 1 is a markdown cell. It can be used to contain descriptions or summary of the code. The cells in the box labelled 2 are code cells. To run the codes from our notes, you can copy and paste the codes into a new cell, and then execute them with Ctrl-Enter.\nTry out this Easter egg that comes with any Python installation:\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nMore information on using Jupyter notebooks can be obtained from this link.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#basic-data-structures-in-python",
    "href": "01-intro.html#basic-data-structures-in-python",
    "title": "1  Introduction to Python",
    "section": "1.3 Basic Data Structures in Python",
    "text": "1.3 Basic Data Structures in Python\nThe main objects in native1 Python that contain data are\n\nLists, which are defined with [ ]. Lists are mutable.\nTuples, which are defined with ( ). Tuples are immutable.\nDictionaries, which are defined with { }. Dictionaries have keys and items. They are also mutable.\n\nVery soon, we shall see that for data analysis, the more common objects we shall deal with are dataframes (from pandas) and arrays (from numpy). However, the latter two require add-on packages; the three object classes listed above are baked into Python.\nBy the way, this is what mean by (im)mutable:\n\nx = [1, 3, 5, 7, 8, 9, 10]\n\n# The following is OK, because \"x\" is a list, and hence mutable\nx[3] = 17     \nprint(x)  \n\n[1, 3, 5, 17, 8, 9, 10]\n\n\n\n# The following will return an error, because x_tuple is a tuple, and hence \n# immutable.\nx_tuple = (1, 3, 5, 6, 8, 9, 10)\nx_tuple[3] = 17 \n\nHere is how we create lists, tuples and dictionaries.\n\nx_list = [1, 2, 3]\nx_tuple = (1, 2, 3)\nx_dict = {'a': 1, 'b': 2, 'c': 3} # access with x_dict['a']",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#slice-operator-in-python",
    "href": "01-intro.html#slice-operator-in-python",
    "title": "1  Introduction to Python",
    "section": "1.4 Slice Operator in Python",
    "text": "1.4 Slice Operator in Python\nOne important point to take note is that, Python begins indexing of objects starting with 0. Second, indexing is aided by the slicing operator ‘:’. It is used in Python to extract regular sequences from a list, tuple or string easily.\nIn general, the syntax is &lt;list-like object&gt;[a:b], where a and b are integers. Such a call would return the elements at indices a, a+1 until b-1. Take note that the end point index is not included.\n\nchar_list = ['P', 'y', 't', 'h', 'o', 'n']\nchar_list[0]           # returns first element\nchar_list[-1]          # returns last element\nlen(char_list)         # returns number of elements in list-like object.\nchar_list[::2]         # from first to last, every 2 apart.\n\n['P', 't', 'o']\n\n\nThis indexing syntax is used in the additional packages we use as well, so it is good to know about it. Figure 1.2 displays a pictorial representation of how positive and negative indexes work together.\n\n\n\n\n\n\nFigure 1.2: Positive and negative indices",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#loops-in-python",
    "href": "01-intro.html#loops-in-python",
    "title": "1  Introduction to Python",
    "section": "1.5 Loops in Python",
    "text": "1.5 Loops in Python\nIt is extremely efficient to execute “for” loops in Python. Many objects in Python are iterators, which means they can be iterated over. Lists, tuples and dictionaries can all be iterated over very easily.\nBefore getting down to examples, take note that Python does not use curly braces to denote code blocks. Instead, these are defined by the number of indentations in a line.\n\nfor i in x[:2]:\n  print(f\"The current element is {i}.\")\n\nThe current element is 1.\nThe current element is 3.\n\n\nNotice how we do not need to set up any running index; the object is just iterated over directly. The argument to the print() function is an f-string. It is the recommended way to create string literals that can vary according to arguments.\nHere is another example of iteration, this time using dictionaries which have key-value pairs. In this case, we iterate over the keys.\n\ndict1 = {'holmes': 'male', 'watson': 'male', 'mycroft': 'male', \n         'hudson': 'female', 'moriarty': 'male', 'adler': 'female'}\n# dict1['hudson']\n\nfor x in dict1.keys():\n    print(f\"The gender of {x} is {dict1[x]}\")\n\nThe gender of holmes is male\nThe gender of watson is male\nThe gender of mycroft is male\nThe gender of hudson is female\nThe gender of moriarty is male\nThe gender of adler is female",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#strings",
    "href": "01-intro.html#strings",
    "title": "1  Introduction to Python",
    "section": "1.6 Strings",
    "text": "1.6 Strings\nText data in Python is handled with str objects. A string object is an immutable sequence of characters. Here are some useful string methods and properties:\n\ncatenating (joining) strings.\nfinding sub-strings.\niterating over strings.\nconverting to lower/upper-case.\n\nA few cells earlier, we saw how we can format a string before sending it to the print() function. Here’s another such example. This is useful when we are debugging loops.\n\nfor x in range(1, 11):\n    print(f\"Sq:{x*x:3d} Cu:{x*x*x:4d}.\")\n\nSq:  1 Cu:   1.\nSq:  4 Cu:   8.\nSq:  9 Cu:  27.\nSq: 16 Cu:  64.\nSq: 25 Cu: 125.\nSq: 36 Cu: 216.\nSq: 49 Cu: 343.\nSq: 64 Cu: 512.\nSq: 81 Cu: 729.\nSq:100 Cu:1000.\n\n\nHere are more examples of working with strings.\n\ntest_str = \"Where in the World is Carmen san Diego?\"\n\nRemember that a string is immutable, just like a tuple, so this assignment will not work:\ntest_str[5] = 'z'\nBut like a tuple, we can also iterate over a string.\n\ncount = 0\nfor x in test_str:\n    if x.isupper():\n        count += 1\nprint(f\"There were {count} upper-case characters in the sentence.\")\n\nThere were 4 upper-case characters in the sentence.\n\n\n\ntest_str.lower()\n\n'where in the world is carmen san diego?'\n\n\nTo join strings, we can use the ‘+’ operator, the str.join() method, or, if they are part of the same expression, we can just place them next to each other separated by whitespace.\n\nx = \"Where shall \"\ny = \"we \"\nprint(\"Where shall \" \"we \" \"go today?\")\n\n# also works - the '+' operator is overloaded to work with strings:\n# \"Where \" + \"shall\" +\" we go\" + \" today?\"\n\n# join using another character:\n# ':'.join([\"Where\", \"shall\", \"we\", \"go\", \"today?\"])\n\nWhere shall we go today?\n\n\nTo find simple patterns, we may turn to the find, replace, startswith and endswith methods.\n\ntest_str.find('Carmen')\n\n22\n\n\n\ntest_str.replace('Carmen', 'John')\n\n'Where in the World is John san Diego?'\n\n\nFor more complicated search operations over strings, we use a special mini-language known as regular expressions. These are used in several other languages such as R and Perl, so it is worth knowing about them if you have time. See Section 1.11 for a good introduction.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#functions-modules-and-packages",
    "href": "01-intro.html#functions-modules-and-packages",
    "title": "1  Introduction to Python",
    "section": "1.7 Functions, Modules and Packages",
    "text": "1.7 Functions, Modules and Packages\nFunctions provide a way to package code that you can re-use several times. To define a function in Python, use the def keyword.\n\ndef test_function(x):\n    print('You typed', x)\n    \ntest_function('test')\n\nYou typed test\n\n\nA Python module is a file containing Python definitions (of functions and constants) and statements. Instead of re-typing functions every time, we can simply load the module. We would then have access to the updated functions. We access objects within the module using the “dot” notation. There are several modules that ship with the default Python installation. Note that Python packages are collections of modules.\nHere are a couple of ways of importing (and then using) constants from the math module.\n\nimport math\n\n# compute e^2\nmath.exp(2)\n\n# print pi\nmath.pi\n\n3.141592653589793\n\n\nAlternatively, we could import the constant \\(\\pi\\) so that we do not need to use the dot notation.\n\nfrom math import pi\npi\n\n3.141592653589793",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#object-oriented-programming",
    "href": "01-intro.html#object-oriented-programming",
    "title": "1  Introduction to Python",
    "section": "1.8 Object-Oriented Programming",
    "text": "1.8 Object-Oriented Programming\nPython has been developed as both a functional and object-oriented programming language. Much of the code we will soon use involves creation of an instance, and then accessing the attributes (data or methods) of that instance.\nConsider the following class Circle.\n\nclass Circle:\n    \"\"\" A simple class definition \n    \n    c0 = Circle()\n    c0.radius\n    \n    \"\"\"\n    def __init__(self, radius = 1.0):\n        self.radius = radius\n        \n    def area(self):\n        \"\"\" Compute area of circle\"\"\"\n        return pi*(self.radius**2)\n\nHaving defined the class, we can instantiate it and use the methods it contains.\n\nc1 = Circle(3.2)\nc2 = Circle(4.0)\nc2.area()\n\n50.26548245743669\n\n\nIn this simple class, we can also set the value of the attribute of an instance, although this is not the recommended way.\n\nc1.radius = 6\nc1.area()\n\n113.09733552923255",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#numpy",
    "href": "01-intro.html#numpy",
    "title": "1  Introduction to Python",
    "section": "1.9 Numpy",
    "text": "1.9 Numpy\n\nimport numpy as np\nimport pandas as pd\nfrom itables import show\n\nThe basic object in this package is the ndarray object, which can represent n-dimensional arrays of homogeneous data types. This is the key difference between NumPy and Pandas objects, which we shall encounter later on in this chapter. While Pandas objects are also tabular in nature, they allow you to deal with inhomogenous objects. Specifically, Pandas’ DataFrames allow columns to be of different types.\nAn ndarray object is an n-dimensional array (i.e., a tensor) of elements, indexed by a tuple of non-negative integers.\nThe dimensions of the array are referred to as axes in NumPy: a three-dimensional array will have three axes.\nEach array has several attributes. These include:\n\nndim: the number of axes/dimensions.\nshape: a tuple describing the length of each dimension.\nsize: the total number of elements in the array. This is a product of the integers in the shape attribute.\n\n\narr = np.array([(1.5, 2, 3), (4, 5, 6)])\n\n# number of axes\narr.ndim\n\n2\n\n\n\n# the length of each axis\narr.shape\n\n(2, 3)\n\n\n\n# number of elements in the array\narr.size\n\n6\n\n\n\nArray Creation\nOne way to create an array containing regular sequences is to use the np.arange() function. This creates a sequence of integers, with a specified separation.\n\nseq = np.arange(0, 10, 3)\nseq\n\narray([0, 3, 6, 9])\n\n\nThe shape of an ndarray is given by a tuple. Note that an array of shape (4,) is different from one with shape (4, 1). The former has only 1 dimension, while the latter has 2 dimensions.\n\n# seq.shape\ncol_vect = seq.reshape(4,1)\ncol_vect\n\narray([[0],\n       [3],\n       [6],\n       [9]])\n\n\nTo create an array of regularly spaced real numbers, use np.linspace().\n\narr_real = np.linspace(start = 0.2, stop = 3.3, num = 24).reshape(2, 3, 4)  \narr_real\n\narray([[[0.2       , 0.33478261, 0.46956522, 0.60434783],\n        [0.73913043, 0.87391304, 1.00869565, 1.14347826],\n        [1.27826087, 1.41304348, 1.54782609, 1.6826087 ]],\n\n       [[1.8173913 , 1.95217391, 2.08695652, 2.22173913],\n        [2.35652174, 2.49130435, 2.62608696, 2.76086957],\n        [2.89565217, 3.03043478, 3.16521739, 3.3       ]]])\n\n\nSometimes we need to create a placeholder array with the appropriate dimensions, and then fill it in later. This is preferrable to growing an array by appending to it.\n\nnp.zeros((3, 5)) # there is also an np.ones() function\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\nInstead of specifying the dimensions of an array ourselves, we can create arrays of zeros or ones in the shape of other existing arrays.\n\n# Creates an array of zeros, of the same shape as \"arr_real\".\nnp.ones_like(arr_real)\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\n\n\nSlice Operator in Multiple Dimensions\nMultidimensional NumPy arrays can be accessed with comma separated slice notation. When fewer indices are provided than the number of axes, the missing indices are considered complete slices for the remaining dimensions.\nBy the way, when printing, the last axis will be printed left-to-right, and the second last axis will be printed from top-to-bottom. The remaining axes will be printed with a line in between:\n\narr_real\n\narray([[[0.2       , 0.33478261, 0.46956522, 0.60434783],\n        [0.73913043, 0.87391304, 1.00869565, 1.14347826],\n        [1.27826087, 1.41304348, 1.54782609, 1.6826087 ]],\n\n       [[1.8173913 , 1.95217391, 2.08695652, 2.22173913],\n        [2.35652174, 2.49130435, 2.62608696, 2.76086957],\n        [2.89565217, 3.03043478, 3.16521739, 3.3       ]]])\n\n\nHere are examples based on this array. Try to guess what each will return before you run it:\n\narr_real[1,2,3]\narr_real[0, 2, ::-1]\narr_real[1, 0:3:2]\narr_real[:,2,:]\n\nHere are examples using Boolean indexing, which means that we use an array of True and False entries to determine which elements to return.\n\narr_real &gt; 3\n\narray([[[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]],\n\n       [[False, False, False, False],\n        [False, False, False, False],\n        [False,  True,  True,  True]]])\n\n\n\narr_real[arr_real &gt; 3]\n\narray([3.03043478, 3.16521739, 3.3       ])\n\n\n\n\nBasic Operations\n\n# Setting a seed allows for reproducibility of random number generation\n# across sessions.\nrng = np.random.default_rng(1361)\n\n\na = rng.uniform(size=(3,5))\nb = rng.uniform(size=(3,5))\nb\n\narray([[0.47031767, 0.43520574, 0.74929584, 0.19794778, 0.91397998],\n       [0.95979996, 0.10301973, 0.82972687, 0.61118187, 0.04727632],\n       [0.51642555, 0.32188431, 0.57800994, 0.73144202, 0.74020866]])\n\n\n\n# Element-wise addition.\na + b \n\narray([[0.92582851, 0.90610288, 1.3360719 , 0.43528561, 1.30890054],\n       [1.78346478, 0.31376731, 1.51889626, 1.40816571, 0.21158293],\n       [0.52725269, 0.57843605, 1.07039529, 1.54221413, 0.82312206]])\n\n\n\n# Element-wise multiplication: NOT matrix multiplication.\na * b\n\narray([[0.2142348 , 0.20493714, 0.43966886, 0.0469805 , 0.36094949],\n       [0.79055346, 0.02171116, 0.57182236, 0.48710208, 0.00776781],\n       [0.00559141, 0.08257998, 0.28460362, 0.59303279, 0.06137322]])\n\n\n\n# Matrix multiplication (need to transpose \"b\" to match get the right dimensions).\n# We can also do \"a @ b.T\".\na.dot(b.T)\n\narray([[1.26677078, 1.13630182, 1.19189672],\n       [1.30342857, 1.87895687, 1.5961133 ],\n       [0.721959  , 0.94481619, 1.02718103]])\n\n\n\n\nAxis-wise Operations\n\narr_real.shape\n\n(2, 3, 4)\n\n\n\narr_real.mean(axis = 0) # mean across the 0th (\"first\") axis\n\narray([[1.00869565, 1.14347826, 1.27826087, 1.41304348],\n       [1.54782609, 1.6826087 , 1.8173913 , 1.95217391],\n       [2.08695652, 2.22173913, 2.35652174, 2.49130435]])\n\n\nThe top-left element comes from the average of arr_real[0,0,0] and arr_real[1,0,0]. Similarly, the element to the right of it comes from the average of arr_real[0,0,1] and arr_real[1,0,1]:\n\n(arr_real[0,0,1] + arr_real[1,0,1]) / 2\n\n1.143478260869565\n\n\n\narr_real.mean(axis = 1)\n\narray([[0.73913043, 0.87391304, 1.00869565, 1.14347826],\n       [2.35652174, 2.49130435, 2.62608696, 2.76086957]])\n\n\nNote that arr_real[0] is a 2D array, with shape (3, 4). Suppose we wish to compute the row means. This means we have to apply the operation by the column axis (axis = 1).\n\n# the mean across the second axis of arr_real[0], not of arr_real itself.\narr_real[0].mean(axis = 1) \n\narray([0.40217391, 0.94130435, 1.48043478])\n\n\nIf we wanted to identify the row with the largest mean, we use argmax() on the resulting array.\n\narr_real[0].mean(axis=1).argmax()\n\n2\n\n\nHere is a table with some common operations that you can apply on a numpy array.\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nshape\nReturns dimensions, e.g. matrix1.shape\n\n\nT\nTransposes the array, e.g. matrix1.T\n\n\nmean\nComputes col- or row-wise means, e.g. matrix1.mean(axis=0) or matrix1.mean(axis=1)\n\n\nsum\nComputes col- or row-wise means, e.g. matrix1.sum(axis=0) or matrix1.sum(axis=1)\n\n\nargmax\nReturn the index corresponding to the max within the specified dimension, e.g. matrix1.argmax(axis=0) for the position with the max within each column.\n\n\nreshape\nTo change the dimensions, e.g. array1.reshape((5,1)) converts the array into a 5x1 matrix",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#pandas",
    "href": "01-intro.html#pandas",
    "title": "1  Introduction to Python",
    "section": "1.10 Pandas",
    "text": "1.10 Pandas\n\nSeries\nA Series is a one-dimensional labeled array. The axis labels are referred to as the index. The simplest way to create a Series is to pass a sequence and an index to pd.Series().\n\nExample 1.1 (Creating Pandas Series) \nConsider the following data, from the football league in Spain.\n\nyear = pd.Series(list(range(2010, 2013) ) * 3)\n\nteam = [\"Barcelona\", \"RealMadrid\", \"Valencia\"] * 3\nteam.sort()\nteam = pd.Series(team)\n\nwins = pd.Series([30, 28, 32, 29, 32, 26, 21, 17, 19])\ndraws = pd.Series([6, 7, 4, 5, 4, 7, 8, 10, 8])\nlosses = pd.Series([2, 3, 2, 4, 2, 5, 9, 11, 11])\n\n#wins.index\n#wins.values\n\nTo access particular values, we can use the slice operator.\n\nwins[0:6:2]\n\n0    30\n2    32\n4    32\ndtype: int64\n\n\nTo convert a Series object to an ndarray, we use the following method:\n\nwins.to_numpy()\n\narray([30, 28, 32, 29, 32, 26, 21, 17, 19])\n\n\nIf we specify an index, we can use it to access values in the Series.\n\ns = pd.Series(rng.uniform(size=5), \n             index=['a', 'b', 'c', 'd', 'e'])\n# s\n# s.index\n# s.values\n\n\ns[['a', 'c']] \n\na    0.974173\nc    0.948907\ndtype: float64\n\n\nBe careful when you combine the slice operator with label-based indexing. Unlike vanilla Python, Pandas includes both end-points!\n\ns['a':'d']\n\na    0.974173\nb    0.270230\nc    0.948907\nd    0.675365\ndtype: float64\n\n\n\n\n\nDataFrames\nA DataFrame is a 2-dimensional labeled data structure with possibly different data types. It is the most commonly used Pandas object. The index of a DataFrame refers to the row labels (axis 0). The columns refer to the column labels (axis 1).\nDataFrames can be constructed from Series, dictionaries, lists and 2-d arrays. For our course, we will typically create a DataFrame directly from a file.\n\nExample 1.2 (Creating Pandas DataFrame from Series) \nWe can create a DataFrame from the earlier series.\n\nlaliga = pd.DataFrame({'Year': year,\n              'Team': team,\n              'Wins': wins,\n              'Draws': draws,\n              'Losses': losses\n})\n\nTo inspect a DataFrame, we can use info(), head() and tail() methods.\n\nshow(laliga.head())\n\n\n\n    \n      \n      Year\n      Team\n      Wins\n      Draws\n      Losses\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\nReading in Data\nPandas can read in data stored in multiple formats, including CSV, tab-separated files, Excel files and HDF5 files.\n\nExample 1.3 (Happiness Dataset) \nThe CSV file read in here contains the happiness scores of 164 countries from 2015 to 2017. Click here for a full report on the dataset. The final score was based on many other factors (such as GDP per capita, family, freedom etc) which is included in the file as well. We will simplify things by just reading in the country, final score computed and year.\nIn each year, not all of the 164 countries had their scores surveyed and taken. This results in some countries having missing values (NaN) in certain years.\n\nhapp = pd.read_csv('data/happiness_report.csv', header=0, \n                    na_values='NA')\n\n\nshow(happ)\n\n\n\n    \n      \n      Country\n      Happiness.Rank\n      Happiness.Score\n      GDP\n      Family\n      Life.Expectancy\n      Freedom\n      Govt.Corruption\n      Generosity\n      Dystopia.Residual\n      Year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\nBasic Selection\nIn dataframes, row selection can be done with integers in with the slice operator. In practice, this is not used, because we typically wish to select a set of rows based on a condition.\n\nprint(happ[10:12])\n\n       Country  Happiness.Rank  Happiness.Score      GDP   Family  \\\n10      Israel            11.0            7.278  1.22857  1.22393   \n11  Costa Rica            12.0            7.226  0.95578  1.23788   \n\n    Life.Expectancy  Freedom  Govt.Corruption  Generosity  Dystopia.Residual  \\\n10          0.91387  0.41319          0.07785     0.33172            3.08854   \n11          0.86027  0.63376          0.10583     0.25497            3.17728   \n\n    Year  \n10  2015  \n11  2015  \n\n\nTo select columns, you may use a list of column names.\n\nhapp[['GDP', 'Freedom']] # note the difference with happ['GDP']\n# happ.GDP.head()\n\n\n\n\n\n\n\n\nGDP\nFreedom\n\n\n\n\n0\n1.39651\n0.66557\n\n\n1\n1.30232\n0.62877\n\n\n2\n1.32548\n0.64938\n\n\n3\n1.45900\n0.66973\n\n\n4\n1.32629\n0.63297\n\n\n...\n...\n...\n\n\n487\nNaN\nNaN\n\n\n488\nNaN\nNaN\n\n\n489\nNaN\nNaN\n\n\n490\nNaN\nNaN\n\n\n491\nNaN\nNaN\n\n\n\n\n492 rows × 2 columns\n\n\n\nRemember that we are not working with numpy arrays, so this will not work:\n\nhapp[0:10, 2:4]\n\n\n\nIndexing and Selecting Data\nThe two main methods of advanced data selection use the .loc and .iloc functions. Although we call them functions, they are summoned using the [ ] notation. The .loc is primarily label-based. The common allowed inputs to .loc are\n\na single label,\na list of labels,\na slice object,\na boolean array.\n\nThe .iloc is primarily an integer-based input. The common allowed inputs to .iloc are\n\na single integer,\na list of integers,\na slice object,\na boolean array.\n\nWhen selecting from a DataFrame with .loc or .iloc, we can provide a comma-separated index, just as with NumPy. It is good to keep this reference page bookmarked.\nTake note that this next command will only work if the index is made up of integers!\n\nprint(happ.loc[2:5])\n\n# happ.loc[[2,3,4,5]]\n\n   Country  Happiness.Rank  Happiness.Score      GDP   Family  \\\n2  Denmark             3.0            7.527  1.32548  1.36058   \n3   Norway             4.0            7.522  1.45900  1.33095   \n4   Canada             5.0            7.427  1.32629  1.32261   \n5  Finland             6.0            7.406  1.29025  1.31826   \n\n   Life.Expectancy  Freedom  Govt.Corruption  Generosity  Dystopia.Residual  \\\n2          0.87464  0.64938          0.48357     0.34139            2.49204   \n3          0.88521  0.66973          0.36503     0.34699            2.46531   \n4          0.90563  0.63297          0.32957     0.45811            2.45176   \n5          0.88911  0.64169          0.41372     0.23351            2.61955   \n\n   Year  \n2  2015  \n3  2015  \n4  2015  \n5  2015  \n\n\nNotice below how the slice operator is inclusive when we use .loc, but not inclusive when we use .iloc.\n\nhapp.loc[2:10:4, \"GDP\":\"Generosity\":2]\n\n\n\n\n\n\n\n\nGDP\nLife.Expectancy\nGovt.Corruption\n\n\n\n\n2\n1.32548\n0.87464\n0.48357\n\n\n6\n1.32944\n0.89284\n0.31814\n\n\n10\n1.22857\n0.91387\n0.07785\n\n\n\n\n\n\n\n\nhapp.iloc[2:11:4, 3:8:2] # Same as above, but with .iloc\n\n\n\n\n\n\n\n\nGDP\nLife.Expectancy\nGovt.Corruption\n\n\n\n\n2\n1.32548\n0.87464\n0.48357\n\n\n6\n1.32944\n0.89284\n0.31814\n\n\n10\n1.22857\n0.91387\n0.07785\n\n\n\n\n\n\n\n\n\nFiltering Data\nSuppose we are interested in the very happy countries. Here is how we can filter the data with a boolean array.\n\nExample 1.4 (Happiest Countries) \nSuppose we retrieve the happiest countries; those with a score more than 6.95. Can you surmise why we use this value?\n\nhappiest = happ[happ['Happiness.Score'] &gt; 6.95]\n\nhappiest.Country.unique()\n\narray(['Switzerland', 'Iceland', 'Denmark', 'Norway', 'Canada', 'Finland',\n       'Netherlands', 'Sweden', 'New Zealand', 'Australia', 'Israel',\n       'Costa Rica', 'Austria', 'Mexico', 'United States', 'Brazil',\n       'Ireland', 'Germany'], dtype=object)\n\n\nNotice that there isn’t a single Asian or African country in the happiest 10% of countries!\n\nWhen filtering, we can also combine Boolean indices.\n\n# Top 3 happiest countries in 2015\nprint(happ[(happ.Year == 2015) & (happ['Happiness.Rank'] &lt;= 3)])\n\n       Country  Happiness.Rank  Happiness.Score      GDP   Family  \\\n0  Switzerland             1.0            7.587  1.39651  1.34951   \n1      Iceland             2.0            7.561  1.30232  1.40223   \n2      Denmark             3.0            7.527  1.32548  1.36058   \n\n   Life.Expectancy  Freedom  Govt.Corruption  Generosity  Dystopia.Residual  \\\n0          0.94143  0.66557          0.41978     0.29678            2.51738   \n1          0.94784  0.62877          0.14145     0.43630            2.70201   \n2          0.87464  0.64938          0.48357     0.34139            2.49204   \n\n   Year  \n0  2015  \n1  2015  \n2  2015  \n\n\n\n\nMissing Values\nThe .info() method will yield information on missing values, column by column. We can see there are 21 rows with missing values.\n\nhapp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 492 entries, 0 to 491\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Country            492 non-null    object \n 1   Happiness.Rank     471 non-null    float64\n 2   Happiness.Score    471 non-null    float64\n 3   GDP                471 non-null    float64\n 4   Family             471 non-null    float64\n 5   Life.Expectancy    471 non-null    float64\n 6   Freedom            471 non-null    float64\n 7   Govt.Corruption    471 non-null    float64\n 8   Generosity         471 non-null    float64\n 9   Dystopia.Residual  471 non-null    float64\n 10  Year               492 non-null    int64  \ndtypes: float64(9), int64(1), object(1)\nmemory usage: 42.4+ KB\n\n\nSometimes, it is appropriate to drop rows with missing values. This can be done with the .dropna method. Remember that it returns a new dataframe. The original one remains unchanged, unless you include the inplace=True argument.\n\nnew_df = happ.dropna()\n\n# pd.isna(happ)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-01-ref",
    "href": "01-intro.html#sec-01-ref",
    "title": "1  Introduction to Python",
    "section": "1.11 References",
    "text": "1.11 References\nIn this chapter, we have introduced the following data science tools:\n\nPython programming language\nJupyter notebooks for working with Python\nComputer set-up for data science with Python\n\nPython is very widely used for data science, and especially for the machine learning aspect of it. For those of you with intentions to take up the GC in Deep Learning or Data Mining, it will be critical to be familiar with the language. It will be used again in at least DSA5102.\n\nRegular expression HOWTO A tutorial with examples on regular expressions (for manipulating and searching through strings).\nFormatting string literals: Or just f-strings\nPython documentation: This is the official Python documentation page. It contains a tutorial, detailed description of the usual libraries and HOWTOs for many specific tasks. Most sections contain working examples that you can learn from, or modify to suit your task. It is good to bookmark it.\nObtaining a github copilot license For proof of student status, access EduRec -&gt; Academics -&gt; Academic Records -&gt; View Student Status Letter, then take a photo of the pdf. Your application status should be shown in the same GitHub page. It will take a few days for the copilot access to be granted after your application is approved.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Introduction to Python",
    "section": "",
    "text": "i.e., Python without any packages imported.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Python</span>"
    ]
  },
  {
    "objectID": "02-inference.html",
    "href": "02-inference.html",
    "title": "2  Statistical Inference",
    "section": "",
    "text": "2.1 Introduction\nIn statistics, we often wish to make inference about a population, using a sample. The sample typically has uncertainty associated with it, because the precise values will differ each time we draw a sample from the population. The process of utilising the observations from the sample to make conclusions regarding population characteristics, is known as statistical inference. This topic introduces two main techniques for statistical inference in common application contexts. The two techniques are (1) hypothesis tests and (2) confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#introduction",
    "href": "02-inference.html#introduction",
    "title": "2  Statistical Inference",
    "section": "",
    "text": "Hypothesis Tests\nYou might have been introduced to hypothesis tests in an introductory course before, but just to get us all on the same page, here is the general approach for conducting a hypothesis test:\n\nStep 1: Assumptions\nIn this step, we make a note of the assumptions required for the test to be valid. In some tests, this step is carried out last, but, it is nonetheless essential to perform as it could invalidate the test. Some tests are very sensitive to the assumptions - this is in fact the main reason that the class of robust statistics was invented.\n\n\nStep 2: State the hypotheses and significance level\nThe purpose of hypothesis testing is to make an inferential statement about the population from which the data arose. This inferential statement is what we refer to as the hypothesis regarding the population.\nThe hypotheses will be stated as a pair: The first hypothesis is the null hypothesis \\(H_0\\) and the second is the alternative hypothesis \\(H_1\\). Both statements will involve the population parameter (not the data summary) of interest. For example, if we have a sample of observations from two groups \\(A\\) and \\(B\\), and we wish to assess if the mean of the populations is different, the hypotheses would be\n\\[\\begin{eqnarray}\nH_0: & \\mu_A = \\mu_B \\\\\nH_1: & \\mu_A \\ne \\mu_B\n\\end{eqnarray}\\]\n\\(H_0\\) is usually a statement that indicates “no difference”, and \\(H_1\\) is the complement of \\(H_0\\), or a subset of it.\nAt this stage, it is also crucial to state the significance level of the test. The significance level corresponds to the Type I error of the test - the probability of rejecting \\(H_0\\) when in fact it was true. This level is usually denoted as \\(\\alpha\\), and is usually taken to be 5%, but there is no reason to adopt this blindly. Where possible, the significance level should be chosen to be appropriate for the problem at hand.\n\nThink of the choice of 5% as corresponding to accepting an error at a rate of 1 in 20 - that’s how it was originally decided upon by R A Fisher.\n\n\n\nStep 3: Compute the test statistic\nThe test statistic is usually a measure of how far the observed data deviates from the scenario defined by \\(H_0\\). Usually, the larger it is, the more evidence we have against \\(H_0\\). The construction of a hypothesis test (by theoretical statisticians) involves the derivation of the exact or approximate distribution of the test statistic under \\(H_0\\). Deviations from the assumption could render this distribution incorrect.\n\n\nStep 4: Compute the \\(p\\)-value\nThe \\(p\\)-value quantifies the chance of occurrence of a dataset as extreme as the one observed, under the assumptions of \\(H_0\\). The distribution of the test statistic under \\(H_0\\) is used to compute this value between 0 and 1. A value closer to 0 indicates stronger evidence against \\(H_0\\).\n\n\nStep 5: State your conclusion\nThis is the binary decision stage - either we reject \\(H_0\\), or we do not reject \\(H_0\\). It is conventional to use this terminology (instead of “accepting \\(H_1\\)”) since our \\(p\\)-value is a measure of evidence against \\(H_0\\) (not for it).\n\n\n\nConfidence Intervals\nConfidence intervals are an alternative method of inference for population parameters. Instead of yielding a binary reject/do-not-reject result, they return an\ninterval that contains the plausible values for the population parameter. Many confidence intervals are derived by inverting hypothesis tests, and almost all confidence intervals are of the form\n\nSample estimate \\(\\pm\\) margin of error\n\nFor instance, if we observe \\(x_1, \\ldots, x_n\\) from a Normal distribution, and wish to estimate the mean of the distribution, the 95% confidence interval based on the the \\(t\\) distribution is\n\\[\\begin{equation*}\n\\bar{x} \\pm t_{0.025, n-1} \\times \\frac{s}{\\sqrt{n}}\n\\end{equation*}\\]\nwhere\n\n\\(\\bar{x}\\) is the sample mean,\n\\(s\\) is the sample standard deviation, and\n\\(t_{0.025, n-1}\\) is the 0.025-quantile from the \\(t\\) distribution with \\(n-1\\) degrees of freedom.\n\nThe formulas for many confidence intervals rely on asymptotic Normality of the estimator. However, this is an assumption that can be overcome with the technique of bootstrapping. If time permits, we shall touch on this in a later topic of our course. Bootstrapping can also be used to sidestep the distributional assumptions in hypothesis tests, but I still much prefer confidence intervals to tests because they yield an interval; they provide much more information than a binary outcome.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#comparing-means",
    "href": "02-inference.html#comparing-means",
    "title": "2  Statistical Inference",
    "section": "2.2 Comparing Means",
    "text": "2.2 Comparing Means\n\n2-sample Tests\nIn an independent samples \\(t\\)-test, observations in one group yield no information about the observations in the other group. Independent samples can arise in a few ways:\n\nIn an experimental study, study units could be assigned randomly to different treatments, thus forming the two groups.\nIn an observational study, we could draw a random sample from the population, and then record an explanatory categorical variable on each unit, such as the gender or senior-citizen status.\nIn an observational study, we could draw a random sample from a group (say smokers), and then a random sample from another group (say non-smokers). This would result in a situation where the independent 2-sample \\(t\\)-test is appropriate.\n\n\nFormal Set-up\nFormally speaking, this is how the independent 2-sample t-test works:\nSuppose that \\(X_1,X_2,\\ldots,X_{n_1}\\) are independent observations from group 1, and \\(Y_1, \\ldots Y_{n_2}\\) are independent observations from group 2. It is assumed that\n\\[\\begin{eqnarray}\nX_i &\\sim& N(\\mu_1,\\, \\sigma^2),\\; i=1,\\ldots,n_1 \\\\\nY_j &\\sim& N(\\mu_2,\\, \\sigma^2),\\; j=1,\\ldots,n_2\n\\end{eqnarray}\\]\nThe null and alternative hypotheses would be\n\\[\\begin{eqnarray}\nH_0: & \\mu_1 = \\mu_2 \\\\\nH_1: & \\mu_1 \\ne \\mu_2\n\\end{eqnarray}\\]\nThe test statistic for this test is:\n\\[\nT_1 = \\frac{(\\bar{X} - \\bar{Y}) - 0 }{s_p\\sqrt{1/n_1 + 1/n_2} }\n\\]\nwhere\n\\[\\begin{equation*}\ns^2_p = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 -2 }\n\\end{equation*}\\]\nNotice that the numerator of \\(T_1\\) will be large (in absolute value) when the difference in sample group means is also large. This is what we mean when we say that the test-statistic measures deviation from the null hypothesis.\nUnder \\(H_0\\), the test statistic \\(T_1\\) follows a \\(t\\)-distribution with \\(n_1 + n_2 -2\\) degrees of freedom. When we use a software to apply the test above, it will typically also return a confidence interval, computed as\n\\[\\begin{equation*}\n(\\bar{X} - \\bar{Y}) \\pm t_{n_1 + n_2 -2, 1 - \\alpha/2} \\times s_p\\sqrt{1/n_1 + 1/n_2}\n\\end{equation*}\\]\nFor more details on the test, refer to the links in the references section Section 2.7.\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy import stats\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels.stats.multicomp as mc\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom itables import show\n\n\nExample 2.1 (Example: Abalone Measurements) \nThe dataset on abalone measurements from the UCI machine learning repository contains measurements of physical characteristics, along with the gender status. We derive a sample of 50 measurements of male and female abalone records for use here. Our goal is to study if there is a significant difference between the viscera weight between males and females. The derived dataset can be found on Canvas.\n\nabl = pd.read_csv(\"data/abalone_sub.csv\")\nshow(abl)\n\n\n\n    \n      \n      gender\n      viscera\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nx = abl.viscera[abl.gender == \"F\"]\ny = abl.viscera[abl.gender == \"M\"]\n\nt_out = stats.ttest_ind(y, x)\nci_95 = t_out.confidence_interval()\n\n\nprint(f\"The $p$-value for the test is {t_out.pvalue:.3f}.\")#\nprint(f\"The actual value of the test statistic is {t_out.statistic:.3f}.\")\nprint(f\"The upper and lower limits of the CI are ({ci_95[0]:.3f}, {ci_95[1]:.3f}).\")\n\nThe $p$-value for the test is 0.365.\nThe actual value of the test statistic is 0.910.\nThe upper and lower limits of the CI are (-0.023, 0.063).\n\n\n\nTo assess the normality assumption, we make histograms and qq-plots. Histograms of data from a Normal distribution should appear symmetric and bell-shaped. The tails on both sides should come down at a moderate pace. If we observe asymmetry in our histogram, we might suspect deviation from Normality. As we can see from the figure below, a histogram with a long tail on the right (left) is referred to as right-skewed (corr. left-skewed).\n\n\n\nSample Histograms\n\n\n\nExample 2.2 (Example: Abalone Measurements) \nNow we turn back to the histograms for the abalone data. Indeed, they do indicate some deviation from Normality - the female group is a little skewed to the right, while the male group appears to have a sharper peak than a Normal.\n\nsns.displot(abl, x='viscera', col='gender', kind='hist', stat='density', \n            binwidth=0.1, height=4, aspect=1.2);\n\n\n\n\n\n\n\n\n\nA Quantile-Quantile plot is a graphical diagnostic tool for assessing if a dataset follows a particular distribution. Most of the time we would be interested in comparing against a Normal distribution.\nA QQ-plot plots the standardized sample quantiles against the theoretical quantiles of a N(0; 1) distribution. If they fall on a straight line, then we would say that there is evidence that the data came from a normal distribution. Especially for unimodal datasets, the points in the middle will fall close to the line. The value of a QQ-plot is in judging if the tails of the data are fatter or thinner than the tails of the Normal.\n\n\n\n\n\n\n\n\n\nThinner\n\n\n\n\n\n\n\nFatter\n\n\n\n\n\n\nExample 2.3 (Example: Abalone Measurements) \nIf we compare the qq-plots from the data (below) with the reference (above), we can infer that for females, the left tail is thinner than a Normal - it abruptly cuts off. For males, both the left and the right tail are fatter than a Normal’s.\n\nf, axs = plt.subplots(1, 2, figsize=(10,4))\ntmp = plt.subplot(121)\nsm.qqplot(x, line=\"q\", ax=tmp)\ntmp.set_title('Females')\ntmp = plt.subplot(122)\nsm.qqplot(y, line=\"q\", ax=tmp)\ntmp.set_title('Males');\n\n\n\n\n\n\n\n\n\nIf you are keen on learning about particular hypothesis tests for Normality, take a look at the references Section 2.7.\nWe also need to assess if the variances are equal. While there are many hypothesis tests specifically for assessing if variances are equal (e.g. Levene, Bartlett), in our class, I advocate a simple rule of thumb. If the larger s.d is more than twice the smaller one, than we should not use the equal variance form of the test. This rule of thumb is widely used in practice (see the references Section 2.7).\n\nabl.groupby('gender').describe()\n\n\n\n\n\n\n\n\nviscera\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n\n\nF\n50.0\n0.28241\n0.108707\n0.095\n0.201250\n0.275\n0.365125\n0.575\n\n\nM\n50.0\n0.30220\n0.108746\n0.040\n0.253125\n0.310\n0.348750\n0.638\n\n\n\n\n\n\n\nWe would conclude that there is no significant difference between the mean viscera weight of males and females.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#paired-sample-tests",
    "href": "02-inference.html#paired-sample-tests",
    "title": "2  Statistical Inference",
    "section": "2.3 Paired Sample Tests",
    "text": "2.3 Paired Sample Tests\nThe data in a paired sample test also arises from two groups, but the two groups are not independent. A very common scenario that gives rise to this test is when the same subject receives both treatments. His/her measurement under each treatment gives rise to a measurement in each group. However, the measurements are no longer independent.\n\nExample 2.4 (Example: Reaction time of drivers) \nConsider a study on 32 drivers sampled from a driving school. Each driver is put in a simulation of a driving situation, where a target flashes red and green at random periods. Whenever the driver sees red, he/she has to press a brake button.\nFor each driver, the study is carried out twice - at one of the repetitions, the individual carries on a phone conversation while at the other, the driver listens to the radio. Each measurement falls under one of two groups - “phone” or “radio”, but the measurements for driver \\(i\\) are clearly related.\nSome people might just have a slower/faster baseline reaction time!\nThis is a situation where a paired sample test is appropriate, not an independent sample test.\n\n\nFormal Set-up\nSuppose that we observe \\(X_1, \\ldots , X_n\\) independent observations from group 1 and \\(Y_1, \\ldots, Y_n\\) independent observations from group 2. However the pair \\((X_i, Y_i)\\) are correlated. Similar to the previous section, it is assumed that\n\\[\\begin{eqnarray}\nX_i &\\sim& N(\\mu_1,\\, \\sigma_1^2),\\; i=1,\\ldots,n \\\\\nY_j &\\sim& N(\\mu_2,\\, \\sigma_2^2),\\; j=1,\\ldots,n\n\\end{eqnarray}\\]\nWe let \\(D_i = X_i - Y_i\\) for \\(i=1, \\ldots, n\\). It follows that \\[\nD_i \\sim N(\\mu_1 - \\mu_2,\\; \\sigma^2_1 + \\sigma^2_2 - 2 cov(X_i, Y_i))\n\\] The null and alternative hypotheses are stated in terms of the distribution of \\(D_i\\):\n\\[\\begin{eqnarray*}\nH_0: & \\mu_D = 0 \\\\\nH_1: & \\mu_D \\ne 0\n\\end{eqnarray*}\\]\nThe test statistic for this test is:\n\\[\nT_2 = \\frac{\\bar{D} - 0 }{s / \\sqrt{n} }\n\\] where \\[\ns^2 = \\frac{\\sum_{i=1}^n (D_i - \\bar{D})^2}{(n - 1)}\n\\]\nUnder \\(H_0\\), the test statistic \\(T_2 \\sim t_{n - 1}\\). When we use a software to apply the test above, it will typically also return a confidence interval, computed as\n\\[\n\\bar{D} \\pm t_{n - 1, 1 - \\alpha/2} \\times s / \\sqrt{n}\n\\]\n\nExample 2.5 (Example: Heart Rate Before/After Treadmill) \nThe following dataset comes from a textbook. In a self-recorded experiment, an individual recorded his heart rate before using a treadmill (baseline) and 5 minutes after use, for 12 days in 2006.\n\nhr_df = pd.read_csv(\"data/health_promo_hr.csv\")\n#hr_df.head()\np_test_out = stats.ttest_rel(hr_df.baseline, hr_df.after5)\n\nprint(f\"The $p$-value for the test is {p_test_out.pvalue:.2g}.\")\nprint(f\"\"\"\nThe difference in means is {hr_df.baseline.mean() - hr_df.after5.mean():.3f}.\n       \"\"\")\n\nThe $p$-value for the test is 2.2e-10.\n\nThe difference in means is -15.229.\n       \n\n\nWhile we do not include them here, it is imperative to also make the checks for Normality. If you were to make them, you would realise that the sample size is rather small. Even so, it is difficult to make the case for Normality here.\nHere is a plot that is particularly useful in paired sample studies:\n\nax1 = hr_df.plot(x='baseline', y='after5',kind='scatter', marker='o', \n                 edgecolor='blue', color='none')\ngroup_means = hr_df.loc[:, ['baseline', 'after5']].mean(axis=0)\nax1.set_xlim(75, 105)\nax1.set_ylim(75,105)\nax1.plot([75,105], [75,105], color=\"lightblue\", linestyle=\"dashed\");\nax1.scatter(group_means.iloc[0], group_means.iloc[1],  marker='o', \n            edgecolor='blue', color='none', s=100);\nax1.set_title('Agreement of after5 and baseline');",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#anova",
    "href": "02-inference.html#anova",
    "title": "2  Statistical Inference",
    "section": "2.4 ANoVA",
    "text": "2.4 ANoVA\nIn this section, we introduce the one-way analysis of variance (ANOVA), which generalises the \\(t\\)-test methodology to more than 2 groups. Hypothesis tests in the ANOVA framework require the assumption of Normality.\nWhile the \\(F\\)-test in ANOVA provides a determination of whether or not the group means are different, in practice, we would always want to follow up with specific comparisons between groups as well.\n\nExample 2.6 (Example: Heifers) \nThe following example was taken from Introduction to Statistical Data Analysis for Life Sciences.\nAn experiment with dung from heifers was carried out in order to explore the influence of antibiotics on the decomposition of dung organic material. As part of the experiment, 36 heifers were randomly assigned into six groups. Note that a heifer is a young, female cow that has not had her first calf yet.\nAntibiotics of different types were added to the feed for heifers in five of the groups. The remaining group served as a control group. For each heifer, a bag of dung was dug into the soil, and after 8 weeks the amount of organic material was measured for each bag.\nHere is a boxplot of the data from each group, along with summary statistics in a table below.\n\nheifers = pd.read_csv('data/antibio.csv')\nsns.boxplot(heifers, x='type', y='org',);\n\n\n\n\n\n\n\n\nCompared to the control group, it does appear that the median organic weight of the dung from the other heifer groups is higher. The following table displays the mean, standard deviation, and count from each group:\n\nheifers.groupby('type').describe()\n\n\n\n\n\n\n\n\norg\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ntype\n\n\n\n\n\n\n\n\n\n\n\n\nAlfacyp\n6.0\n2.895000\n0.116748\n2.75\n2.7950\n2.915\n2.9900\n3.02\n\n\nControl\n6.0\n2.603333\n0.118771\n2.43\n2.5450\n2.595\n2.6825\n2.76\n\n\nEnroflox\n6.0\n2.710000\n0.161988\n2.42\n2.6775\n2.735\n2.8075\n2.88\n\n\nFenbenda\n6.0\n2.833333\n0.123558\n2.66\n2.7675\n2.850\n2.8725\n3.02\n\n\nIvermect\n6.0\n3.001667\n0.109438\n2.81\n2.9625\n3.045\n3.0600\n3.11\n\n\nSpiramyc\n4.0\n2.855000\n0.054467\n2.80\n2.8300\n2.845\n2.8700\n2.93\n\n\n\n\n\n\n\nObserve that the Spiramycin group only yielded 4 readings instead of 6. Our goal in this topic is to understand a technique for assessing if group means are statistically different from one another. Here are the specific analyses that we shall carry out:\n\nHeifers: Questions of Interest\n\nIs there any significant difference, at 5% level, between the mean decomposition level of the groups?\nAt 5% level, is the mean level for Enrofloxacin different from the control group?\nPharmacologically speaking, Ivermectin and Fenbendazole are similar to each other. Let us call this sub-group (A). They work differently than Enrofloxacin. At 5% level, is there a significant difference between the mean from sub-group A and Enrofloxacin?\n\n\n\n\nFormal Set-up\nSuppose there are \\(k\\) groups with \\(n_i\\) observations in the \\(i\\)-th group. The \\(j\\)-th observation in the \\(i\\)-th group will be denoted by \\(Y_{ij}\\). In the One-Way ANOVA, we assume the following model:\n\\[\\begin{equation}\nY_{ij}  = \\mu + \\alpha_i + e_{ij},\\; i=1,\\ldots,k,\\; j=1,\\ldots,n_i\n\\end{equation}\\]\n\n\\(\\mu\\) is a constant, representing the underlying mean of all groups taken together.\n\\(\\alpha_i\\) is a constant specific to the \\(i\\)-th group. It represents the difference between the mean of the \\(i\\)-th group and the overall mean.\n\\(e_{ij}\\) represents random error about the mean \\(\\mu + \\alpha_i\\) for an individual observation from the \\(i\\)-th group.\n\nIn terms of distributions, we assume that the \\(e_{ij}\\) are i.i.d from a Normal distribution with mean 0 and variance \\(\\sigma^2\\). This leads to the model for each observation:\n\\[\nY_{ij} \\sim N(\\mu + \\alpha_i,\\; \\sigma^2)\n\\]\nIt is not possible to estimate both \\(\\mu\\) and all the \\(k\\) different \\(\\alpha_i\\)’s, since we only have \\(k\\) observed mean values for the \\(k\\) groups. For identifiability purposes, we need to constrain the parameters. There are two common constraints used, and note that different software have different defaults:\n\nSetting \\(\\sum_{i=1}^k \\alpha_i = 0\\), or\nSetting \\(\\alpha_1= 0\\).\n\nContinuing on from the equation for \\(Y_{ij}\\), let us denote the mean for the \\(i\\)-th group as \\(\\overline{Y_i}\\), and the overall mean of all observations as \\(\\overline{\\overline{Y}}\\). We can then write the deviation of an individual observation from the overall mean as:\n\\[\nY_{ij} - \\overline{\\overline{Y}} = \\underbrace{(Y_{ij} - \\overline{Y_i})}_{\\text{within}} +\n\\underbrace{(\\overline{Y_i} - \\overline{\\overline{Y}})}_{\\text{between}}\n\\]\nThe first term on the right of the above equation is an indication of within-group variability. The second term on the right is an indication of between-group variability. The intuition behind the ANOVA procedure is that if the between-group variability is large and the within-group variability is small, then we have evidence that the group means are different.\nIf we square both sides of the above equation and sum over all observations, we arrive at the following equation; the essence of ANOVA:\n\\[\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\overline{\\overline{Y}} \\right)^2 =\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\overline{Y_i} \\right)^2 +\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( \\overline{Y_i} -\n                                     \\overline{\\overline{Y}} \\right)^2\n\\]\nThe squared sums above are referred to as: \\[\nSS_T = SS_W + SS_B\n\\]\n\n\\(SS_T\\): Sum of Squares Total,\n\\(SS_W\\): Sum of Squares Within, and\n\\(SS_B\\): Sum of Squares Between.\n\nIn addition the following definitions are important for understanding the ANOVA output:\n\nThe Between Mean Square: \\[\nMS_B = \\frac{SS_B}{k-1}\n\\]\nThe Within Mean Square: \\[\nMS_W = \\frac{SS_W}{n - k}\n\\]\n\nThe mean squares are estimates of the variability between and within groups. The ratio of these quantities is the test statistic.\n\n\n\\(F\\)-Test in One-Way ANOVA\nThe null and alternative hypotheses are:\n\\[\\begin{eqnarray*}\nH_0 &:& \\alpha_i = 0 \\text{ for all } i \\\\\nH_1 &:& \\alpha_i \\ne 0 \\text{ for at least one } i\n\\end{eqnarray*}\\]\nThe test statistic is given by \\[\nF = \\frac{MS_B}{MS_W}\n\\]\nUnder \\(H_0\\), the statistic \\(F\\) follows an \\(F\\) distribution with \\(k-1\\) and \\(n-k\\) degrees of freedom.\n\n\nAssumptions\nThese are the assumptions that will need to be validated.\n\nThe observations are independent of each other. This is usually a characteristic of the design of the experiment, and is not something we can always check from the data.\nThe errors are Normally distributed. Residuals can be calculated as follows: \\[\nY_{ij} - \\overline{Y_i}\n\\] The distribution of these residuals should be checked for Normality.\nThe variance within each group is the same. In ANOVA, the \\(MS_W\\) is a pooled estimate (across the groups) that is used; in order for this to be valid, the variance within each group should be identical. As in the 2-sample situation, we shall avoid separate hypotheses tests and proceed with the rule-of-thumb that if the ratio of the largest to smallest standard deviation is less than 2, we can proceed with the analysis.\n\n\nExample 2.7 (Example: Heifers (Cont’d)) \nLet us now apply the F-test to the heifers dataset.\n\nheifer_lm = ols('org ~ type', data=heifers).fit()\nanova_tab = sm.stats.anova_lm(heifer_lm, type=3,)\nanova_tab\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\ntype\n5.0\n0.590824\n0.118165\n7.972558\n0.00009\n\n\nResidual\n28.0\n0.415000\n0.014821\nNaN\nNaN\n\n\n\n\n\n\n\nAt the 5% significance level, we reject the null hypothesis to conclude that the group means are significantly different from one another. This answers the first question we set out to.\nTo extract the estimated parameters, we can use the following code:\n\nheifer_lm.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\norg\nR-squared:\n0.587\n\n\nModel:\nOLS\nAdj. R-squared:\n0.514\n\n\nMethod:\nLeast Squares\nF-statistic:\n7.973\n\n\nDate:\nTue, 23 Sep 2025\nProb (F-statistic):\n8.95e-05\n\n\nTime:\n09:05:59\nLog-Likelihood:\n26.655\n\n\nNo. Observations:\n34\nAIC:\n-41.31\n\n\nDf Residuals:\n28\nBIC:\n-32.15\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.8950\n0.050\n58.248\n0.000\n2.793\n2.997\n\n\ntype[T.Control]\n-0.2917\n0.070\n-4.150\n0.000\n-0.436\n-0.148\n\n\ntype[T.Enroflox]\n-0.1850\n0.070\n-2.632\n0.014\n-0.329\n-0.041\n\n\ntype[T.Fenbenda]\n-0.0617\n0.070\n-0.877\n0.388\n-0.206\n0.082\n\n\ntype[T.Ivermect]\n0.1067\n0.070\n1.518\n0.140\n-0.037\n0.251\n\n\ntype[T.Spiramyc]\n-0.0400\n0.079\n-0.509\n0.615\n-0.201\n0.121\n\n\n\n\n\n\n\n\nOmnibus:\n2.172\nDurbin-Watson:\n2.146\n\n\nProb(Omnibus):\n0.338\nJarque-Bera (JB):\n1.704\n\n\nSkew:\n-0.545\nProb(JB):\n0.427\n\n\nKurtosis:\n2.876\nCond. No.\n6.71\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nWhen estimating, Python sets one of the \\(\\alpha_i\\) to be equal to 0. We can tell from the output that the constraint has been placed on the coefficient for Alfacyp (since it is missing).\nFrom the output, we can read off (the Intercept term) that the estimate for Alfacyp is precisely \\[\n2.895 + 0 = 2.895\n\\]\nTo check the assumptions, we can use the following code:\n\nf, axs = plt.subplots(1, 2, figsize=(10,4))\ntmp = plt.subplot(121)\nheifer_lm.resid.hist();\ntmp = plt.subplot(122)\nsm.qqplot(heifer_lm.resid, line=\"q\", ax=tmp);\n\n\n\n\n\n\n\n\n\n\n\nComparing specific groups\nThe \\(F\\)-test in a One-Way ANOVA indicates if all means are equal, but does not provide further insight into which particular groups differ. If we had specified beforehand that we wished to test if two particular groups \\(i_1\\) and \\(i_2\\) had different means, we could do so with a t-test. Here are the details to compute a confidence interval in this case:\n\nCompute the estimate of the difference between the two means: \\[\n\\overline{Y_{i_1}} - \\overline{Y_{i_2}}\n\\]\nCompute the standard error of the above estimator: \\[\n\\sqrt{MS_W \\left( \\frac{1}{n_{i_1}} + \\frac{1}{n_{i_2}} \\right) }\n\\]\nCompute the \\(100(1- \\alpha)%\\) confidence interval as: \\[\n\\overline{Y_{i_1}} - \\overline{Y_{i_2}} \\pm\nt_{n-k, \\alpha/2}  \\times\n\\sqrt{MS_W \\left( \\frac{1}{n_{i_1}} + \\frac{1}{n_{i_2}} \\right) }\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIf you notice from the summary statistics output for each group, the rule-of-thumb regarding standard deviations has not been satisfied. The ratio of largest to smallest standard deviations is slightly more than 2. Hence we should not continue with ANOVA; the pooled estimate of the variance may not be valid.\nHowever, we shall proceed with this dataset just to demonstrate the next few techniques, instead of introducing a new dataset.\n\n\n\nExample 2.8 (Example: Enrofloxacin vs. Control) \nLet us attempt to answer question (2), that we had set out earlier.\n\nest1  = heifer_lm.params.iloc[2] - heifer_lm.params.iloc[1]\nMSW = heifer_lm.mse_resid\ndf = heifer_lm.df_resid\nq1 = -stats.t.ppf(0.025, df)\n\nlower_ci = est1 - q1*np.sqrt(MSW * (1/6 + 1/4))\nupper_ci = est1 + q1*np.sqrt(MSW * (1/6 + 1/4))\nprint(\"The 95% CI for the diff. between Enrofloxacin and control is\" +\n      f\"({lower_ci:.3f}, {upper_ci:.3f}).\") \n\nThe 95% CI for the diff. between Enrofloxacin and control is(-0.054, 0.268).\n\n\nAs the confidence interval contains the value 0, the binary conclusion would be to not reject the null hypothesis at the 5% level.\n\n\n\nContrast Estimation\nA more general comparison, such as the comparison of a collection of \\(l_1\\) groups with another collection of \\(l_2\\) groups, is also possible. First, note that a linear contrast is any linear combination of the individual group means such that the linear coefficients add up to 0. In other words, consider \\(L\\) such that\n\\[\nL = \\sum_{i=1}^k c_i \\overline{Y_i}, \\text{ where } \\sum_{i=1}^k c_i = 0\n\\]\nNote that the comparison of two groups in Section 2.4.4 is a special case of this linear contrast.\nHere is the procedure for computing confidence intervals for a linear contrast:\n\nCompute the estimate of the contrast: \\[\nL = \\sum_{i=1}^k c_i \\overline{Y_i}\n\\]\nCompute the standard error of the above estimator: \\[\n\\sqrt{MS_W \\sum_{i=1}^k \\frac{c_i^2}{n_i} }\n\\]\nCompute the \\(100(1- \\alpha)%\\) confidence interval as: \\[\nL \\pm\nt_{n-k, \\alpha/2}  \\times\n\\sqrt{MS_W \\sum_{i=1}^k \\frac{c_i^2}{n_i} }\n\\]\n\n\nExample 2.9 (Example: Enrofloxacin vs. Control) \nAs we mentioned earliers, let sub-group 1 consist of Ivermectin and Fenbendazole. Here is how we can compute a confidence interval for the difference between this sub-group, and Enrofloxacin.\n\nc1 = np.array([-1, 0.5, 0.5])\nn_vals = np.array([6, 6, 6,])\nL = np.sum(c1 * heifer_lm.params.iloc[2:5])\n\nMSW = heifer_lm.mse_resid\ndf = heifer_lm.df_resid\nq1 = -stats.t.ppf(0.025, df)\nse1 = np.sqrt(MSW*np.sum(c1**2 / n_vals))\n\nlower_ci = L - q1*se1\nupper_ci = L + q1*se1\nprint(\"The 95% CI for the diff. between the two groups is \" +\n      f\"({lower_ci:.3f}, {upper_ci:.3f}).\") \n\nThe 95% CI for the diff. between the two groups is (0.083, 0.332).\n\n\n\n\n\nMultiple Comparisons\nThe procedures in the previous two subsections correspond to contrasts that we had specified before collecting or studying the data. If, instead, we wished to perform particular comparisons after studying the group means, or if we wish to compute all pairwise contrasts, then we need to adjust for the fact that we are conducting multiple tests. If we do not do so, the chance of making at least one false positive increases greatly.\n\nBonferroni\nThe simplest method for correcting for multiple comparisons is to use the Bonferroni correction. Suppose we wish to perform \\(m\\) pairwise comparisons, either as a test or by computing confidence intervals. If we wish to maintain the significance level of each test at \\(\\alpha\\), then we should perform each of the \\(m\\) tests/confidence intervals at \\(\\alpha/m\\).\n\n\nTukeyHSD\nThis procedure is known as Tukey’s Honestly Significant Difference. It is designed to construct confidence intervals for all pairwise comparisons. For the same \\(\\alpha\\)-level, Tukey’s HSD method provides shorter confidence intervals than a Bonferroni correction for all pairwise comparisons.\n\nExample 2.10 (Example: Multiple Comparisons) \nLet us apply Tukey’s procedure to the heifers dataset.\n\ncp = mc.MultiComparison(heifers.org, heifers.type)\ntk = cp.tukeyhsd()\n#print(tk)\n\n\ntk.plot_simultaneous();",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#categorical-variables",
    "href": "02-inference.html#categorical-variables",
    "title": "2  Statistical Inference",
    "section": "2.5 Categorical Variables",
    "text": "2.5 Categorical Variables\nThere are two sub-types of categorical variables:\n\nA categorical variable is ordinal if the observations can be ordered, but do not have specific quantitative values.\nA categorical variable is nominal if the observations can be classified into categories, but the categories have no specific ordering.\n\nIn this topic, we shall discuss techniques for identifying the presence, and for measuring the strength, of the association between two categorical variables.\n\nExample 2.11 (Example: Chest Pain and Gender) \nSuppose that 1073 NUH patients who were at high risk for cardiovascular disease (CVD) were randomly sampled. They were then queried on two things:\n\nHad they experienced the onset of severe chest pain in the preceding 6 months? (yes/no)\nWhat was their gender? (male/female)\n\nThe data would probably be summarised and presented in this format, which is known as a contingency table.\n\nchest_array = np.array([[46, 474], [37, 516]])\n#print(chest_array)\n\nsns.heatmap(chest_array, annot=True, square=True, fmt='', \n            xticklabels=['pain', 'no pain'],\n            yticklabels=['Male', 'Female'], \n            cmap='Reds', cbar=False, );\n\n\n\n\n\n\n\n\nIn the above table, the rows correspond to gender (Male, followed by Female) and the columns correspond to chest pain (from left to right: presence, then absence). Hence 516 of the 1073 patients were females who did not experience chest pain.\nIn a contingency table, each observation from the dataset falls in exactly one of the cells. The sum of all entries in the cells equals the number of independent observations in the dataset. All the techniques we shall touch upon in this chapter are applicable to contingency tables.\n\n\n\\(\\chi^2\\)-Test for Independence\nIn the contingency table above, the two categorical variables are Gender and Presence/absence of Pain. With contingency tables, the main inferential task usually relates to assessing the association between the two categorical variables.\n\n\n\n\n\n\nNote\n\n\n\nIf two categorical variables are independent, then the joint distribution of the variables would be equal to the product of the marginals. If two variables are not independent, we say that they are associated.\n\n\nThe \\(\\chi^2\\)-test uses the definition above to assess if two variables in a contingency table are associated. The null and alternative hypotheses are\n\\[\\begin{eqnarray*}\nH_0 &:& \\text{The two variables are indepdendent.}  \\\\\nH_1 &:& \\text{The two variables are not indepdendent.}\n\\end{eqnarray*}\\]\nUnder the null hypothesis, we can estimate the joint distribution from the observed marginal counts. Based on this estimated joint distribution, we then compute expected counts (which may not be integers) for each cell. The test statistic essentially compares the deviation of observed cell counts from the expected cell counts.\n\nExample 2.12 (Example: Chest Pain and Gender Expected Counts) \nContinuing from the chest pain example, we can compute the estimated marginals using row and column proportions\n\ncol_prop,row_prop = sm.stats.Table(chest_array).marginal_probabilities\n\nprint(f\"The proportion of males in the sample was {col_prop[0]:.3f}.\")\nprint(\"The proportion of all patients who experienced chest pain \" +\n      f\"was {row_prop[0]:.3f}.\")\n\nThe proportion of males in the sample was 0.485.\nThe proportion of all patients who experienced chest pain was 0.077.\n\n\nIf we let \\(X\\) represent gender and \\(Y\\) represent chest pain, then we can estimate that:\n\\[\\begin{eqnarray*}\n\\widehat{P}(X = \\text{male}) &=& 0.485  \\\\\n\\widehat{P}(Y = \\text{pain}) &=& 0.077\n\\end{eqnarray*}\\]\nConsequently, under \\(H_0\\) (independence), we would estimate \\[\n\\widehat{P}(X = \\text{male},\\, Y= \\text{pain}) = 0.485 \\times 0.077 \\approx 0.04\n\\]\nFrom a sample of size 1073, the expected count for this cell is then\n\\[\n0.04 \\times 1073 = 42.92\n\\]\n\nUsing the approach above, we can derive a general formula for the expected count in each cell: \\[\n\\text{Expected count} = \\frac{\\text{Row total} \\times \\text{Column total}}{\\text{Total sample size}}\n\\]\nThe formula for the \\(\\chi^2\\)-test statistic (with continuity correction) is: \\[\n\\chi^2 = \\sum \\frac{|\\text{expected} - \\text{observed} |^2}{\\text{expected count}}\n\\]\nThe sum is taken over every cell in the table. Hence in a \\(2\\times2\\) table, as we have here, there would be 4 terms in the summation.\n\nExample 2.13 (Example: Chest Pain and Gender \\(\\chi^2\\) Test) \nLet us see how we can apply and interpret the \\(\\chi^2\\)-test for the data in the chest pain example.\n\nchisq_output = stats.chi2_contingency(chest_array, correction=False)\n\nprint(f\"The p-value is {chisq_output.pvalue:.3f}.\")\nprint(f\"The test-statistic value is {chisq_output.statistic:.3f}.\")\n\nThe p-value is 0.187.\nThe test-statistic value is 1.744.\n\n\nSince the \\(p\\)-value is 0.1817, we would not reject the null hypothesis at significance level 5%. We do not have sufficient evidence to conclude that the variables are not independent.\nTo extract the expected cell counts, we can use the following code:\n\nchisq_output.expected_freq\n\narray([[ 40.22367195, 479.77632805],\n       [ 42.77632805, 510.22367195]])\n\n\nThe test statistic in the \\(\\chi^2\\)-test compares the above table to the observed table.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is only suitable to use the \\(\\chi^2\\)-test when all expected cell counts are larger than 5. If this condition fails, one recommendation is to use a continuity correction, which modifies the test statistic to be\n\\[\n\\chi^2 = \\sum \\frac{(|\\text{expected} - \\text{observed}| - 0.5)^2}{\\text{expected count}}\n\\]\nHowever, I advocate using Fisher’s exact test, which is also a test of independence, but measures deviations under slightly different assumptions. The test uses the exact distribution of the test statistic (not an approximation) so it will apply to small datasets as well.\n\n\n\nstats.fisher_exact(chest_array)\n\nSignificanceResult(statistic=1.3534040369483407, pvalue=0.2088776906675503)\n\n\n\n\nMeasures of Association\nMeasures of association quantify how two random variables vary together. When both variables are continuous, a common measure that is used is Pearson correlation. We shall return to this in topics such as regression. For now, we touch on bivariate measures of association for contingency tables (two categorical variables).\n\n\nOdds Ratio\nThe most generally applicable measure of association, for 2x2 tables with nominal variables, is the Odds Ratio (OR). Suppose we have \\(X\\) and \\(Y\\) to be Bernoulli random variables with (population) success probabilities \\(p_1\\) and \\(p_2\\).\nWe define the odds of success for \\(X\\) to be \\[\n\\frac{p_1}{1-p_1}\n\\] Similarly, the odds of success for random variable \\(Y\\) is \\(\\frac{p_2}{1-p_2}\\).\nIn order to measure the strength of their association, we use the odds ratio: \\[\n\\frac{p_1/ (1-p_1)}{p_2/(1-p_2)}\n\\]\nThe odds ratio can take on any value from 0 to \\(\\infty\\).\n\nA value of 1 indicates no association between \\(X\\) and \\(Y\\). If \\(X\\) and \\(Y\\) were independent, this is what we would observe.\nDeviations from 1 indicate stronger association between the variables.\nNote that deviations from 1 are not symmetric. For a given pair of variables, an association of 0.25 or 4 is the same - it is just a matter of which variable we put in the numerator odds.\n\nDue to the above asymmetry, we often use the log-odds-ratio instead: \\[\n\\log \\frac{p_1/ (1-p_1)}{p_2/(1-p_2)}\n\\]\n\nLog-odds-ratios can take values from \\(-\\infty\\) to \\(\\infty\\).\nA value of 0 indicates no association between \\(X\\) and \\(Y\\).\nDeviations from 0 indicate stronger association between the variables, and deviations are now symmetric; a log-odds-ratio of -0.2 indicates the same strength as 0.2, just the opposite direction.\n\nTo obtain a confidence interval for the odds-ratio, we work with the log-odds ratio and then exponentiate the resulting interval. Here are the steps:\n\nThe sample data in a 2x2 table can be labelled as \\(n_{11}, n_{12}, n_{21}, n_{22}\\).\nThe sample odds ratio is \\[\n\\widehat{OR} = \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}}\n\\]\nFor a large sample size, it can be shown that \\(\\log \\widehat{OR}\\) follows a Normal distribution. Hence a 95% confidence interval can be obtained through \\[\n\\log \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}} \\pm z_{0.025}\n\\times ASE(\\log \\widehat{OR})\n\\]\n\nwhere\n\nthe ASE (Asymptotic Standard Error) of the estimator is \\[\n\\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}}\n\\]\n\n\nExample 2.14 (Example: Chest Pain and Gender Odds Ratio) \nLet us compute the confidence interval for the odds ratio in the chest pain and gender example from earlier.\n\nchest_tab2 = sm.stats.Table2x2(chest_array)\n\nchest_tab2.summary()\n\n\n\n\n\nEstimate\nSE\nLCB\nUCB\np-value\n\n\nOdds ratio\n1.353\n\n0.863\n2.123\n0.188\n\n\nLog odds ratio\n0.303\n0.230\n-0.148\n0.753\n0.188\n\n\nRisk ratio\n1.322\n\n0.872\n2.004\n0.188\n\n\nLog risk ratio\n0.279\n0.212\n-0.137\n0.695\n0.188\n\n\n\n\n\n\n\n\nFor Ordinal Variables\nWhen both variables are ordinal, it is often useful to compute the strength (or lack) of any monotone trend association. It allows us to assess if\n\nAs the level of \\(X\\) increases, responses on \\(Y\\) tend to increase toward higher levels, or responses on \\(Y\\) tend to decrease towards lower levels.\n\nFor instance, perhaps job satisfaction tends to increase as income does. In this section, we shall discuss a measure for ordinal variables, analogous to Pearson’s correlation for quantitative variables, that describes the degree to which the relationship is monotone. It is based on the idea of a concordant or discordant pair of subjects.\n\nA pair of subjects is concordant if the subject ranked higher on \\(X\\) also ranks higher on \\(Y\\).\nA pair is discordant if the subject ranking higher on \\(X\\) ranks lower on \\(Y\\).\nA pair is tied if the subjects have the same classification on \\(X\\) and/or \\(Y\\).\n\nIf we let\n\n\\(C\\): number of concordant pairs in a dataset, and\n\\(D\\): number of discordant pairs in a dataset.\n\nThen if \\(C\\) is much larger than \\(D\\), we would have reason to believe that there is a strong positive association between the two variables. Here are two measures of association based on \\(C\\) and \\(D\\):\n\nGoodman-Kruskal \\(\\gamma\\) is computed as \\[\n\\gamma = \\frac{C - D}{C + D}\n\\]\nKendall \\(\\tau_b\\) is \\[\n\\tau_b = \\frac{C - D}{A}\n\\] where \\(A\\) is a normalising constant that results in a measure that works better with ties, and is less sensitive than \\(\\gamma\\) to the cut-points defining the categories. \\(\\gamma\\) has the advantage that it is more easily interpretable.\n\nFor both measures, values close to 0 indicate a very weak trend, while values close to 1 (or -1) indicate a strong positive (negative) association.\n\nExample 2.15 (Example: Job Satisfaction by Income) \nConsider the following table, obtained from Agresti (2012). The original data come from a nationwide survey conducted in the US in 1996.\n\nus_svy_tab = np.array([[1, 3, 10, 6], \n                      [2, 3, 10, 7],\n                      [1, 6, 14, 12],\n                      [0, 1,  9, 11]])\ncol_names = ['V. Diss', 'L. Diss', 'M. Sat', 'V. Sat']\nrow_names = ['&lt;15K', '15-25K', '25-40K', '&gt;40K']\nsns.heatmap(us_svy_tab, annot=True, square=True, fmt='', \n            xticklabels=col_names,\n            yticklabels=row_names, \n            cmap='Reds', cbar=False, );\n\n\n\n\n\n\n\n\nIn measuring the association between these two variables (job satisfaction and income), we are interested in answering questions such as:\n\nIf individual A has higher income than individual B, is individual A more likely to be satisfied in his/her job?\n\n\\(\\gamma\\) and \\(\\tau_b\\) are measures that quantify this association. For the function in scipy.stats that computes this association, we need the data in “long format”. Hence we unroll it manually before summoning the method.\n\ndim1 = us_svy_tab.shape\nx = []; y=[]\nfor i in range(0, dim1[0]):\n    for j in range(0, dim1[1]):\n        for k in range(0, us_svy_tab[i,j]):\n            x.append(i)\n            y.append(j)\n\n\nstats.kendalltau(x, y, variant='b')\n\nSignificanceResult(statistic=0.15235215134659688, pvalue=0.0860294855671433)\n\n\nThe output shows that \\(\\tau_b =0.15\\) is positive, and is borderline significant at 5% level. We can conclude that there is a weak association between job satisfaction and income.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#summary",
    "href": "02-inference.html#summary",
    "title": "2  Statistical Inference",
    "section": "2.6 Summary",
    "text": "2.6 Summary\nIn this topic, the primary take-aways are the notions of hypothesis tests (HT) and confidence intervals (CI). Both of these approaches aim to uncover information about a population using a sample. I strongly advocate the choice of CI over HT, as the latter option only provides a binary decision. CIs, on the other hand, provide a range of values to provide a better understanding of the situation.\nUsing a small set of applications, we have demonstrated these techniques. We covered a common scenario where a researcher may need to compare the means between several groups. We then moved onto another common situation, where one might be faced with a contingency table. A common query is: What should we do if the assumptions of the test are not fulfilled? In those circumstances, one possibility is to turn to nonparametric versions of the test. For instance, the Kruskal-Wallis test is a HT for comparing the means of several groups whose distributions are not Normal.\nIn the section on categorical data, we introduced measures of association for categorical variables - odds ratio, and Kendall \\(\\tau\\). When we have contingency tables, a common choice has been to display barcharts, or heat maps and use those to make decisions on. However, I encourage you to use the measures of association instead. They are intuitive to understand, and can allow you to compare sub-groups or variation of association over time.\nWith HT and with CI, there has been a tendency to manipulate the data or tests until the desired outcome has been reached. Do be watchful of this. Conducting multiple tests increases the false positive error rate, so please do avoid this as well. If you conduct multiple tests, adjust for multiple testing. Use statitsical inference techniques as a guide together with common sense and domain expertise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "02-inference.html#sec-02-ref",
    "href": "02-inference.html#sec-02-ref",
    "title": "2  Statistical Inference",
    "section": "2.7 References",
    "text": "2.7 References\n\nWebsite References\n\nInference recap from Penn State:\n\nHypothesis testing recap\nConfidence intervals recap\n\nTests for Normality More information on the Kolmogorov-Smirnov and Shapiro-Wilks Tests for Normality.\nOverview of \\(t\\)-tests This page includes the rule of thumb about deciding when to use the equal variance test, and when to use the unequal variances version.\n\n\n\nDocumentation links\n\nstatsmodels ANOVA A more complete example on the application of ANOVA. Return to this when we complete the topic on regression.\n\nUseful functions from scipy.stats: Under the “Hypothesis Tests” section of this page, you can find:\n\nKruskal Wallis - the nonparametric equivalent of ANOVA\nWilcoxon signed rank - the nonparametric equivalent of paired sample t-test\nMann Whitney test - the nonparametric equivalent of independent samples t test\nSomer’s D - measure of association for two categorical variables (one ordinal and one nominal).\n\n\n\n\n\n\nAgresti, Alan. 2012. Categorical Data Analysis. Vol. 792. John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Inference</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html",
    "href": "03-unsupervised.html",
    "title": "3  Unsupervised Learning",
    "section": "",
    "text": "3.1 Introduction\nSuppose that we have a set of \\(N\\) observations \\((x_1, x_2, \\ldots, x_N)\\) of a random \\(p\\)-vector \\(X\\). The goal in unsupervised learning is to infer properties of the probability density of \\(X\\). Note the primary difference with supervised learning - in that context, we had a set of labels \\(y_1,\\ldots, y_N\\) in addition to the \\(x_i\\)’s. Here, we do not have labelled data.\nIn situations where \\(x_i\\) is vector of 3 or less, then graphical methods and numerical summaries such as correlations will suffice to help us understand the structure of the data. However, these methods breakdown as soon as \\(p\\) increases beyond 3. This topic introduces techniques that we can use, even when \\(p\\) is large, to :\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom itables import show\nfrom ind5003 import clust\nimport folium\nimport geopandas\n\nfrom sklearn import decomposition, preprocessing\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.manifold import MDS, TSNE\nfrom sklearn.ensemble import IsolationForest\n\nfrom scipy.cluster import hierarchy\n\nfrom sentence_transformers import SentenceTransformer",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html#introduction",
    "href": "03-unsupervised.html#introduction",
    "title": "3  Unsupervised Learning",
    "section": "",
    "text": "Understand and interpret the main sources of variation in the data,\nIdentify “groups” or clusters within the data for further study.\nVisualise high-dimensional data\n\n\n\nExample 3.1 (Wine Quality Data) \nThe UCI Machine Learning Repository contains a dataset on Wine Quality. It consists of two tables - one corresponding to white wine and one corresponding to red wine. Each table contains the following columns:\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality (score between 0 and 10)\n\nColumns 1 - 11 are numeric variables, measured objectively on the wines. Column 12 is a subjective evaluation made by wine experts, based on sensory data. Each quality score is the median of at least 3 evaluations. Although this dataset was created for a supervised learning problem, we shall use it to practice unsupervised learning techniques. To do so, we shall ignore the column corresponding to quality in most sections until the end, when we try to interpret the findings.\nOur first step is to read in the two tables and combine them into one.\n\nwine_red = pd.read_csv(\"data/wine+quality/winequality-red.csv\", \n                       delimiter=\";\" )\nwine_red['type'] = \"red\"\nwine_white = pd.read_csv(\"data/wine+quality/winequality-white.csv\", \n                         delimiter=\";\")\nwine_white['type'] = \"white\"\n\n# remove spaces in column names:\ncol_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n             'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide',\n             'density', 'pH', 'sulphates', 'alcohol', 'quality', 'type']\nwine2 = pd.concat([wine_red, wine_white], ignore_index=True)\nwine2.columns = col_names\n\nHere is a brief overview of the data.\n\nshow(wine2.head(20))\n\n\n\n    \n      \n      fixed_acidity\n      volatile_acidity\n      citric_acid\n      residual_sugar\n      chlorides\n      free_sulfur_dioxide\n      total_sulfur_dioxide\n      density\n      pH\n      sulphates\n      alcohol\n      quality\n      type\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html#principal-components-analysis",
    "href": "03-unsupervised.html#principal-components-analysis",
    "title": "3  Unsupervised Learning",
    "section": "3.2 Principal Components Analysis",
    "text": "3.2 Principal Components Analysis\nA Principal Components Analysis (PCA) explains the covariance matrix of a set of variables through a few linear combinations of these variables. The general objectives are\n\ndata reduction, into features that are uncorrelated with one another,\ninterpretation, and\nvisualisation.\n\n\nFormal Set-up\nSuppose that we have \\(N\\) observations of a random vector of length \\(p\\). We can represent these values in a matrix with \\(N\\) rows and \\(p\\) columns:\n\\[\n\\mathbf{X}_{N\\times p} =\n\\begin{bmatrix}\nx_{1,1} & x_{1,2} & \\ldots & x_{1,p}\\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nx_{N,1} & x_{N,2} & \\ldots & x_{N,p}\n\\end{bmatrix}\n\\]\nLet \\(\\mathbf{x}_j = \\begin{bmatrix} x_{1,j} & x_{2,j} & \\cdots & x_{N,j} \\end{bmatrix}^T\\) correspond to column \\(j\\) in \\(\\mathbf{X}\\), for \\(j=1,\\ldots,p\\). We represent the mean of column \\(j\\) with\n\\[\n\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N x_{i,j}\n\\]\nThe first step in a PCA is to compute the covariance matrix of the data:\n\\[\nS_{p\\times p} =\n\\begin{bmatrix}\ns^2_1 & s^2_{1,2} & \\ldots & s^2_{1,p}\\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\ns^2_{p,1} & s^2_{p,2} & \\ldots & s^2_p\n\\end{bmatrix}\n\\]\nwhere \\(s^2_{m,n} = \\frac{1}{N-1} \\sum_{k=1}^N (x_{k,m} - \\bar{x}_m)(x_{k,n} - \\bar{x}_n)\\) is the sample covariance between columns \\(m\\) and \\(n\\) of matrix \\(X\\), where \\(1 &lt; m,n &lt; p\\). A PCA analysis yields coefficients \\(a_i\\) such that:\n\\[\n\\mathbf{y}_i = a_{i,1} \\mathbf{x}_1 + a_{i,2} \\mathbf{x}_2 + \\cdots + a_{i,p} \\mathbf{x}_p, \\quad i = 1,\\ldots,p\n\\]\nIn other words, \\(\\mathbf{y}_i\\) is a column vector of length \\(N\\), formed from a linear combinations of the columns in the original \\(\\mathbf{X}\\) matrix. Each \\(\\mathbf{y}_i\\) is what we refer to as a principal component. From a symmetric \\(p \\times p\\) matrix, we can always compute \\(p\\) principal components, and these vectors will be uncorrelated with each other.\n\n\n\n\n\n\nNote\n\n\n\nHowever this does not help us much! We have not achieved any reduction!?\n\n\nThe value of PCA comes from the possibility that the first few principal components usually explain most of the variability in the data (\\(\\sum_i^p s^2_i\\)). The last few principal components typically explain little of the variability in the data. Indeed, there is some loss of information when we drop them, but the benefit is that we can (hopefully) focus on much fewer dimensions than the original \\(p\\) (which could be in the hundreds, even). Moreover, as these components will be uncorrelated by design, they can be used as features to solve any issues of multicollinearity in our data.\n\nExample 3.2 (Example: PCA on Wine Dataset) \nWhile it is possible to extract principal components using either the covariance matrix or the correlation matrix, using the latter avoids situations where the primary principal component is simply driven by the scale of one or more columns in the original dataset. Here, we scale the first 11 columns (exclude quality and type) so that each column has mean 0 and variance 1.\n\nX_raw = wine2.iloc[:, :-2]\nscaler = preprocessing.StandardScaler().fit(X_raw)\nX_scaled = scaler.transform(X_raw)\n\nIt is theoretically possible to extract 11 components from this \\(X\\) matrix. Let us proceed with that, and assess how many we should keep using a scree plot.\n\npca_full = decomposition.PCA(n_components=11)\npca_full.fit(X_scaled)\n\nPCA(n_components=11)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \n11\n\n\n\ncopy \nTrue\n\n\n\nwhiten \nFalse\n\n\n\nsvd_solver \n'auto'\n\n\n\ntol \n0.0\n\n\n\niterated_power \n'auto'\n\n\n\nn_oversamples \n10\n\n\n\npower_iteration_normalizer \n'auto'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\nA scree plots the variance explained by each subsequent principal component (on the \\(y\\)-axis) versus the order of the principal component. It indicates how much more value there is in including the subsequent principal component. Generally, we look for an “elbow” shape to inform us of how many to keep. From below, we would probably want to retain 4 or 5 principal components.\n\nPC_values = np.arange(pca_full.n_components_) + 1\nplt.plot(PC_values, pca_full.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained');\n\n\n\n\n\n\n\n\nTo find the amount of total variance explained, we can use the following command:\n\npca_full.explained_variance_ratio_.cumsum()\n\narray([0.2754426 , 0.50215406, 0.64364015, 0.73187216, 0.79731533,\n       0.85252548, 0.90008537, 0.94567722, 0.97631577, 0.99701538,\n       1.        ])\n\n\nIt appears that 5 components are enough to explain 79.7% of the variance. Let us try to take a look at the \\(a_{i,j}\\) coefficients matrix to interpret the principal components. This set of coefficients is also known as the loadings matrix.\n\npca = decomposition.PCA(n_components=5)\npca.fit(X_scaled)\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\nloading_matrix = pd.DataFrame(loadings, \n                              columns=['PC' + str(x+1) for x in range(0, 5)], \n                              index=col_names[:-2])\n\nIn social sciences especially, it is a convention to drop loadings that are smaller than 0.3 in absolute value and then to interpret the remaining coefficients as well as possible.\n\nloading_matrix2 = loading_matrix.copy()\nloading_matrix2[loading_matrix.abs()  &lt; 0.300] = 0.00\nloading_matrix2.round(3).style.background_gradient(cmap='coolwarm_r', \n                                                   vmin=-1, vmax=1)\n\n\n\n\n\n\n \nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\nfixed_acidity\n-0.416000\n0.531000\n0.542000\n0.000000\n0.000000\n\n\nvolatile_acidity\n-0.663000\n0.000000\n-0.383000\n0.000000\n0.000000\n\n\ncitric_acid\n0.000000\n0.000000\n0.737000\n0.000000\n0.000000\n\n\nresidual_sugar\n0.602000\n0.521000\n0.000000\n0.000000\n0.000000\n\n\nchlorides\n-0.505000\n0.498000\n0.000000\n0.000000\n0.521000\n\n\nfree_sulfur_dioxide\n0.750000\n0.000000\n0.000000\n0.352000\n0.000000\n\n\ntotal_sulfur_dioxide\n0.848000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\ndensity\n0.000000\n0.922000\n0.000000\n0.000000\n0.000000\n\n\npH\n-0.381000\n0.000000\n-0.568000\n0.408000\n-0.385000\n\n\nsulphates\n-0.512000\n0.303000\n0.000000\n0.631000\n0.000000\n\n\nalcohol\n0.000000\n-0.734000\n0.326000\n0.000000\n0.000000\n\n\n\n\n\nFrom above, if we focus on the top two principal components, we could interpret them as:\n\nA combination of sugar and sulphur dioxides contrasted against acidity, chlorides and sulphates.\nA contrast between density and alcohol.\n\nIn the following code, we apply the transformation to obtain the actual principal components (\\(\\mathbf{y}_j\\)’s).\n\nX_transformed = pca.transform(X_scaled)\nX_transformed_df = pd.DataFrame(X_transformed, \n                                columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\nX_transformed_df[['quality', 'type']] = wine2[['quality', 'type']]\n\nsns.relplot(data=X_transformed_df, x='PC1', y='PC2', col='quality', col_wrap= 3, \n            hue='type', marker='o', alpha=0.3, height=3, aspect=1.2);\n\n\n\n\n\n\n\n\nJudging from the plots, red wines tend to be lower on PC1. Can we tie this back to the columns in the data to understand the difference between white and red wines more?\nFor white wines, there seems to be a linear relation between PC1 and PC2. However, for red wines, the range of PC1 values is quite narrow, and the PC1 values do not appear to suggest what the PC2 values could be.\nFinally, neither PC1 nor PC2 appears to provide a clue on the the subjective quality of the wine.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html#clustering",
    "href": "03-unsupervised.html#clustering",
    "title": "3  Unsupervised Learning",
    "section": "3.3 Clustering",
    "text": "3.3 Clustering\nIn the previous section, the goal was to reduce the dimensionality of the dataset. In this section, our goal is to segment the data. By assigning individual observations into groups (or clusters) such that those within each group are “closely related”, we can gain an understanding of our data at a higher level.\nThere are many different clustering algorithms. You may have heard of K-means, a very popular one before. The one we are going to use here is very similar to it. It is known as agglomerative hierarchical clustering. Let’s take a look at how it works first.\n\nHierarchical Clustering\n\nDissimilarity Measures Between Individual Observations\nAs we mentioned earlier, cluster analysis tries to identify groups such that those within a group a “similar” to one another. In order to proceed, we need to formalise this idea of similarity/dissimilarity.\nAs before, suppose that we have \\(N\\) observations \\(x_1, x_2, x_3,\\ldots,x_N\\) and we wish to group them into \\(K\\) clusters. Each observation is typically a vector of \\(p\\) observations, so we may write \\(x_i = (x_{i,1}, x_{i,2}, \\ldots, x_{i,N})\\).\nMost clustering algorithms require a dissimilarity matrix as input, so we need function that can measure pairwise dissimilarity. One of the most common choices is the Euclidean distance (or rather the L2-norm) between \\(x_i\\) and \\(x_j\\):\n\\[\\begin{equation*}\nd(x_i,\\;x_j) = \\sqrt{  \\sum_{s=1}^p (x_{i,s} - x_{j,s})^2 }\n\\end{equation*}\\]\nAnother common choice is the \\(L1\\)-norm:\n\\[\\begin{equation*}\nd(x_i,\\;x_j) = \\sum_{s=1}^p |x_{i,s} - x_{j,s}|\n\\end{equation*}\\]\n\n\nDissimilarity Measures Between Clusters or Groups\nFor hierarchical clustering, we need to build on this choice of pairwise dissimilarity to obtain a measure of dissimilarity between groups. In other words, suppose we have two groups of points \\(G\\) and \\(H\\), with \\(N_G\\) and \\(N_H\\) points within them respectively. We wish to use the pairwise dissimilarity between points in \\(G\\) and \\(H\\), to compute a dissimilarity between \\(G\\) and \\(H\\). We call this the linkage method, and there are several options for this too:\n\nSingle linkage takes the intergroup dissimilarity to be that of the closest (least dissimilar) pair. \\[\\begin{equation*}\nd_S(G,H) = \\min_{i \\in G, j \\in H} d(x_i,\\, x_j)\n\\end{equation*}\\]\nComplete linkage takes the intergroup dissimilarity to be that of the furthest (most dissimilar) pair. \\[\\begin{equation*}\nd_C(G,H) = \\max_{i \\in G, j \\in H} d(x_i,\\, x_j)\n\\end{equation*}\\]\nAverage linkage utilises the average of all pairwise dissimilarities between the groups: \\[\\begin{equation*}\nd_A(G,H) = \\frac{1}{N_G N_H} \\sum_{i \\in G} \\sum_{j \\in H} d(x_i,x_j)\n\\end{equation*}\\]\nWard linkage uses a more complicated distance to minimise the variance within groups. It usually returns more compact clusters than the others. Suppose that group \\(G\\) was formed by merging groups \\(G_1\\) and \\(G_2\\). Then the Ward distance between groups is \\[\\begin{equation*}\nd_W(G,H) = \\sqrt{\\frac{|H| + |G_1|}{N_G + N_H}d_W(H,G_1)^2 + \\frac{|H| + |G_2|}{N_G + N_H}d_W(H,G_2)^2 + \\frac{H}{N_G +N_H}d_W(G_1,G_2)^2}\n\\end{equation*}\\]\n\nThe choice of linkage can affect the final clusters we obtain, so it is important to choose carefully based on the subject matter. Here is a plot from sklearn, demonstrating the impact of the linkage on the clusters in toy datasets.\n\n\n\nsklearn documentation\n\n\n\n\nAgglomerative Hierarchical Clustering Algorithm\nThe output of the algorithm is a hierarchical representation of the data, where clusters at each level of the hierarchy are created by merging clusters at the next lower level. At the lowest level, each cluster contains a single observation. At the highest level there is only one cluster containing all of the data.\nStarting at the bottom (with \\(N\\) clusters of singletons), we recursively merge a selected pair of clusters into a single cluster. This produces a grouping at the next higher level with one less cluster. The pair chosen for merging consist of the two groups with the smallest intergroup dissimilarity.\nAs you can tell, this algorithm does not require the number of clusters as an input. The final number of clusters can be based on a visualisation of this hierarchy of clusterings, through a dendrogram.\n\nX = np.array([[.25,.7], [.3, .8], [.7, .6]])\nfc_dict={'Stage 0': ['red', 'blue', 'green'], 'Stage 1':['red', 'red', 'green'], \n        'Stage 2':['red']*3}\n\nplt.figure(figsize=(10, 3))\nfor x,y in enumerate(fc_dict.items()):\n    plt.subplot(1,3,x+1);\n    plt.scatter(X[:,0], X[:,1], facecolor=y[1]);\n    plt.ylim(0.2,1); plt.xlim(0,1);\n    plt.title(y[0]);\n\n\n\n\n\n\n\n\nAs we can see the number of clusters changed from 3 to 2 and then to 1. Here is how we can visualise the hierarchy:\n\nlm0 = hierarchy.linkage(X)\nhierarchy.dendrogram(lm0,p=2)\nplt.title('A Dendrogram');\n\n\n\n\n\n\n\n\nThe dendrogram shows that points with index 0 and 1 (the closest two points) merge at a small vertical distance (height of green lines), but the group containing them merges with point 2 at a much higher vertical distance (blue line on the left). This shows that points 0 and 1 are less dissimilar to one another than they are (as a group) to point 2. In other words, the height of each node is proportional to the value of the intergroup dissimilarity between its two child nodes.\n\nExample 3.3 (Example: Hierarchical Clustering of Wine) \nNow we apply this technique to the scaled wine dataset.\n\nhc1 = hierarchy.linkage(X_transformed_df.iloc[:, :-2], method='ward')\n\nplt.figure(figsize=(12,4))\nhierarchy.dendrogram(hc1, p=4, truncate_mode='level');\n\n\n\n\n\n\n\n\n\nFrom the dendrogram alone, it appears plausible that we can break the original set of data points into two, maybe 3, groups.\n\n\n\n\n\n\nNote\n\n\n\nCan we come up with a formal method of determining the optimal number of clusters?\n\n\n\n\n\nDetermining the optimal number of clusters\nThe Silhouette coefficient summarises the within similarity to the between similarity using the following formula:\n\\[\\begin{equation*}\nS = \\frac{b-a}{\\max(a,b)}\n\\end{equation*}\\]\nwhere \\(b\\) is the average distance between an observation and a cluster that it is not a part of. On the other hand, \\(a\\) is the mean distance within a cluster. This coefficient takes values between -1 and 1, with values closer to 1 indicating a more optimal clustering.\n\nExample 3.4 (Example: Clustering Quality) \nHere are the silhouette scores from the two clusterings of the wine data.\n\nout = hierarchy.cut_tree(hc1, n_clusters=3).ravel() \nX_transformed_df['groups'] = out\n\nclust.compute_silhouette_scores(hc1, X_transformed_df.iloc[:, :-2], [2,3,4])\n\nThe silhouette coefficient values we are obtaining are not very good. However, out of the possible values we tried, \\(K=2\\) seems to be the best.\n\nout = hierarchy.cut_tree(hc1, n_clusters=2).ravel() \nX_transformed_df['groups'] = out\n\nsns.relplot(data=X_transformed_df, x='PC1', y='PC2', col='groups', \n            hue='type', marker='o', alpha=0.3, height=3, aspect=1.2);\n\n\n\n\n\n\n\n\nAs we can see, the groupings closely mirror the type of wine (red or white).\n\nwine2.type.groupby(X_transformed_df.groups).describe()\n\n\n\n\n\n\n\n\ncount\nunique\ntop\nfreq\n\n\ngroups\n\n\n\n\n\n\n\n\n0\n1684\n2\nred\n1549\n\n\n1\n4813\n2\nwhite\n4763\n\n\n\n\n\n\n\nHowever, notice that there are a number of white wines grouped as 0 (with most of the other reds). It would be interesting to study what qualities of these wines led to them being grouped with the reds.\nAlso, consider what value the PCA brought to this problem. Go back and re-run the clustering algorithm with the scaled but untransformed data. Does the quality of clustering differ?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html#outlier-detection",
    "href": "03-unsupervised.html#outlier-detection",
    "title": "3  Unsupervised Learning",
    "section": "3.4 Outlier Detection",
    "text": "3.4 Outlier Detection\nOne efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest object in sklearn “isolates” observations by:\n\nRandomly selecting features, and then randomly selecting a split value between the maximum and minimum values of the selected feature to form a decision tree.\n\nIn each tree, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\nRepeating step 1 to create a forest of trees.\n\nFor each observation, the average path length, over the forest of random trees, is a measure of how anomalous it is. Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n\nExample 3.5 (Example: Isolation Forest with Taiwan Dataset) \nLet us apply this technique to the Taiwan real estate dataset from the regression topic.\n\nre2 = pd.read_csv(\"data/taiwan_dataset.csv\")\nX_re = re2.loc[:, ['trans_date', 'house_age', 'dist_MRT', 'num_stores', \n                   'Xs', 'Ys', 'price']]\nX_re_scaled = preprocessing.StandardScaler().fit_transform(X_re)\n\nAfter reading in and scaling the data, we fit the IsolationForest estimator.\n\nclf = IsolationForest(max_samples=300, max_features=2, contamination=0.01, \n                      random_state=503)\nclf.fit(X_re_scaled)\n\nIsolationForest(contamination=0.01, max_features=2, max_samples=300,\n                random_state=503)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForest?Documentation for IsolationForestiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\nmax_samples \n300\n\n\n\ncontamination \n0.01\n\n\n\nmax_features \n2\n\n\n\nbootstrap \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n503\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\n\n            \n        \n    \n\n\nThe contamination factor is the proportion of outliers in the dataset. max_features corresponds to the number of features to be drawn for each base estimator, while max_samples is the number of samples (observations) to draw from the original dataset for each base estimator.\n\nid_outliers = pd.Series(clf.predict(X_re_scaled))\nid_outliers.value_counts()\n\n 1    409\n-1      5\nName: count, dtype: int64\n\n\nThere are 5 points that have been identified as outliers (coded as -1). As analysts, we should do our best to understand what property, or combination of features, led to this.\n\nre2['outliers'] = id_outliers\n\n\nshow(re2[['house_age', 'dist_MRT', 'num_stores', 'price', \n          'Xs', 'Ys']].groupby(re2.outliers).describe().T)\n\n\n\n    \n      \n      outliers\n      -1\n      1\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nHere is the same map of the data points, but with colours to code the outlier points.\n\nm = folium.Map(location=(45.5236, -122.6750))\ngdf = geopandas.GeoDataFrame(\n    re2, geometry=geopandas.points_from_xy(re2.X, re2.Y), crs=\"EPSG:3825\"\n)\n\ngdf.explore(\"outliers\", tiles=\"CartoDB positron\", \n            tooltip=\"outliers\", marker_type=\"circle\", \n            marker_kwds = {\"radius\": 50, \"fill\": True}, \n            legend_kwds = {\"caption\": \"Price\"})\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we drop the location parameters, would different points be identified as outliers?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html#visualisation",
    "href": "03-unsupervised.html#visualisation",
    "title": "3  Unsupervised Learning",
    "section": "3.5 Visualisation",
    "text": "3.5 Visualisation\n\nMDS\nMultidimensional Scaling is another technique for visualising high-dimensional data. Just like in hierarchical clustering, we begin with a square matrix consisting of all pairwise dissimilarities \\(d(x_i,x_j)\\) between our \\(N\\) high-dimensional vectors. With a choice of \\(k\\), we seek values \\(z_1, z_2, \\ldots, z_N \\in \\mathbb{R}^k\\) such that the following function is minimised:\n\\[\nS(z_1,\\ldots,z_N) = \\left[ \\sum_{i \\ne j} \\left( d(x_i,x_j) - ||z_i -\nz_j||\\right)^2 \\right]^{1/2}\n\\]\nMDS is not the same as Principal Component Analysis (PCA):\n\nPCA maximises variance, orthogonal to earlier components.\nPrincipal components are ordered; MDS are not.\nPrincipal components are linear combinations of the original vectors; MDS output is not.\n\nThe goal of MDS is to find a lower dimensional set of vectors whose pairwise Euclidean distances are as close as possible to the dissimilarity matrix of the original vectors.\n\nExample 3.6 (Example: MDS on Disease Symptoms) \nThe dataset disease.csv contains a list of symptoms that were reported for a set diseases. Each row in the dataframe corresponds to a particular disease, while each binary column indicates whether that particular symptom was frequently present for this disease.\n\ndisease = pd.read_csv(\"data/disease.csv\")\n\n\nshow(disease)\n\n\n\n    \n      \n      itching\n      skin_rash\n      nodal_skin_eruptions\n      dischromic_patches\n      continuous_sneezing\n      shivering\n      chills\n      watering_from_eyes\n      stomach_pain\n      acidity\n      ulcers_on_tongue\n      vomiting\n      cough\n      chest_pain\n      yellowish_skin\n      nausea\n      loss_of_appetite\n      abdominal_pain\n      yellowing_of_eyes\n      burning_micturition\n      spotting_urination\n      passage_of_gases\n      internal_itching\n      indigestion\n      muscle_wasting\n      patches_in_throat\n      high_fever\n      extra_marital_contacts\n      fatigue\n      weight_loss\n      restlessness\n      lethargy\n      irregular_sugar_level\n      blurred_and_distorted_vision\n      obesity\n      excessive_hunger\n      increased_appetite\n      polyuria\n      sunken_eyes\n      dehydration\n      diarrhoea\n      breathlessness\n      family_history\n      mucoid_sputum\n      headache\n      dizziness\n      loss_of_balance\n      lack_of_concentration\n      stiff_neck\n      depression\n      irritability\n      visual_disturbances\n      back_pain\n      weakness_in_limbs\n      neck_pain\n      weakness_of_one_body_side\n      altered_sensorium\n      dark_urine\n      sweating\n      muscle_pain\n      mild_fever\n      swelled_lymph_nodes\n      malaise\n      red_spots_over_body\n      joint_pain\n      pain_behind_the_eyes\n      constipation\n      toxic_look_(typhos)\n      belly_pain\n      yellow_urine\n      receiving_blood_transfusion\n      receiving_unsterile_injections\n      coma\n      stomach_bleeding\n      acute_liver_failure\n      swelling_of_stomach\n      distention_of_abdomen\n      history_of_alcohol_consumption\n      fluid_overload\n      phlegm\n      blood_in_sputum\n      throat_irritation\n      redness_of_eyes\n      sinus_pressure\n      runny_nose\n      congestion\n      loss_of_smell\n      fast_heart_rate\n      rusty_sputum\n      pain_during_bowel_movements\n      pain_in_anal_region\n      bloody_stool\n      irritation_in_anus\n      cramps\n      bruising\n      swollen_legs\n      swollen_blood_vessels\n      prominent_veins_on_calf\n      weight_gain\n      cold_hands_and_feets\n      mood_swings\n      puffy_face_and_eyes\n      enlarged_thyroid\n      brittle_nails\n      swollen_extremeties\n      abnormal_menstruation\n      muscle_weakness\n      anxiety\n      slurred_speech\n      palpitations\n      drying_and_tingling_lips\n      knee_pain\n      hip_joint_pain\n      swelling_joints\n      painful_walking\n      movement_stiffness\n      spinning_movements\n      unsteadiness\n      pus_filled_pimples\n      blackheads\n      scurring\n      bladder_discomfort\n      foul_smell_of_urine\n      continuous_feel_of_urine\n      skin_peeling\n      silver_like_dusting\n      small_dents_in_nails\n      inflammatory_nails\n      blister\n      red_sore_around_nose\n      yellow_crust_ooze\n      disease\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nOur goal is to visualise the 41 diseases - diseases with “similar” symptoms should be plotted “close” to one another. However, the data begets the natural question: How can we define dissimilarity between the symptom lists of two diseases?\nFor this purpose, we shall use the Jaccard similarity index. For two disease symptom sets \\(A\\) and \\(B\\), the Jaccard dissimilarity is defined to be\n\\[\nJ = 1 - \\frac{|A \\cap B}{|A \\cup B|}\n\\]\nIf there are no common symptoms between the two diseases, then the intersection between the two sets would be the empty set. In that situation, \\(J\\) would take on the maximum value of 1. If the two symptom lists are identical, then \\(J\\) takes on the smallest possible value of 0.\n\ndisease_names = disease.disease.to_list()\nsymptoms = disease.columns.to_list()[:-1]\n\nX = disease.iloc[:, 0:-1].to_numpy()\n\nsymptom_text = []\n\nfor i in range(0, X.shape[0]):\n    symptom_text.append(','.join([symptoms[x] for x in np.where(X[i] == 1)[0]]))\ndisease['symptom_text'] = symptom_text\n\n# pdist2 is 41x41\npdist2 = pairwise_distances(X!=0, metric='jaccard')\ndisease.loc[disease.disease.isin(['GERD', 'Heart attack']), 'symptom_text'].to_list()\n\n['stomach_pain,acidity,ulcers_on_tongue,vomiting,cough,chest_pain',\n 'vomiting,chest_pain,breathlessness,sweating']\n\n\nBased on the set of symptoms for GERD and Heart attack, we have the following calculation:\n\\[\nJ = 1 - \\frac{2}{8} = 0.75\n\\]\nNow we turn to the MDS transformation.\n\nembedding = MDS(n_components=2, normalized_stress='auto', dissimilarity='precomputed',\n                random_state=42, max_iter=500, verbose=0)\nX_transformed = embedding.fit_transform(pdist2)\nX_transformed_df = pd.DataFrame(X_transformed, columns=['X', 'Y'])\nX_transformed_df['disease'] = disease_names\n\nfig = px.scatter(X_transformed_df, x='X', y='Y', text='disease', hover_name=symptom_text,\n                 width=1024, height=960)\nfig.update_traces(textposition='top center')\n#fig.show()\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/manifold/_mds.py:677: FutureWarning:\n\nThe default value of `n_init` will change from 4 to 1 in 1.9.\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nTo verify if the plot makes intuitive sense, we should inspect the points that occur nearby to one another. Here are some example codes we can use:\n\ndisease.loc[disease.disease.isin(['Typhoid', 'Dengue', 'Malaria']),\n            'symptom_text'].to_list()\n# disease.loc[disease.disease.isin(['Common Cold', 'Pneumonia', 'Bronchial asthma']), \n#              'symptom_text'].to_list()\n# disease.loc[disease.disease.isin(['GERD', 'Heart attack', 'Drug reaction']),\n#             'symptom_text'].to_list()\n\n['chills,vomiting,nausea,high_fever,diarrhoea,headache,sweating,muscle_pain',\n 'skin_rash,chills,vomiting,nausea,loss_of_appetite,high_fever,fatigue,headache,back_pain,muscle_pain,malaise,red_spots_over_body,joint_pain,pain_behind_the_eyes',\n 'chills,vomiting,nausea,abdominal_pain,high_fever,fatigue,diarrhoea,headache,constipation,toxic_look_(typhos),belly_pain']\n\n\nIt is important to remember that above, we requested the MDS algorithm to return us \\(z\\)-coordinates in \\(\\mathbf{R}^2\\) - two-dimensional Euclidean space, that replicate the dissimilarity matrix from the higher-dimensional data. This was for convenience, since the two dimensional plane is easier to plot. One problem is that, due to information loss, it is possible that in 2D, two points appear near, but in fact, they may be far apart on a third dimension.\nWith plotly, we can make interactive 3d plots, which can go some way to alleviating this problem.\n\n\n\n\n\n\nWarning\n\n\n\nStay away from non-interactive 3d plots!\n\n\n\nExample 3.7 (Example: Disease Symptoms 3D-plot) \nHere is the 3-D version of the earlier plot:\n\nembedding3 = MDS(n_components=3, normalized_stress='auto', dissimilarity='precomputed',  \n                 random_state=42, max_iter=500, verbose=0)\nX_transformed3 = embedding3.fit_transform(pdist2)\nX_transformed3_df = pd.DataFrame(X_transformed3, columns=['X', 'Y', 'Z'])\nX_transformed3_df['disease'] = disease_names\n\nfig = px.scatter_3d(X_transformed3_df, x='X', y='Y', z='Z', text='disease', \n                     width=1024, height=960)\nfig.show()\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/manifold/_mds.py:677: FutureWarning:\n\nThe default value of `n_init` will change from 4 to 1 in 1.9.\n\n\n\n                            \n                                            \n\n\n\n\n\nt-SNE\nt-SNE is an fast, iterative algorithm for visualising high-dimensional data.\nOnce again, suppose we have \\(N\\) data points \\(x_i \\in \\mathbf{R}^p\\). We would like to choose \\(N\\) map points \\(y_i \\in \\mathbf{R}^2\\) to represent them. Here is how the algorithm works:\n\nCompute pairwise similarity between the data points, using a Gaussian (Normal) kernel.\nIteratively update map points so that their pairwise similarity is as close as possible to the original data points.\n\nThe innovation of these algorithm is that the similarity between map points is computed using a \\(t\\)-distribution instead of Gaussian. The \\(t\\)-distribution has fatter tails than the Normal. This ensures that data points that are not close in \\(\\mathbf{R}^D\\) are pushed apart in the map points space.\nThere are a couple of important parameters in this algorithm.\n\nPerplexity: This is a parameter that provides a guide on how many neighbours a point has. It is recommended to try different values between 5 and 50 and assess if results are meaningful and consistent.\nNumber of iterations: This is the number of adjustments to make to the map points before stopping. The difference between map point similarity and data point similarity is measured using Kullback-Leibler Divergence. When this no longer drops quickly, we can stop the t-SNE algorithm.\n\nAlthough you will find t-SNE used in many analyses, you should be aware of certain caveats when using it. Please take a look at the link in the references for plots that expound on the following points.\n\nt-SNE is for visualisation and exploration, not for clustering.\nDistances in the map space are not reflective of true distances between datapoints.\nt-SNE preserves small pairwise distances so that local relationships are preserved.\nIn the map space, distances between clusters (between far-away points) might not mean much (unlike MDS).\nMake sure you try with different perplexity values, and check that the algorithm has converged.\n\n\nExample 3.8 (Example: Twitter Dataset) \nThe UCI Machine Learning repository contains tweets pertaining to health news from more than 15 major news agencies in 2015. In this example, we are going to encode each BBC tweet in to a numerical vector of length 384. Then we are going to visualise it using t-SNE.\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n#sentences = [\"This is an example sentence. can we move on?\", \"Each sentence is converted\"]\n#embeddings = model.encode(sentences)\n#embeddings.shape\n\n\nbbchealth_df = pd.read_table('data/health+news+in+twitter/Health-Tweets/bbchealth.txt', \n                             delimiter='|', names=['id', 'datetime', 'tweet'])\nbbchealth_tweets = bbchealth_df.tweet\nprint(bbchealth_tweets[10])\n\nHave GP services got worse? http://bbc.in/1Ci5c22\n\n\n\nAbove, we have an example of a tweet. We are going to strip off the URL at the end before encoding each (short) sentence.\n\nt1 = bbchealth_tweets.str.replace(' http.*$', '', regex=True)\nt2 = t1.str.replace('^VIDEO:', '', regex=True)\nt2_l = t2.to_list()\n\nembeddings = model.encode(t2_l)\n\n\nExample 3.9 (Example: Twitter Dataset t-SNE Output) \nThe following code generates an interactive plot based on the t-SNE visualisation.\n\ntsne1 = TSNE(n_components=2, init=\"random\", perplexity=10, verbose=0, \n             random_state=43, max_iter=5000)\nX_transformed2 = tsne1.fit_transform(embeddings)\n\nThe plot is only visible in the HTML version of the textbook.\n\ndf2 = pd.DataFrame(X_transformed2, columns=['x','y'])\ndf2['labels'] = t2_l\n\nfig = px.scatter(df2, x='x', y='y', hover_name='labels',\n                 width=1024, height=960)\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "03-unsupervised.html#references",
    "href": "03-unsupervised.html#references",
    "title": "3  Unsupervised Learning",
    "section": "3.6 References",
    "text": "3.6 References\n\nWebsite references\n\nClustering performance evaluation There are many other ways of assessing what the optimal number of clusters should be.\nMore information on isolation forests\nWikipedia entry on Jaccard Similarity (or index): This page introduces variants of the index, which may be relevant when one has counts of the number of times each item appears in the set.\nCaveats when using t-SNE\nDataCamp on t-SNE: This tutorial consists of a worked-through example on a churn dataset. It includes a comparison with PCA.\n\n\n\nVideo references\n\nt-SNE, explained by Josh Starmer\nA general video on high-dimensional space: A nice explainer from Google.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "04-nlp.html",
    "href": "04-nlp.html",
    "title": "4  Natural Language Processing",
    "section": "",
    "text": "4.1 Introduction\nIn our world, Natural Language Processing (NLP) is used in several scenarios. For example,\nBut as we begin to explore Natural Language, we realise that it is an extremely difficult subject. Here are some specific points to note:\nimport numpy as np\nimport pandas as pd\n\nfrom itables import show\nfrom IPython.display import YouTubeVideo, display, HTML\nimport ipywidgets as widgets\nimport pprint\n\nimport gensim\nfrom gensim.parsing.preprocessing import *\nimport gensim.downloader as api\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn import manifold\n\nfrom transformers import pipeline\n\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nvideo = YouTubeVideo(\"NfN_gcjGoJo\", width=640, height=480)\ndisplay(video)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#introduction",
    "href": "04-nlp.html#introduction",
    "title": "4  Natural Language Processing",
    "section": "",
    "text": "phones and handheld computers support predictive text and handwriting recognition;\nweb search engines give access to information locked up in unstructured text;\nmachine translation allows us to understand texts written in languages that we do not know;\ntext analysis enables us to detect sentiment in tweets and blogs.\n\n\n\nSome words mean different things in different contexts, but us humans know which meaning is being used.\n\nHe served the dish.\n\nIn the following two sentences, the word “by” has different meanings:\n\nThe lost children were found by the lake.\nThe lost children were found by the search party.\n\nIn the following cases, we (humans) can resolve what “they” is referring to, but it is not easy to generate a simple rule that a computer can follow.\n\nThe thieves stole the paintings. They were subsequently recovered.\nThe thieves stole the paintings. They were subsequently arrested.\n\nHow can we get a computer to understand the following tweet?:\n\n“Wow. Great job st@rbuck’s. Best cup of coffee ever.”\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan you catch all three jokes in the movie clip below? 🤣",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#definitions",
    "href": "04-nlp.html#definitions",
    "title": "4  Natural Language Processing",
    "section": "4.2 Definitions",
    "text": "4.2 Definitions\nBefore we go on, it would be useful to establish some terminology:\n\nA corpus is a collection of documents.\n\nExamples are a group of movie reviews, a group of essays, a group of paragraphs, or just a group of tweets.\nPlural of corpus is corpora.\n\nA document is a single unit within a corpus.\n\nDepending on the context, examples are a single sentence, a single paragraph, or a single essay.\n\nTerms are the elements that make up the document. They could be individual words, bigrams or trigrams from the sentences. These are also sometimes referred to as tokens.\nThe vocabulary is the set of all terms in the corpus.\n\nConsider the sentence:\nI am watching television.\nThe process of splitting up the document into tokens is known as tokenisation. The result for the above sentence would be\n'I',  'am',  'watching', 'television', '.'\nHow we tokenize and pre-process things will affect our final results. We shall discuss this more in a minute.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#overview-of-applications",
    "href": "04-nlp.html#overview-of-applications",
    "title": "4  Natural Language Processing",
    "section": "4.3 Overview of Applications",
    "text": "4.3 Overview of Applications\nHere are some of the use-cases that we shall discuss:\n\nTopic Modeling: This is an unsupervised technique that allows us to identify the salient topics of a new document automatically. This could be useful in a customer feedback setting, because it would allow quick allocation or prioritisation of resources. This approach requires one to decide on the number of topics. It typically also requires some study of the topics in order to interpret and verify them.\nInformation Retrieval: This is also an unsupervised approach. If the new document can be considered a “query”, then this can be used to retrieve and prioritise documents that are relevant to the query.\nSentiment Analysis: Using a lexicon of words and their tagged sentiments, we can assess whether the sentiment in a document is mostly positive or negative.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#text-pre-processing",
    "href": "04-nlp.html#text-pre-processing",
    "title": "4  Natural Language Processing",
    "section": "4.4 Text Pre-processing",
    "text": "4.4 Text Pre-processing\n\nExample 4.1 (Example: Wine Reviews Dataset) \nA dataset containing wine reviews is accessible from Kaggle. We shall work with one of the csv files. It contains 130,000 rows, although some are duplicates.\n\nrng = np.random.default_rng(5001)\n\nwine_reviews = pd.read_csv(\"data/winemag-data-130k-v2.csv\", index_col=0)\nwine_reviews.drop_duplicates(inplace=True)\n\npp = pprint.PrettyPrinter(indent=4, compact=True,)\nfor x in rng.choice(wine_reviews.description, size=5):\n    pprint.pp(x)\n\n('Bell pepper and sharp red fruit aromas provide a shaky start, which is '\n 'followed by cranberry, tart cherry and other pointed flavors. The feel is '\n 'racy and tight, with gritty acids. Airing does improve it somewhat. Tasted '\n 'twice; this is a review of the better bottle. Cabernet, Merlot, Cab Franc '\n 'and Carmenère is the blend. From Brazil.')\n('Consistent with previous releases, this Michel Rolland effort is a soft, '\n 'silky, smoky wine that introduces itself with round cherry fruit and then '\n 'charges ahead with layers of licorice, citrus, coffee and rock that enliven '\n 'the finish. There is plenty of tart raspberry fruit to open, and the '\n \"balancing acids to give the wine a tight core. It's a very polished and \"\n 'appealing balance of forward, approachable fruit married to more elegant, '\n 'ageworthy tannins and acids.')\n(\"This is a light and soft selection, with an upfront gamy note that's framed \"\n 'by soft strawberry, rhubarb, red cherry and currant fruit tones on the nose '\n \"and mouth. Overall, it's short and direct; drink up.\")\n('Fresh, clean and easy, this would taste great at an outdoor pool party or '\n 'during a sunny lunch outdoors. It delivers fresh crispness, with lingering '\n 'tones of green apple and passion fruit.')\n('This easy-drinking wine has a bouquet of honeydew melon and lime juice. '\n 'Flavors of lemon, tangerine, guava and white peach with a soft hint of '\n 'baking spice continue into the finish, which is marked by flavors of stone '\n 'fruits and nutmeg.')\n\n\nThe “descriptions” column contains the review for a particular wine by a user, whose name and twitter handle are provided. Also included is information such as the price, originating county, region of the wine, and so on. In our activity, we are going to apply NLP techniques to the wine reviews.\n\n\nPre-processing Text with Gensim\nText documents consist of sentences of varying lengths. Usually, the first step to analysing a document is to break it up into pieces. This process is known as tokenizing. When tokenizing a document, we can do it at several levels of resolution: at the sentence, line, word or even punctuation level.\nTokenizing can easily done using the .split() within built-in python. But after that, we need to further pre-process the tokens.\nThe gensim package includes a module for pre-processing text strings. Here is a list of some of the functions there:\n\nstrip_multiple_whitespaces\nstrip_non_alphanum\nstrip_numeric\nstrip_punctuation\nstrip_short\n\nSince what we are about to do in the initial part of our activity is based on frequency counts of tokens, apart from some of the above steps, we are also going to remove common “filler” words that could end up skewing the eventual probability distributions of counts. These filler words are known as stop words. They were identified by linguists, and they vary from model to model, from Python package to package, and of course, from language to language.\nWhether stop-word removal is meaningful or not also depends on your particular application. At times, it is only done in order to speed up the training of a model. However, it is possible to change the entire meaning of a sentence by removing stop-words.\nFor us, we are going to apply this list of filters to each review:\n\nstrip_punctuation(),\nstrip_multiple_whitespaces(),\nstrip_numeric(),\nremove_stopwords(),\nstrip_short(),\nlemmatize()\n\nLemmatizing a word is to reduce it to its root word. You will come across stemming whenever you read about lemmatizing. In both cases, we wish to reduce a word to its root word so that we do not have to deal with multiple variations of a token, such as ate, eating, and eats.\nWhen we stem a word, the prefix and/or suffix will be removed according to a set of rules. Since it is primarily rule-based, the resulting word may not be an actual English word.\nLike stemming, lemmatizing also aims to reduce a word to its root form. However, it differs from stemming in that the final word must be a proper English language word. For this purpose, the algorithm has to be supplied with a lexicon or dictionary, along with the text to be lemmatized.\nHere is an example that demonstrates the differences.\n\nporter = PorterStemmer()\nwn = WordNetLemmatizer()\n\ndemo_sentence = 'Cats and ponies have a meeting'.split()\ndemo_sentence\n\n['Cats', 'and', 'ponies', 'have', 'a', 'meeting']\n\n\n\n#import nltk\n#nltk.download('wordnet')\n\n[porter.stem(x) for x in demo_sentence]\n\n['cat', 'and', 'poni', 'have', 'a', 'meet']\n\n\n\n[wn.lemmatize(x) for x in demo_sentence]\n\n['Cats', 'and', 'pony', 'have', 'a', 'meeting']\n\n\nNow let us go ahead and perform the pre-processing on the wine reviews.\n\nCUSTOM_FILTER = [lambda x: x.lower(), strip_punctuation, \n                 strip_multiple_whitespaces, strip_numeric, \n                 remove_stopwords, strip_short]\n#CUSTOM_FILTER[1]\nall_review_strings = wine_reviews.description.values\n#all_review_strings[:3]\nall_strings_tokenized = [preprocess_string(x, CUSTOM_FILTER) for x in all_review_strings]\n\nHere is an example of the pre-processed review:\n\npp.pprint(all_strings_tokenized[1])\n\n[   'ripe', 'fruity', 'wine', 'smooth', 'structured', 'firm', 'tannins',\n    'filled', 'juicy', 'red', 'berry', 'fruits', 'freshened', 'acidity',\n    'drinkable', 'certainly', 'better']\n\n\nAt this point in time, what we have is a list of lists. Each sub-list contains the tokens for a particular wine_review. For instance, the original review for row 234 was:\n\npp.pprint(wine_reviews.description.values[233])\npp.pprint(all_strings_tokenized[233])\n\n('There is an odd, piercing edge to the aromas, a mix of acetic acid and '\n \"pungent herb, with a hint of diesel. Somehow it's not off-putting, just \"\n 'atypical. The light, tart fruit is a mix of rhubarb and cranberry, very '\n 'earthy and tasting of dirt and bark in the finish. This could be quite '\n 'pleasant with a hearty, rustic dish such as beef Bourgogne.')\n[   'odd', 'piercing', 'edge', 'aromas', 'mix', 'acetic', 'acid', 'pungent',\n    'herb', 'hint', 'diesel', 'putting', 'atypical', 'light', 'tart', 'fruit',\n    'mix', 'rhubarb', 'cranberry', 'earthy', 'tasting', 'dirt', 'bark',\n    'finish', 'pleasant', 'hearty', 'rustic', 'dish', 'beef', 'bourgogne']\n\n\ngensim does not have a lemmatizer, so we use the nltk lemmatizer on each token.\n\npreprocessed_corpus = [[wn.lemmatize(w) for w in dd ] for dd in all_strings_tokenized]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#representation-of-text",
    "href": "04-nlp.html#representation-of-text",
    "title": "4  Natural Language Processing",
    "section": "4.5 Representation of Text",
    "text": "4.5 Representation of Text\nTokenisation of the corpus is merely the first step in processing natural language. All mathematical algorithms work on numerical representations of the data. Hence the next step is to convert the text into numeric representations. In natural language, there are two common ways of representing text:\n\nSparse vectors, using tf-idf or PPMI, or\nDense embeddings, which could result from word2vec, GLoVe, or from neural models.\n\n\nSparse embeddings with Tf-idf\nIn this section, we demonstrate how we can use Tf-idf (Term frequency-Inverse document frequency) to create vector representations of documents. Consider the following set of three simple text documents. Each document is a single sentence.\n\nraw_docs =[\n  \"Here are some very simple basic sentences.\", \n  \"They won’t be very interesting , I’m afraid. \",  \n  \"\"\"\n  The point of these basic examples is to learn how basic text  \n  counting works on *very simple* data, so that we are not afraid when  \n  it comes to larger text documents. The sentences are here just to provide words.\n  \"\"\"] \n\nAs the name tf-idf suggests, our first step should be to compute the frequency of each term (token) within each document.\n\nvectorizer1 = CountVectorizer(stop_words='english', min_df=1)\nX1 = vectorizer1.fit_transform(raw_docs)\n\nprint(pd.DataFrame(X1.toarray(), \n  columns=vectorizer1.get_feature_names_out()).iloc[:, :10])\n\n   afraid  basic  comes  counting  data  documents  examples  interesting  \\\n0       0      1      0         0     0          0         0            0   \n1       1      0      0         0     0          0         0            1   \n2       1      2      1         1     1          1         1            0   \n\n   just  larger  \n0     0       0  \n1     0       0  \n2     1       1  \n\n\nThe counts indicate the number of times each feature (or words) were present in the document. As you might observe, longer documents tend to contain larger counts (see document 3, which has many more 1’s and even a couple of 2’s. Thus, instead of dealing with counts, we shall convert each row into a vector of length 1. Words that appear in all documents will be weighted down by this transformation, since these do not help to distinguish the document from others. This transformation is known as the TF-IDF transformation.\nInstead of the raw counts, we define:\n\n\\(N\\) to be the number of documents (\\(N=3\\) in the little example above).\n\\(tf_{i,j}\\) to be the frequency of term \\(i\\) in document \\(j\\).\n\\(df_{i}\\) to be the frequency of term \\(i\\) across all documents.\n\\(w'_{i,j}\\) to be:\n\n\\[\\begin{equation}\nw'_{i,j} = tf_{i,j} \\times \\left[ \\log \\left( \\frac{1 + N}{1 + df_{i}} \\right) + 1 \\right]\n\\end{equation}\\]\nThen the final \\(w_{i,j}\\) for term \\(i\\) in document \\(j\\) is the normalised version of \\(w'_{i,j}\\) across the terms that document.\nConsider the word “sentences”, in document id 02 (the third document).\n\n\\(N = 3\\)\n\\(tf_{i,j} = 1\\)\n\\(df_{i} = 2\\)\n\nThus\n\\[\\begin{equation}\nw'_{ij} = 1 \\times \\log ( (1+3)/(1 +2)) = 1.287\n\\end{equation}\\]\n\nvectorizer2 = TfidfVectorizer(stop_words='english', norm=None)\nX2 = vectorizer2.fit_transform(raw_docs)\n\nprint(pd.DataFrame(X2.A, \n  columns=list(vectorizer2.get_feature_names_out())).iloc[:, :10].round(3))\n\n   afraid  basic  comes  counting   data  documents  examples  interesting  \\\n0   0.000  1.288  0.000     0.000  0.000      0.000     0.000        0.000   \n1   1.288  0.000  0.000     0.000  0.000      0.000     0.000        1.693   \n2   1.288  2.575  1.693     1.693  1.693      1.693     1.693        0.000   \n\n    just  larger  \n0  0.000   0.000  \n1  0.000   0.000  \n2  1.693   1.693  \n\n\nThe final step normalises the weights across each document, so now the weight for sentences in document 00 is higher than the weight in document 02 since document 00 is shorter.\n\nvectorizer3 = TfidfVectorizer(stop_words='english')\nX3 = vectorizer3.fit_transform(raw_docs)\n\nprint(pd.DataFrame(X3.A, \n  columns=list(vectorizer3.get_feature_names_out())).iloc[:, :10].round(3))\n\n   afraid  basic  comes  counting   data  documents  examples  interesting  \\\n0   0.000  0.577  0.000     0.000  0.000      0.000     0.000        0.000   \n1   0.474  0.000  0.000     0.000  0.000      0.000     0.000        0.623   \n2   0.170  0.340  0.223     0.223  0.223      0.223     0.223        0.000   \n\n    just  larger  \n0  0.000   0.000  \n1  0.000   0.000  \n2  0.223   0.223  \n\n\nThe above matrix is known as a document-term matrix, since the columns are defined by terms, and each row is a document. At this point, we can use each row as a vector representation of each document. If necessary, for this corpus, we could even represent each term using its corresponding column.\nNote that some books/software use a slightly different convention - they may work with the term-document matrix. However, the idea is the same. Take a look at the following term-document matrix, assembled from the complete works of Shakespeare:\n\n\n\nJurafsky and Martin (2025)\n\n\nIf we intend to represent each document as a numeric vector, the columns, highlighted by the red boxes, would be a natural choice. Suppose we only focus on the coordinates corresponding to the words battle and fool. Then a visualisation of the documents would look like this:\n\n\n\nJurafsky and Martin (2025)\n\n\nVisually, it is easy to tell that “Henry V” and “Julius Caesar” are similar (they point in the same direction) as opposed to “As You Like It” and “Twelfth Night”. But it is also easy to see why - the former two contain similar high counts of battle compared to the latter two, which are comedies.\nTf-idf are a normalised version of the above raw counts; they provide a numerical representation of documents, adjusting for document length and words that are common across all documents in a corpus.\n\n\nCosine similarity\nIn order to quantify the similarity (or nearness) of vector representations in NLP, the common method used is cosine similarity. Suppose that we have a vector representation of two documents \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). If the vocabulary size is \\(N\\), then each of the vectors is of length \\(N\\). Since we are dealing with counts the coordinate values of each vector will be non-negative. We use the angle \\(\\theta\\) between the vectors as a measure of their similarity:\n\\[\n\\cos \\theta = \\frac{\\sum_{i=1}^N v_i w_i}{\\sqrt{\\sum_{i=1}^N v_i^2} \\sqrt{\\sum_{i=1}^N w_i^2}}\n\\]\nGeometrically, cosine similarity measures the size of the angle between vectors:\n\n\n\nJurafsky and Martin (2025)\n\n\n\n\nDense Embeddings\nOne of the drawbacks of sparse vectors is that they are very long (the length of the vocabulary), and most entries in the vector will be 0. As a result, researchers worked on methods that would pack the information in the vectors into shorter ones. Instead of working on representations of the documents, the methods aimed to create representations of each token (or word) in the vocabulary. These are referred to as embeddings.\nHere, we shall discuss word2vec (Mikolov et al. (2013)), but take note that there are others. GLoVe (Pennington, Socher, and Manning (2014)) was invented soon after, but the most common embeddings used today arise from Deep Learning models. The most widely used version is BERT (see the video references below, as well as Devlin et al. (2019)).\nThe approach in word2vec deviates considerably from tf-idf, in that the goal is to obtain a numeric representation of a word, in the context of it’s surrounding words. Consider this statement:\n\n13% of the United States population eats pizza on any given day. Mozzarella is commonly used on pizza, with the highest quality mozzarella from Naples. In Italy, pizza served in formal &gt; settings is eaten with a fork and knife.\n\nThe words eats, served and mozzarella appear close to pizza. Hence another word that appears in similar contexts, should be similar to pizza. Examples could be certain baked dishes or even salad.\nTo achieve such a representation, word2vec runs a self-supervised algorithm, with two tasks:\n\nPrimary task: To “learn” a numeric vector that represents each word.\nPretext task (stepping stone): To train a classifier that, when given a word \\(w\\), predicts nearby context words \\(c\\).\n\nSelf-supervised algorithms differ from supervised algorithms in that there are no labels that need to be created. The pre-text task trains a model to perform predictions, based on a sliding window context:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStarting with an initial random vector for each word, the algorithm updates the vectors as it proceeds through the corpus, finally ending up with an embedding for each word that reflects its semantic value, based on neighbouring words.\nIn NLP, the quality of an embedding can be evaluated using an analogy task:\n\nGiven X, Y and Z, find W such that W is related to to Z in the same way that X is related to Y.\n\nFor instance, if we are given the pair man:king, and the word woman, then the embedding should return queen, since woman:queen in the same way that man is related to king. Geometrically, the answer to the analogy is obtained by adding (king - man) to woman. The nearest embedding to the result, is returned as the answer.\nOn the left are examples of the types of analogy pairs that word2vec is able to solve, while on the right, we have visualisations of GLoVe.\n\n\n\n\n\n\n\n\n\nword2vec\n\n\n\n\n\n\n\nGloVE\n\n\n\n\n\nHere’s how we can use gensim code to conduct the analogy task.\n\n# load pre-trained word-vectors from gensim-data\nword_vectors = api.load(\"glove-wiki-gigaword-100\")  \n\n# Check the \"most similar words\", using the default \"cosine similarity\" measure.\nresult = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\nmost_similar_key, similarity = result[0]  # look at the first match\nprint(f\"{most_similar_key}: {similarity:.4f}\")\n\nqueen: 0.7699\n\n\n\nprint(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n#similarity = word_vectors.similarity('woman', 'man')\n\ncereal",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#visualisation-with-t-sne",
    "href": "04-nlp.html#visualisation-with-t-sne",
    "title": "4  Natural Language Processing",
    "section": "4.6 Visualisation with t-SNE",
    "text": "4.6 Visualisation with t-SNE\nWhen compared with sparse embeddings, dense embeddings are compact. However, a more important difference is that dense vectors represent the semantic meaning of the words. This means that vectors that are close to each other are similar in meaning. Let us use t-SNE to visualise the GloVe embeddings.\nThere are a total of 400,000 vectors in the embedding. Even with t-SNE that will be difficult to make sense of. Hence for now, we sample a set of 100 to visualise them.\n\nrng1 = np.random.default_rng(1111)\n\nnn = 1000\nid = rng1.choice(len(word_vectors), size=(nn,), replace=False)\n\nX = np.zeros((nn, 100))\nfor ii in np.arange(nn):\n    #X[ii,] = glove_vectors.get_vector(id[ii])\n    X[ii,] = word_vectors.get_vector(ii)\nlabels = pd.Series([word_vectors.index_to_key[x] for x in np.arange(nn)])\n\ntsne1 = manifold.TSNE(n_components=2, init=\"random\", perplexity=10, metric='cosine', verbose=0, max_iter=5000, random_state=222)\nX_transformed2 = tsne1.fit_transform(X)\n\nYou should get the same plot as us since we have set the same seed at the start of the cell, and when we initialise the transformer. Explore the resulting plot - notice how months of the year appear close together at the bottom left. Around the left as well, the calendar years appear as a group.\n(The figure below only appears in the html version of the text)\n\ndf2 = pd.DataFrame(X_transformed2, columns=['x','y'])\ndf2['labels'] = labels\nfig = px.scatter(df2, x='x', y='y', text='labels', width=1024, height=960)\nfig.update_traces(textposition='top center')\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nNote\n\n\n\nWould we be able to make such a plot using tf-idf? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#neural-language-models",
    "href": "04-nlp.html#neural-language-models",
    "title": "4  Natural Language Processing",
    "section": "4.7 Neural Language Models",
    "text": "4.7 Neural Language Models\nLanguage is complex. It is incredible how we can understand such long paragraphs of texts with such ease. We somehow seem to have learnt complicated sets of grammar and syntax just by listening to others speak. To get a machine to learn language has not been easy. It is only recently that Large Language Models such as chatGPT have demonstrated that it is possible for machines to converse with humans just as we do to one another.\nNeural Models (or deep learning models) have been the key to this. In this subsection, we provide a very brief overview of their characteristics that allow them to achieve impressive performance on a range of language-related tasks.\nThe basic unit of a neural model is the neural unit (on the left). It consists of weights and a non-linear activation function. Given an input vector, the weights are multiplied by the input vector, summed and then fed through the activation function to generate an output.\n\n\n\n\n\n\n\n\n\nJurafsky and Martin (2025)\n\n\n\n\n\n\n\nNeuron figure from https://en.wikipedia.org/wiki/Neuron\n\n\n\n\n\nNeural models are made up of many neural units, organised into layers. The first neural models were Feed-Forward Networks. Due to the virtue of being able to incorporate many parameters, and due to semi-supervised learning, they were already a huge improvement over earlier models. Here is a simple set up, with one hidden layer for training a language model (used to predict the next word). It can also be used to learn embeddings.\n\n\n\nJurafsky and Martin (2025), FFN\n\n\nThe next evolution in neural models was the ability to incorporate words in the recent history. For humans, this comes naturally. For instance, we know that this is grammatically correct:\n\nThe flights the airline was cancelling were full.\n\nFor neural models to have this ability, it was necessary to incorporate the hidden layers from recent words when processing the current word. Recurrent Neural Networks (RNNs) and Long-Short Term Memory (LSTM) networks had these features, but they were very slow to train. The major breakthrough came with the invention of the transformer architecture. The self-attention layer of these networks gave a word access to all preceding words in the training window, instead of just one. Most importantly. the training of these networks could be parallelised!\n\n\n\nJurafsky and Martin (2025)\n\n\nHere are some examples where transformers excel:\n\nThe keys to the cabinet are on the table.\nThe chicken crossed the road because it wanted to get to the other side.\nI walked along the pond, and noticed that one of the trees along the bank had fallen into the water after the storm.\n\nIn the final sentence, the word bank has two meanings - how will a model know to decide the correct one? With transformers, because the full context of a word is captured along with it, it is possible to perform this disambiguation.\n\n\n\nJurafsky and Martin (2025)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#applications",
    "href": "04-nlp.html#applications",
    "title": "4  Natural Language Processing",
    "section": "4.8 Applications",
    "text": "4.8 Applications\nHugging Face has spent a considerable effort to make Neural Language Models accessible and available to all with minimal coding. For starters, they have ensured that all their models are described in a standardised manner with model cards. Here is an example of a model card for BERT.\nMoreover, they have developed easy to use pipelines. For NLP, the following tasks have mature pipelines:\n\nfeature-extraction (obtaining the embedding of a text)\nner\nquestion-answering\nsentiment-analysis\nsummarization\ntext-generation\ntranslation, and\nzero-shot-classification.\n\n\nSentiment Analysis\nIn this subsection, we shall utilise one of their sentiment analysis models on the wine reviews dataset. This is a transformer-based neural language model (BERT) that has been fine-tuned with data labelled with sentiments. All we have to do is feed in the sentence, and we will obtain a confidence score, and a sentiment label.\n\nclassifier = pipeline(\"sentiment-analysis\", \n  model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n  \nclassifier([\"I love this course!\", \"I absolutely detest this course.\"])\n\nDevice set to use cpu\n\n\n[{'label': 'POSITIVE', 'score': 0.9998835325241089},\n {'label': 'NEGATIVE', 'score': 0.9973570704460144}]\n\n\n\nExample 4.2 (Example: Wine Reviews Dataset) \nThe number of reviews we have is close to 120,000. Hence, computing the sentiments for each and every one will take a long time. Instead, we shall compute the sentiments for a sample (of size 20, where possible) from each variety of wine.\nThe following snippet samples 20 reviews from each wine type.\n\ntmp_df = pd.DataFrame(columns= wine_reviews.columns)\n# w = widgets.IntProgress(\n#     value=0,\n#     min=0,\n#     max=len(wine_reviews.variety.unique()),\n#     description='Progress: ',\n#     bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n#     style={'bar_color': 'lightblue'},\n#     orientation='horizontal'\n# )\n# display(w)\n\nfor x,vv in wine_reviews.groupby(wine_reviews.variety):\n    grp_len = vv.shape[0]\n    if(grp_len &gt;= 20):\n        vv = vv.sample(n=20, random_state=99)\n    tmp_df = pd.concat([tmp_df, vv], ignore_index=True)\n    # w.value += 1\n    \nreview_list = list(tmp_df.description)\n\n/tmp/ipykernel_12351/2593812303.py:17: FutureWarning:\n\nThe behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n\n\nThe next snippet computes the sentiment scores for those sampled reviews.\n\ntmp_df['score'] = 0.00\ntmp_df['label'] = ''\n# w = widgets.IntProgress(\n#     value=0,\n#     min=0,\n#     max=tmp_df.shape[0],\n#     description='Progress: ',\n#     bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n#     style={'bar_color': 'lightblue'},\n#     orientation='horizontal'\n# )\n# display(w)\nfor i,rr in enumerate(review_list):\n    tmp = classifier(rr)[0]\n    tmp_df.loc[i, 'score'] = tmp['score']\n    tmp_df.loc[i, 'label'] = tmp['label']\n    #w.value = i\n\n\nsent_counts = pd.crosstab(tmp_df.variety, tmp_df.label,margins=True)\nsent_counts['proportion'] = sent_counts.POSITIVE/sent_counts.All\n\n\nshow(sent_counts)\n\n\n\n    \n      label\n      NEGATIVE\n      POSITIVE\n      All\n      proportion\n    \n    \n      variety\n      \n      \n      \n      \n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nThese are the reviews for one of the varieties that had a proportion of positive reviews close to 50%.\n\nfor x in wine_reviews[wine_reviews.variety == 'Tempranillo Blanco'].description.values:\n    pp.pprint(x) \n\n(\"Gold in color and lightly oxidized on the nose, and it's still young. Smells \"\n 'heavy and creamy, like hay. Feels flat, with pickled flavors and mealy apple '\n 'on the finish. Runs plump, sweet and seems like an imposter for Chardonnay.')\n('Oily, stalky, bready aromas are a bit tired. This has a chunky feel offset '\n 'by citric acidity. Briny, salty flavors of citrus fruits and lees are '\n \"lasting. For varietal Tempranillo Blanco, this isn't bad.\")\n('Maderized in color, this wine has a yeasty, creamy nose with baked '\n \"white-fruit aromas and caramel. It's OK in feel, with pickled, mildly briny \"\n 'flavors of apple and apricot. The finish is showing some oxidization, '\n 'leading to a chunky, fleshy feel.')\n(\"Waxy peach aromas seem slightly oxidized. It's round and citrusy on the \"\n 'palate, but in a monotone way that fades to pithy white fruits and mealy '\n \"citrus. Shows some flashes of uniqueness and class; mostly it's wayward and \"\n 'slightly bitter.')\n('Forget the high price on this Tempranillo Blanco. Looking at the wine alone, '\n \"it's briny and stalky on the nose, with wiry lemon-like acids that push sour \"\n \"orange flavors. Overall it's monotone, briny and citrusy.\")\n(\"A maderized color is apropos for the wine's fully mature, nutty nose. This \"\n 'is big and cidery feeling, with apple and orange flavors. A finish of '\n 'vanilla, nuttiness and oxidation matches the color and aromas of this '\n 'interesting but midlevel Tempranillo Blanco.')\n('Green grassy aromas are modest and watery. This feels oily, but with decent '\n 'acidity. Oxidized flavors of stone fruits finish wheaty, bland and eggy.')\n('Rough, stalky, yeasty aromas are all over the map. Lemony acidity renders '\n 'this tight as a drum, while bitter, stalky flavors finish wheaty and bitter. '\n 'This Tempranillo Blanco is barely worth a go; the pleasure factor is at base '\n 'level.')\n('Green aromas of herbs and tomatillo are harsh, rubbery and outweigh peach '\n 'and other stone-fruit scents. This Tempranillo Blanco is plump and fair on '\n 'the palate, while flavors of apple and peach are briny and finish with '\n 'controlled bitterness.')\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo you agree with the classifications above? What would you investigate next?\n\n\n\n\nInformation Retrieval\nIn the NLP context, Information Retrieval (IR) refers to the task of returning the most relevant set of documents, when given a query string. Search engines, e.g. Google, are trained to perform fast and accurate IR. Typically, a long list of documents is returned, with the most relevant one on top.\n\n\n\n\n\n\nNote\n\n\n\nPause for a moment, and consider how you would assess the performance of such a search engine.\n\n\n\nExample 4.3 (Example: Wine Reviews Dataset) \nA simple way to perform IR is to use cosine similarity to compute how close the given query vector is to the individual documents in the corpus.\nThe next snippet initialises a model for retrieving similar documents.\n\ndct = gensim.corpora.Dictionary(all_strings_tokenized)\nbow_corpus = [dct.doc2bow(text) for text in all_strings_tokenized]\ntfidf = gensim.models.TfidfModel(dictionary=dct)\n\nNLP corpora are typically very large. Before we can find matching documents, we build a similarity index, so that matches are returned quicker. We try something simple at first:\n\nWhich documents/reviews are similar to the first one?\n\n\nindex = gensim.similarities.Similarity(None, \n  corpus=tfidf[bow_corpus], num_features=len(dct))\nsims = index[tfidf[bow_corpus[0]]]\n\nThese are the most similar reviews to review id 0. Of course, the first review itself is there! Let’s retrieve and print all the reviews similar to the first one.\n\n#np.argsort(-sims)[:10]\nfor x in np.argsort(-sims)[:5]:\n    pp.pprint(all_review_strings[x]) \n\n('Aromas include tropical fruit, broom, brimstone and dried herb. The palate '\n \"isn't overly expressive, offering unripened apple, citrus and dried sage \"\n 'alongside brisk acidity.')\n(\"The nose isn't very expressive but reveals white flower and tropical fruit. \"\n 'The simple palate delivers pineapple and lemon zest alongside brisk acidity.')\n(\"The nose isn't very expressive but the palate eventually reveals raw red \"\n 'berry, espresso, brimstone and grilled rosemary alongside astringent and '\n 'rather drying tannins.')\n('This opens with aromas of pressed acacia flowers, ripe stone fruits and '\n \"dried sage. The palate isn't overly sweet, offering dried apricot, \"\n 'wildflower honey and toasted almond notes.')\n('Subdued aromas of Spanish broom and brimstone float from the glass. The '\n 'vertical palate offers yellow apple, citrus zest and mineral alongside crisp '\n 'acidity.')\n\n\nNow we try a new query of our own: “acidic chardonnay”. First we preprocess it, like we did the original documents.\n\nq1 = [wn.lemmatize(x) for x in preprocess_string('acidic white chardonnay', CUSTOM_FILTER)]\nsims = index[tfidf[dct.doc2bow(q1)]]\n\nNow we print the top 5 most similar reviews to our query.\n\nq1_results = np.argsort(-sims)[:10]\n#q1_results\n#pp.pprint(wine_reviews.description.values[q1_results])\nfor x in q1_results[:5]:\n    pp.pprint(all_review_strings[x]) \n\n('A standard Chardonnay, dry and nicely acidic, with citrus, pear, vanilla, '\n 'lees and oak flavors.')\n('Dry and acidic, this Chardonnay has a herbaceous earthiness, plus flavors of '\n 'orange and pear.')\n'This is thin and acidic, with flavors of sour cherry candy and spice.'\n'This is acidic and sweet, with a medicinal taste.'\n('Pungent up front, with green herb, white pepper and citrus aromas, this is '\n 'zesty and acidic on the palate, with a monotonous lemon flavor on the '\n 'finish. It turns more tart and acidic as it airs.')\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry your favourite tastes, see if you discover a wine you like/dislike 🍷🍇🥂\n\n\n\n\nTopic Modeling\nThe LDA (Latent Dirichlet Allocation) model assumes the following intuitive generative process for the documents:\n\nThere is a set of \\(K\\) topics that the documents come from. Each document contains words from several topics. There is a probability mass function on the topics for each document.\nFor each topic, there is a probability mass function for the distribution of words in that topic.\n\nAt the end of LDA topic modeling, we will be able to tell, for a particular (new or old) document: the weight combination of the topics for that document. For each topic, we would be able to tell the terms that are salient.\nLDA only gives us the probabilistic weights - we have to interpret them ourselves. Suppose we decide to split the corpus into 10 topics. Let us investigate what these topics consist of.\n\nlda1 = gensim.models.LdaModel(corpus= bow_corpus, num_topics=10, id2word=dct)\nreviews_vis_data = gensimvis.prepare(lda1, bow_corpus, dct)\n\npp.pprint(lda1.show_topics())\n\n[   (   0,\n        '0.081*\"wine\" + 0.033*\"acidity\" + 0.033*\"drink\" + 0.026*\"ripe\" + '\n        '0.024*\"fruit\" + 0.022*\"fruits\" + 0.019*\"tannins\" + 0.017*\"rich\" + '\n        '0.014*\"character\" + 0.014*\"flavors\"'),\n    (   1,\n        '0.048*\"flavors\" + 0.038*\"finish\" + 0.036*\"aromas\" + 0.021*\"palate\" + '\n        '0.020*\"berry\" + 0.017*\"plum\" + 0.013*\"feels\" + 0.012*\"herbal\" + '\n        '0.010*\"notes\" + 0.010*\"nose\"'),\n    (   2,\n        '0.040*\"oak\" + 0.037*\"fruit\" + 0.023*\"finish\" + 0.019*\"flavors\" + '\n        '0.013*\"red\" + 0.013*\"tannins\" + 0.013*\"aromas\" + 0.012*\"palate\" + '\n        '0.012*\"cherry\" + 0.011*\"french\"'),\n    (   3,\n        '0.030*\"wine\" + 0.025*\"flavors\" + 0.019*\"pinot\" + 0.017*\"cherry\" + '\n        '0.017*\"fruit\" + 0.012*\"drink\" + 0.012*\"noir\" + 0.010*\"oak\" + '\n        '0.010*\"texture\" + 0.009*\"like\"'),\n    (   4,\n        '0.039*\"cabernet\" + 0.025*\"tannins\" + 0.024*\"blend\" + 0.024*\"black\" + '\n        '0.022*\"flavors\" + 0.021*\"merlot\" + 0.021*\"sauvignon\" + 0.020*\"wine\" + '\n        '0.018*\"blackberry\" + 0.015*\"chocolate\"'),\n    (   5,\n        '0.028*\"wine\" + 0.026*\"vineyard\" + 0.022*\"flavors\" + 0.019*\"oak\" + '\n        '0.017*\"acidity\" + 0.016*\"vanilla\" + 0.012*\"chardonnay\" + 0.011*\"rich\" '\n        '+ 0.010*\"toast\" + 0.010*\"valley\"'),\n    (   6,\n        '0.038*\"black\" + 0.031*\"cherry\" + 0.030*\"palate\" + 0.026*\"tannins\" + '\n        '0.019*\"aromas\" + 0.016*\"red\" + 0.016*\"nose\" + 0.014*\"pepper\" + '\n        '0.014*\"spice\" + 0.013*\"plum\"'),\n    (   7,\n        '0.039*\"fruit\" + 0.035*\"aromas\" + 0.035*\"wine\" + 0.025*\"spice\" + '\n        '0.022*\"flavors\" + 0.017*\"cherry\" + 0.015*\"notes\" + 0.011*\"followed\" + '\n        '0.010*\"pair\" + 0.010*\"bright\"'),\n    (   8,\n        '0.025*\"flavors\" + 0.023*\"apple\" + 0.019*\"citrus\" + 0.019*\"finish\" + '\n        '0.019*\"lemon\" + 0.018*\"wine\" + 0.017*\"palate\" + 0.016*\"peach\" + '\n        '0.016*\"fresh\" + 0.015*\"acidity\"'),\n    (   9,\n        '0.045*\"palate\" + 0.039*\"aromas\" + 0.026*\"white\" + 0.023*\"acidity\" + '\n        '0.023*\"offers\" + 0.022*\"note\" + 0.021*\"alongside\" + 0.018*\"opens\" + '\n        '0.016*\"flower\" + 0.015*\"hint\"')]\n\n\nThe output provides the most common terms that define each topic (remember: each topic is defined as a probability distribution over the vocabulary).\n\n\n\n\n\n\nNote\n\n\n\nWhat name would you give each topic?\n\n\nWe can also find out which topics a particular document is distributed over. For instance, the output below shows that words in document 0 are predominantly drawn from topics 2 and 5.\n\nlda1.get_document_topics(bow_corpus[0])\n\n[(1, 0.087330736), (7, 0.13275942), (8, 0.21943033), (9, 0.53188884)]\n\n\n\npp.pprint(all_review_strings[0])\n\n('Aromas include tropical fruit, broom, brimstone and dried herb. The palate '\n \"isn't overly expressive, offering unripened apple, citrus and dried sage \"\n 'alongside brisk acidity.')\n\n\nA delightful visualisation from pyLDAvis allows us to understand the “distance” between topics, and the frequent words from each topic easily.\n\npyLDAvis.display(reviews_vis_data)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#interpretation-of-neural-models",
    "href": "04-nlp.html#interpretation-of-neural-models",
    "title": "4  Natural Language Processing",
    "section": "4.9 Interpretation of Neural Models",
    "text": "4.9 Interpretation of Neural Models\nNeural models have achieved impressive performance on a number of language-related tasks. However, one criticism of them is that they are “black-box” models; we do not fully grasp how they work. This can lead to a mistrust of such models, with good reason. If we do not fully know how these models work, we would not know when they are might fail, or we might not know the reason when they do fail (or make an incorrect prediction). For this reason, a huge amount of research effort is currently directed towards understanding and interpreting neural models.\nOne approach is to identify which examples in the training set were most influential for predictions regarding particular test instances. For incorrect predictions, this could give us intuition on why the model is failing, and guide us to ways to fix it. Here is an example where it was possible to pinpoint why a model yielded incorrect sentiment prediction.\n\n\n\nHan, Wallace, and Tsvetkov (2020)\n\n\nAnother approach is to identify which parts of the test sentence itself were important to the eventual prediction. Imagine perturbing the test sentence in some ways, and studying how the prediction changed. In one study of a Question-Answering model, the question was modified by dropping the least important word, until the question was answered incorrectly. In this case, the study revealed something pathological about the model:\n\n\n\nSun et al. (2021)\n\n\nFor transformers in particular, a great deal of study has focused on the weights that the attention layers pick up. By relating these to linguistic information, researchers try to infer the precise language-related information that models retain. For instance, it has been found that BERT learns parts of speech. It is also able to identify the dependencies between words.\n\n\n\nClark et al. (2019)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "04-nlp.html#references",
    "href": "04-nlp.html#references",
    "title": "4  Natural Language Processing",
    "section": "4.10 References",
    "text": "4.10 References\n\nVideo explainers\n\nTransformer models and BERT: A very good video from Google Cloud Tech on current neural models (11:37)\nIntroduction to RNN\nIntroduction to BERT\n\n\n\nWebsite References\n\nHugging Face course on transformers\nGensim documentation: Contains tutorials as well.\nUsing sklearn to perform LDA: We can also use scikit-learn to perform LDA.\nVisualising LDA: Contains sample notebooks for the visualisation.\n\n\n\n\n\nClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. “What Does Bert Look at? An Analysis of Bert’s Attention.” arXiv Preprint arXiv:1906.04341.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86.\n\n\nHan, Xiaochuang, Byron C Wallace, and Yulia Tsvetkov. 2020. “Explaining Black Box Predictions and Unveiling Data Artifacts Through Influence Functions.” arXiv Preprint arXiv:2005.06676.\n\n\nJurafsky, Daniel, and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, with Language Models. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” Advances in Neural Information Processing Systems 26.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. “Glove: Global Vectors for Word Representation.” In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–43.\n\n\nSun, Xiaofei, Diyi Yang, Xiaoya Li, Tianwei Zhang, Yuxian Meng, Han Qiu, Guoyin Wang, Eduard Hovy, and Jiwei Li. 2021. “Interpreting Deep Learning Models in Natural Language Processing: A Review.” arXiv Preprint arXiv:2110.10470.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "05-regression.html",
    "href": "05-regression.html",
    "title": "5  Linear Regression",
    "section": "",
    "text": "5.1 Introduction\nRegression analysis is a technique for investigating and modeling the relationship between variables like X and Y. Here are some examples:\nIn all the above cases, we refer to \\(X\\) as the explanatory or independent variable. It is also sometimes referred to as a predictor. \\(Y\\) is referred to as the response or dependent variable. In this topic, we shall first introduce the case of simple linear regression, where we model the \\(Y\\) on a single \\(X\\). In later sections, we shall model the \\(Y\\) on multiple \\(X\\)’s. This latter technique is referred to as multiple linear regression.\nRegression models are used for two primary purposes:\nIn this topic, we shall focus on the estimation aim, since prediction models require a paradigm of their own, and are best learnt alongside a larger suite of models e.g. decision trees, support vector machines, etc. We shall cover prediction in the topic of supervised learning.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport folium\nimport geopandas\nfrom itables import show\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nfrom scipy import stats\nLet us first explore the dataset with Python. How do this plots aid in your understanding of the dataset?\nplt.figure(figsize=(15,4));\nax=plt.subplot(131)\nre2.plot(x='dist_MRT', y='price', kind='scatter', ax=ax, title='distance to MRT')\n\nax=plt.subplot(132)\nre2.plot(x='house_age', y='price', kind='scatter', ax=ax, title='House age')\n\nax= plt.subplot(133)\nz = re2.num_stores.unique()\nz.sort()\ntmp2 = np.array([re2.price[re2.num_stores == x].to_numpy() for x in z], dtype=object)\nax.boxplot(tmp2, tick_labels=z); ax.set_title('Num. of stores');",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#introduction",
    "href": "05-regression.html#introduction",
    "title": "5  Linear Regression",
    "section": "",
    "text": "Within a country, we may wish to use per capita income (X) to estimate the life expectancy (Y) of residents.\nWe may wish to use the size of a crab claw (X) to estimate the closing force that it can exert (Y).\nWe may wish to use the height of a person (X) to estimate their weight (Y).\n\n\n\n\nTo understand how certain explanatory variables affect the response variable. This aim is typically known as estimation, since the primary focus is on estimating the unknown parameters of the model.\nTo predict the response variable for new values of the explanatory variables. This is referred to as prediction.\n\n\n\n\nExample 5.1 (Example: Taiwan Real Estate) \nFor this tutorial, we shall work with a data set from the UCI machine learning repository. It contains real estate prices in the Xindian district of Taiwan. Our goal is to answer the following question:\n\nHow well can we explain real-estate prices in Taiwan?\n\nHere is a brief description of the columns in the dataset:\n\ntrans_date: The date of the transaction. As you can see, this has been coded to be a numerical value, so 2013.5 refers to June 2013.\nhouse_age: Age of the house in years.\ndist_MRT: Distance the the nearest MRT (in metres)\nnum_stores: Number of convenience stores within walking distance\nlat, long: Latitude and longitude\nprice: House price per unit area (10,000 New Taiwan Dollars per Ping, which is about \\(3.3 m^2\\))\nX, Y, Xs, Ys: Projected coordinates\n\n\nre2 = pd.read_csv(\"data/taiwan_dataset.csv\")\nre2.head()\n\n\n\n\n\n\n\n\nid\ntrans_date\nhouse_age\ndist_MRT\nnum_stores\nlat\nlong\nprice\nX\nY\nXs\nYs\n\n\n\n\n0\n1\n2012.916667\n32.0\n84.87882\n10\n24.98298\n121.54024\n37.9\n506501.554580\n2.766295e+06\n0.666075\n1.559003\n\n\n1\n2\n2012.916667\n19.5\n306.59470\n9\n24.98034\n121.53951\n42.2\n506433.292007\n2.766001e+06\n0.597812\n1.265025\n\n\n2\n3\n2013.583333\n13.3\n561.98450\n5\n24.98746\n121.54391\n47.3\n506862.973688\n2.766798e+06\n1.027494\n2.062484\n\n\n3\n4\n2013.500000\n13.3\n561.98450\n5\n24.98746\n121.54391\n54.8\n506862.973688\n2.766798e+06\n1.027494\n2.062484\n\n\n4\n5\n2012.833333\n5.0\n390.56840\n5\n24.97937\n121.54245\n43.1\n506732.307872\n2.765899e+06\n0.896828\n1.163084\n\n\n\n\n\n\n\nThe following interactive map is available on the HTML version.\n\nm = folium.Map(location=(45.5236, -122.6750))\ndf1 = geopandas.read_file('data/taiwan_dataset.csv')\ngdf = geopandas.GeoDataFrame(\n    re2, geometry=geopandas.points_from_xy(re2.X, re2.Y), crs=\"EPSG:3825\"\n)\ngdf.explore(\"price\", #tiles=\"CartoDB positron\", \n            tooltip=\"price\", marker_type=\"circle\", \n            marker_kwds = {\"radius\": 50, \"fill\": True}, \n            legend_kwds = {\"caption\": \"Price\"})\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#simple-linear-regression",
    "href": "05-regression.html#simple-linear-regression",
    "title": "5  Linear Regression",
    "section": "5.2 Simple Linear Regression",
    "text": "5.2 Simple Linear Regression\n\nFormal Set-up\nThe simple linear regression model is applicable when we have observations \\((X_i, Y_i)\\) for \\(n\\) individuals. For now, let’s assume both the \\(X\\) and \\(Y\\) variables are quantitative.\nThe simple linear regression model is given by\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i\n\\] where\n\n\\(\\beta_0\\) is intercept term,\n\\(\\beta_1\\) is the slope, and\n\\(e_i\\) is an error term, specific to each individual in the dataset.\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown constants that need to be estimated from the data. There is an implicit assumption in the formulation of the model that there is a linear relationship between \\(Y_i\\) and \\(X_i\\). In terms of distributions, we assume that the \\(e_i\\) are i.i.d Normal.\n\\[\ne_i \\sim N(0, \\sigma^2), \\; i =1\\ldots, n\n\\]\nThe constant variance assumption is also referred to as homoscedascity (homo-skee-das-city). The validity of the above assumptions will have to be checked after the model is fitted. All in all, the assumptions imply that:\n\n\\(E(Y_i | X_i) = \\beta_0 + \\beta_1 X_i\\), for \\(i=1, \\ldots, n\\).\n\\(Var(Y_i | X_i) = Var(e_i) = \\sigma^2\\), for \\(i=1, \\ldots, n\\).\nThe \\(Y_i\\) are independent.\nThe \\(Y_i\\)’s are Normally distributed.\n\n\n\nEstimation\nBefore deploying or using the model, we need to estimate optimal values to use for the unknown \\(\\beta_0\\) and \\(\\beta_1\\). We shall introduce the method of Ordinary Least Squares (OLS) for the estimation. Let us define the error Sum of Squares to be\n\\[\nSS_E = S(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\]\nThen the OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are given by \\[\n\\mathop{\\arg \\min}_{\\beta_0, \\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n\\] The minimisation above can be carried out analytically, by taking partial derivative with respect to the two parameters and setting them to 0.\n\\[\\begin{eqnarray*}\n\\frac{\\partial S}{\\partial \\beta_0}  &=& -2 \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i) = 0 \\\\\n\\frac{\\partial S}{\\partial \\beta_1}  &=& -2 \\sum_{i=1}^n X_i (Y_i - \\beta_0 - \\beta_1 X_i) = 0\n\\end{eqnarray*}\\]\nSolving and simplifying, we arrive at the following:\n\\[\\begin{eqnarray*}\n\\hat{\\beta_1} &=& \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\\\\n\\hat{\\beta_0} &=& \\bar{Y} - \\hat{\\beta_0} \\bar{X}\n\\end{eqnarray*}\\] where \\(\\bar{Y} = (1/n)\\sum Y_i\\) and \\(\\bar{X} = (1/n)\\sum X_i\\).\nIf we define the following sums:\n\\[\\begin{eqnarray*}\nS_{XY} &=& \\sum_{i=1}^n X_i Y_i - \\frac{(\\sum_{i=1}^n X_i )(\\sum_{i=1}^n Y_i )}{n} \\\\\nS_{XX} &=& \\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i )^2}{n}\n\\end{eqnarray*}\\] then a form convenient for computation of \\(\\hat{\\beta_1}\\) is \\[\n\\hat{\\beta_1} = \\frac{S_{XY}}{S_{XX}}\n\\]\nOnce we have the estimates, we can use the estimated model to compute fitted values for each observation, corresponding to our best guess of the mean of the distributions from which the observations arose: \\[\n\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} X_i, \\quad i = 1, \\ldots, n\n\\] As always, we can form residuals as the deviations from fitted values. \\[\nr_i = Y_i - \\hat{Y}_i\n\\] Residuals are our best guess at the unobserved error terms \\(e_i\\). Squaring the residuals and summing over all observations, we can arrive at the following decomposition, which is very similar to the one in the ANOVA model:\n\\[\n\\underbrace{\\sum_{i=1}^n (Y_i  - \\bar{Y})^2}_{SS_T} =  \n\\underbrace{\\sum_{i=1}^n (Y_i  - \\hat{Y_i})^2}_{SS_{Res}} +\n\\underbrace{\\sum_{i=1}^n (\\hat{Y_i}  - \\bar{Y})^2}_{SS_{Reg}}\n\\]\nwhere\n\n\\(SS_T\\) is known as the total sum of squares.\n\\(SS_{Res}\\) is known as the residual sum of squares.\n\\(SS_{Reg}\\) is known as the regression sum of squares.\n\nIn our model, recall that we had assumed equal variance for all our observations. We can estimate \\(\\sigma^2\\) with \\[\n\\hat{\\sigma^2} = \\frac{SS_{Res}}{n-2}\n\\] Our distributional assumptions lead to the following for our estimates \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\):\n\\[\\begin{eqnarray}\n\\hat{\\beta_0} &\\sim& N(\\beta_0,\\; \\sigma^2(1/n + \\bar{X}^2/S_{XX})) \\\\\n\\hat{\\beta_1} &\\sim& N(\\beta_1,\\; \\sigma^2/S_{XX})\n\\end{eqnarray}\\]\nThe above are used to construct confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\), based on \\(t\\)-distributions.\n\n\nHypothesis Test for Model Significance\nThe first test that we introduce here is to test if the coefficient \\(\\beta_1\\) is significantly different from 0. It is essentially a test of whether it was worthwhile to use a simple linear regression, instead of a simple mean to represent the data.\nThe null and alternative hypotheses are:\n\\[\\begin{eqnarray*}\nH_0 &:& \\beta_1 = 0\\\\\nH_1 &:& \\beta_1 \\ne 0\n\\end{eqnarray*}\\]\nThe test statistic is\n\\[\nF_0 = \\frac{SS_{Reg}/1}{SS_{Res}/(n-2)}\n\\]\nUnder the null hypothesis, \\(F_0 \\sim F_{1,n-2}\\).\nIt is also possible to perform this same test as a \\(t\\)-test, using the result earlier. The statement of the hypotheses is equivalent to the \\(F\\)-test. The test statistic \\[\nT_0 = \\frac{\\hat{\\beta_1}}{\\sqrt{\\hat{\\sigma^2}/S_{XX}}}\n\\] Under \\(H_0\\), the distribution of \\(T_0\\) is \\(t_{n-2}\\). This \\(t\\)-test and the earlier \\(F\\)-test in this section are identical. It can be proved that \\(F_0 = T_0^2\\); the obtained \\(p\\)-values will be identical.\n\n\nCoefficient of Determination, \\(R^2\\)\nThe coefficient of determination \\(R^2\\) is defined as\n\\[\nR^2 = 1 - \\frac{SS_{Res}}{SS_T} = \\frac{SS_{Reg}}{SS_T}\n\\] It can be interpreted as the proportion of variation in \\(Yi\\), explained by the inclusion of \\(X_i\\). Since \\(0 \\le SS_{Res} \\le SS_T\\), we can easily prove that \\(0 \\le R^2 \\le 1\\). The larger the value of \\(R^2\\) is, the better the model is.\nWhen we get to the case of multiple linear regression, take note that simply including more variables in the model will increase \\(R^2\\). This is undesirable; it is preferable to have a parsimonious model that explains the response variable well.\n\nExample 5.2 (Example: Price vs. House Age) \nAs a first model, we fit price (\\(Y\\)) against house age (\\(X_1\\)). From the plot above, we already suspect this may not be ideal, but let us use it as a starting point.\n\nlm_house_age_1 = ols('price ~ house_age', data=re2).fit()\nprint(lm_house_age_1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.044\nModel:                            OLS   Adj. R-squared:                  0.042\nMethod:                 Least Squares   F-statistic:                     19.11\nDate:                Tue, 23 Sep 2025   Prob (F-statistic):           1.56e-05\nTime:                        09:16:24   Log-Likelihood:                -1658.3\nNo. Observations:                 414   AIC:                             3321.\nDf Residuals:                     412   BIC:                             3329.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     42.4347      1.211     35.042      0.000      40.054      44.815\nhouse_age     -0.2515      0.058     -4.372      0.000      -0.365      -0.138\n==============================================================================\nOmnibus:                       48.404   Durbin-Watson:                   1.957\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              119.054\nSkew:                           0.589   Prob(JB):                     1.40e-26\nKurtosis:                       5.348   Cond. No.                         39.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nFrom the output, we can tell that the estimated model for Price (\\(Y\\)) against Housing age (\\(X_1\\)) is:\n\\[\nY = 42.43 - 0.25 X_1\n\\]\nThe estimates are \\(\\hat{\\beta_0} = 42.43\\) and \\(\\hat{\\beta_1} = -0.25\\). The output includes the 95% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\). The \\(R^2\\) is 0.044, which means that means that only 4.4% of the variation in \\(Y\\) is explained by \\(X\\). This is extremely poor, even though the \\(p\\)-value for the \\(F\\)-test is very small (0.000016).\nA simple interpretation of the model is as follows:\n\nFor every 1 year increase in house age, there is an average associated decrease in price of \\(0.25 \\times 10,000\\) New Taiwan Dollars.\n\n\nNote that this interpretation has to be taken very cautiously, especially when there are other explanatory variables in the model.\n\nExample 5.3 (Example: Price vs. House Age Estimated Line) \nIn linear regression, we almost always wish to use the model to understand what the mean of future observations would be. In this case, we may wish to use the model to understand how the Price changes as house age increases. This is because, based on our formulation,\n\\[\nE(Y | X) = \\beta_0 + \\beta_1 X\n\\]\nAfter estimating the parameters, we would have: \\[\n\\widehat{E(Y | X)} = \\hat{\\beta_0} + \\hat{\\beta_1} X\n\\]\nThus we can vary the values of \\(X\\) to study how the mean of \\(Y\\) changes. Here is how we can do so for the model that we have just fit.\n\nnew_df = sm.add_constant(pd.DataFrame({'house_age' : np.linspace(0, 45, 100)}))\npredictions_out = lm_house_age_1.get_prediction(new_df)\n\nax = re2.plot(x='house_age', y='price', kind='scatter', alpha=0.5 )\nax.set_title('Price vs. age');\nax.plot(new_df.house_age, predictions_out.conf_int()[:, 0].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.house_age, predictions_out.conf_int()[:, 1].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.house_age, predictions_out.predicted, color='blue');\n\n\n\n\n\n\n\n\n\nnew_df = sm.add_constant(pd.DataFrame({'house_age' : np.linspace(0, 45, 100)}))\npredictions_out = lm_house_age_1.get_prediction(new_df)\n\nax = re2.plot(x='house_age', y='price', kind='scatter', alpha=0.5 )\nax.set_title('Price vs. age');\nax.plot(new_df.house_age, predictions_out.conf_int()[:, 0].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.house_age, predictions_out.conf_int()[:, 1].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.house_age, predictions_out.predicted, color='blue');",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#multiple-linear-regression",
    "href": "05-regression.html#multiple-linear-regression",
    "title": "5  Linear Regression",
    "section": "5.3 Multiple Linear Regression",
    "text": "5.3 Multiple Linear Regression\n\nFormal Setup\nWhen we have more than 1 explanatory variable, we turn to multiple linear regression - a more general version of what we have been dealing with so far. We still assume that we have observed information from \\(n\\) individuals, but for each one, we now observe a vector of values:\n\\[\nY_i, \\, X_{1,i},  \\, X_{2,i}, \\ldots, \\, X_{p-1,i},  X_{p,i}\n\\]\nIn other words, we observe \\(p\\) independent variables and 1 response variable for each individual in our dataset. The analogous equation to the earlier model is\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1,i} + \\cdots + \\beta_p  X_{p,i} + e\n\\]\nIt is easier to write things with matrices for multiple linear regression:\n\\[\n\\textbf{Y} = \\begin{bmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{bmatrix}, \\;\n\\textbf{X} = \\begin{bmatrix}\n1 & X_{1,1} & X_{2,1} & \\cdots &X_{p,1}\\\\\n1 & X_{1,2} & X_{2,2} & \\cdots &X_{p,2}\\\\\n\\vdots & \\vdots & \\vdots & {} & \\vdots \\\\\n1 & X_{1,n} & X_{2,n} & \\cdots &X_{p,n}\\\\\n\\end{bmatrix}, \\;\n\\boldsymbol{ \\beta } = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}, \\;\n\\boldsymbol{e} = \\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n\n\\end{bmatrix}\n\\]\nWith the above matrices, we can re-write the regression model as \\[\n\\textbf{Y} = \\textbf{X} \\boldsymbol{\\beta} + \\textbf{e}\n\\] We retain the same distributional assumptions as in simple linear regression.\n\n\nEstimation\nSimilar to estimation in the earlier case, we can define \\(SS_E\\) to be \\[\nSS_E = S(\\beta_0, \\beta_1,\\ldots,\\beta_p) = \\sum_{i=1}^n (Y_i - \\beta_0 -\n\\beta_1 X_{1,i} - \\cdots - \\beta_p X_{p,i} )^2\n\\]\nMinimising the above cost function leads to the OLS estimates: \\[\n\\hat{\\boldsymbol{\\beta}} =  (\\textbf{X}'\\textbf{X})^{-1} \\textbf{X}'\\textbf{Y}\n\\] The fitted values can be computed with \\[\n\\hat{\\textbf{Y}} = \\textbf{X} \\hat{\\boldsymbol{\\beta}} =\n\\textbf{X} (\\textbf{X}'\\textbf{X})^{-1} \\textbf{X}'\\textbf{Y}\n\\] Residuals are obtained as \\[\n\\textbf{r} = \\textbf{Y} - \\hat{\\textbf{Y}}\n\\] Finally, we estimate \\(\\sigma^2\\) using \\[\n\\hat{\\sigma^2} = \\frac{SS_{Res}}{n-p} = \\frac{\\textbf{r}' \\textbf{r}}{n-p}\n\\]\n\n\nAdjusted \\(R^2\\)\nIn the case of multiple linear regression, \\(R^2\\) is calculated exactly as in simple linear regression, and its interpretation remains the same: \\[\nR^2 = 1 - \\frac{SS_{Res}}{SS_T}\n\\]\nHowever, note that \\(R^2\\) can be inflated simply by adding more terms to the model (even insignificant terms). Thus, we use the adjusted \\(R^2\\), which penalizes us for adding more and more terms to the model: \\[\nR^2_{adj} = 1 - \\frac{SS_{Res}/(n-p)}{SS_T/(n-1)}\n\\]\n\n\nHypothesis Tests\nThe \\(F\\)-test in the multiple linear regression helps determine if our regression model provides any advantage over the simple mean model. The null and alternative hypotheses are:\n\\[\\begin{eqnarray*}\nH_0 &:& \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\\\\nH_1 &:& \\beta_j \\ne 0 \\text{ for at least one } j \\in \\{1, 2, \\ldots, p\\}\n\\end{eqnarray*}\\]\nThe test statistic is\n\\[\nF_1 = \\frac{SS_{Reg}/p}{SS_{Res}/(n-p-1)}\n\\]\nUnder the null hypothesis, \\(F_0 \\sim F_{p,n-p-1}\\).\nIt is also possible to test for the significance of individual \\(\\beta\\) terms, using a \\(t\\)-test. The output is typically given for all the coefficients in a table. The statement of the hypotheses pertaining to these tests is:\n\\[\\begin{eqnarray*}\nH_0 &:& \\beta_j = 0\\\\\nH_1 &:& \\beta_j \\ne 0\n\\end{eqnarray*}\\]\nHowever, note that these \\(t\\)-tests are partial because it should be interpreted as a test of the contribution of \\(\\beta_j\\), given that all other terms are already in the model.\n\nExample 5.4 (Example: Price vs. House Age and Distance to MRT) \nFor our first example on multiple linear regression, let us regress price (\\(Y\\)) on house age (\\(X_1\\)) and distance to MRT (\\(X_2\\)).\n\nlm_age_mrt_1 = ols('price ~ house_age + dist_MRT', data=re2).fit()\nprint(lm_age_mrt_1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.491\nModel:                            OLS   Adj. R-squared:                  0.489\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Tue, 23 Sep 2025   Prob (F-statistic):           5.07e-61\nTime:                        09:16:24   Log-Likelihood:                -1527.9\nNo. Observations:                 414   AIC:                             3062.\nDf Residuals:                     411   BIC:                             3074.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     49.8856      0.968     51.547      0.000      47.983      51.788\nhouse_age     -0.2310      0.042     -5.496      0.000      -0.314      -0.148\ndist_MRT      -0.0072      0.000    -18.997      0.000      -0.008      -0.006\n==============================================================================\nOmnibus:                      161.397   Durbin-Watson:                   2.130\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1297.792\nSkew:                           1.443   Prob(JB):                    1.54e-282\nKurtosis:                      11.180   Cond. No.                     3.37e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.37e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe estimated equation for the model is\n\\[\nY = 49.89 - 0.23 X_1 - 0.0072 X_2\n\\]\nAt 5%-level, the estimates for both \\(\\beta_1\\) and \\(\\beta_2\\) are significantly different from 0. Moreover, the adjusted \\(R^2\\) is now a more respectable 0.49.\n\n\n\nExample 5.5 (Example: Broken Line Regression) \nAlthough multiple linear regression is usually carried out with distinct variables, it is possible to include functions of the same variable. Suppose we define:\n\n\\(X_1\\): House age\n\\(X_2\\): Distance to MRT\n\\(X_3\\) to be as follows:\n\n\\[\nX_3 =\n\\begin{cases}\n0 & \\text{ if } X_1 \\le 25, \\\\\nX_1 - 25 &\\text{ if } X_1 &gt; 25\n\\end{cases}\n\\]\nThis allows a non-linear function of house age to be included in the model. It is similar to including a polynomial term, but this is simpler to interpret.\n\nre2['x3'] = [(x - 25) if x &gt; 25 else 0 for x in re2.house_age]\n\nlm_age_mrt_2 = ols('price ~ house_age + x3 + dist_MRT', data=re2).fit()\nprint(lm_age_mrt_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.527\nModel:                            OLS   Adj. R-squared:                  0.523\nMethod:                 Least Squares   F-statistic:                     152.0\nDate:                Tue, 23 Sep 2025   Prob (F-statistic):           3.24e-66\nTime:                        09:16:24   Log-Likelihood:                -1512.9\nNo. Observations:                 414   AIC:                             3034.\nDf Residuals:                     410   BIC:                             3050.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     52.9935      1.090     48.598      0.000      50.850      55.137\nhouse_age     -0.6047      0.079     -7.674      0.000      -0.760      -0.450\nx3             1.1553      0.209      5.533      0.000       0.745       1.566\ndist_MRT      -0.0065      0.000    -16.644      0.000      -0.007      -0.006\n==============================================================================\nOmnibus:                      165.669   Durbin-Watson:                   2.120\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1558.270\nSkew:                           1.436   Prob(JB):                         0.00\nKurtosis:                      12.060   Cond. No.                     3.95e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.95e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nWe’ve managed to improve the adjusted \\(R^2\\) by a little, but let’s focus on understanding the estimated relationship. To help us out, let’s fix the distance-to-MRT to be two values: 300m, and 1500m. Roughly speaking, these values correspond to the 1st and 3rd quartiles of the distance values. For a given distance value, we then have two sub-cases:\n\nHouse age less than or equal to 25 years, and\nHouse age more than 25 years.\n\nThe full equation is as follows:\n\\[\nY = 53.00 - 0.60 X_1 - 0.0065 X_2 + 1.16 X_3\n\\]\nThe special cases correspond to:\n\n\\(X_2 = 300\\):\n\n\\(X_1 &lt;= 25\\):\n\n\\(Y = 51.05 - 0.60 X_1\\)\n\n\\(X_1 &gt; 25\\):\n\n\\(Y = 22.05 + 0.56 X_1\\)\n\n\n\\(X_2 = 1500\\):\n\n\\(X_1 &lt;= 25\\):\n\n\\(Y = 43.25 - 0.60 X_1\\)\n\n\\(X_1 &gt; 25\\):\n\n\\(Y = 14.25 + 0.56 X_1\\)\n\n\n\nVisually, here is what the estimated lines look like:\n\nnew_df = sm.add_constant(pd.DataFrame({'house_age' : np.linspace(0, 45, 100)}))\nnew_df['x3'] = [(x - 25) if x &gt; 25 else 0 for x in new_df.house_age]\nnew_df2 = new_df.copy()\n\nnew_df['dist_MRT'] = 300\nnew_df2['dist_MRT'] = 1500\n\npredictions_out = lm_age_mrt_2.get_prediction(new_df)\npredictions_out2 = lm_age_mrt_2.get_prediction(new_df2)\n\n\nax = re2.plot(x='house_age', y='price', kind='scatter', alpha=0.5 )\nax.set_title('Price vs. age');\nax.plot(new_df.house_age, predictions_out.predicted_mean, \n        color='blue', linestyle='dashed', label='dist = 300m');\nax.plot(new_df2.house_age, predictions_out2.predicted_mean, \n        color='red', linestyle='dashed', label='dist = 1500m');\nax.legend();",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#including-a-categorical-variable",
    "href": "05-regression.html#including-a-categorical-variable",
    "title": "5  Linear Regression",
    "section": "5.4 Including a Categorical Variable",
    "text": "5.4 Including a Categorical Variable\nThe explanatory variables in a linear regression model do not need to be continuous. Categorical variables can also be included in the model. In order to include them, they have to be coded using dummy variables.\nFor instance, suppose that we wish to include gender in a model as \\(X_3\\). There are only two possible genders in our dataset: Female and Male. We can represent \\(X_3\\) as an indicator variable, with\n\\[\nX_{3,i} =\n\\begin{cases}\n1 & \\text{individual $i$ is male}\\\\\n0 & \\text{individual $i$ is female}\n\\end{cases}\n\\]\nThe model (without subscripts for the \\(n\\) individuals) is then: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + e\n\\] For females, the value of \\(X_3\\) is 0. Hence the model reduces to \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\] On the other hand, for males, the model reduces to \\[\nY = (\\beta_0 + \\beta_3) + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\] The difference between the two models is in the intercept. The other coefficients remain the same.\nIn general, if the categorical variable has \\(a\\) levels, we will need \\(a-1\\) columns of indicator variables to represent it. This is in contrast to machine learning models which use one-hot encoding. The latter encoding results in columns that are linearly dependent if we include an intercept term in the model.\n\nExample 5.6 (Example: Price vs. Num Stores and Distance to MRT) \nFor this example, let us work with a reduced model in order to understand how things work. Price will remain the dependent variable, but we shall use distance to MRT (quantitative) and number of nearby convenience stores. However, we shall recode the number of stores as low (or high) corresponding to whether or not there were 4 stores or less (resp. more than 5).\n\nre2['num_stores_cat'] = ['low' if x &lt;= 4 else 'high' for x in re2.num_stores]\n\nlm_cat_1 = ols('price ~ dist_MRT + num_stores_cat', re2).fit()\nprint(lm_cat_1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.502\nModel:                            OLS   Adj. R-squared:                  0.500\nMethod:                 Least Squares   F-statistic:                     207.3\nDate:                Tue, 23 Sep 2025   Prob (F-statistic):           5.54e-63\nTime:                        09:16:25   Log-Likelihood:                -1523.3\nNo. Observations:                 414   AIC:                             3053.\nDf Residuals:                     411   BIC:                             3065.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept                47.8055      0.696     68.679      0.000      46.437      49.174\nnum_stores_cat[T.low]    -7.4086      1.171     -6.325      0.000      -9.711      -5.106\ndist_MRT                 -0.0055      0.000    -11.913      0.000      -0.006      -0.005\n==============================================================================\nOmnibus:                      190.015   Durbin-Watson:                   2.138\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2327.960\nSkew:                           1.618   Prob(JB):                         0.00\nKurtosis:                      14.157   Cond. No.                     4.31e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.31e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe categorical variable has been coded (by Python) as follows:\n\\[\nX_4 =\n\\begin{cases}\n1, & \\text{if there were 4 nearby stores or less} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\nAs a result, we have estimated two models:\n\nCorresponding to a large number of nearby stores: \\(Y = 47.81 - 0.0055 X_2\\)\nCorresponding to a small number of nearby stores: \\(Y = 40.40 - 0.0055 X_2\\)\n\n\n\nIncluding an Interaction Term\nA more complex model arises from an interaction between two terms. Here, we shall consider an interaction between a continuous variable and a categorical explanatory variable. Suppose that we have three predictors: height (\\(X_1\\)), weight (\\(X_2\\)) and gender (\\(X_3\\)). As spelt out in the previous section, we should use indicator variables to represent \\(X_3\\) in the model.\nIf we were to include an interaction between gender and weight, we would be allowing for a males and females to have separate coefficients for \\(X_2\\). Here is what the model would appear as: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_2 X_3 + e\n\\] Remember that \\(X_3\\) will be 1 for males and 0 for females. The simplified equation for males would be:\n\\[\nY = (\\beta_0 + \\beta_3) + \\beta_1 X_1 + (\\beta_2 + \\beta_4) X_2 + e\n\\] For females, it would be: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n\\]\nBoth the intercept and coefficient of \\(X_2\\) are different, for each value of \\(X_3\\). Recall that in the previous section, only the intercept term was different.\n\nExample 5.7 (Example: Interaction between Num of Stores and Distance to MRT) \n\nlm_cat_2 = ols('price ~ dist_MRT * num_stores_cat', re2).fit()\nprint(lm_cat_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.538\nModel:                            OLS   Adj. R-squared:                  0.534\nMethod:                 Least Squares   F-statistic:                     158.9\nDate:                Tue, 23 Sep 2025   Prob (F-statistic):           2.54e-68\nTime:                        09:16:25   Log-Likelihood:                -1508.0\nNo. Observations:                 414   AIC:                             3024.\nDf Residuals:                     410   BIC:                             3040.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==================================================================================================\n                                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------------------\nIntercept                         54.8430      1.425     38.499      0.000      52.043      57.643\nnum_stores_cat[T.low]            -14.9553      1.759     -8.504      0.000     -18.412     -11.498\ndist_MRT                          -0.0278      0.004     -6.948      0.000      -0.036      -0.020\ndist_MRT:num_stores_cat[T.low]     0.0226      0.004      5.602      0.000       0.015       0.031\n==============================================================================\nOmnibus:                      215.525   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             3236.645\nSkew:                           1.844   Prob(JB):                         0.00\nKurtosis:                      16.192   Cond. No.                     1.10e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.1e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nNotice that we now have the largest adjusted \\(R^2\\) out of all of the models we have fit so far.\nThe model that we have fit consists of two models:\n\nFor the case that the number of stores is 4 or less: \\(Y = 39.88 - 0.005 X_2\\)\nFor the case that the number of stores is more than 4: \\(Y = 54.84 - 0.028 X_2\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan you interpret the result above based on your intuition or understanding of real estate prices?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#residual-analysis",
    "href": "05-regression.html#residual-analysis",
    "title": "5  Linear Regression",
    "section": "5.5 Residual Analysis",
    "text": "5.5 Residual Analysis\nRecall from earlier that residuals are computed as \\[\nr_i = Y_i - \\hat{Y_i}\n\\] Residual analysis is a standard approach for identifying how we can improve a model. In the case of linear regression, we can use the residuals to assess if the distributional assumptions hold. We can also use residuals to identify influential points that are masking the general trend of other points. Finally, residuals can provided some direction on how to improve the model.\n\nStandardised Residuals\nIt can be shown that the variance of the residuals is in fact not constant! Let us define the hat-matrix as \\[\n\\textbf{H} = \\textbf{X}(\\textbf{X}'\\textbf{X} )^{-1} \\textbf{X}'\n\\] The diagonal values of \\(\\textbf{H}\\) will be denoted \\(h_{ii}\\), for \\(i = 1, \\ldots, n\\). It can then be shown that \\[\nVar(r_i) = \\sigma^2 (1 - h_{ii}), \\quad Cov(r_i, r_j) = -\\sigma^2 h_{ij}\n\\] As such, we use the standardised residuals when checking if the assumption of Normality has been met.\n\\[\nr_{i,std}  = \\frac{r_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}\n\\] If the model fits well, standardised residuals should look similar to a \\(N(0,1)\\) distribution. In addition, large values of the standardised residual indicate potential outlier points.\nBy the way, \\(h_{ii}\\) is also referred to as the leverage of a point. It is a measure of the potential influence of a point (on the parameters, and future predictions). \\(h_{ii}\\) is a value between 0 and 1. For a model with \\(p\\) parameters, the average \\(h_{ii}\\) should be should be \\(p/n\\). We consider points for whom \\(h_{ii} &gt; 2 \\times p/n\\) to be high leverage points.\nIn the literature and in textbooks, you will see mention of residuals, standardised residuals and studentised residuals. While they differ in definitions slightly, they typically yield the same information. Hence we shall stick to standardised residuals for our course.\n\nExample 5.8 (Example: Normality Check for lm_age_mrt_1) \nOne of the first checks for Normality is to create a histogram. If the residuals adhere to a Normal distribution, we should observe a symmetric bell-shaped distribution.\n\nr_s = pd.Series(lm_age_mrt_1.resid_pearson)\nr_s.hist();\n\n\n\n\n\n\n\n\nFrom above, it appears that the distribution is slightly skewed, and there is one noticeable outlier. A second graphical diagnostic plot that we make is a QQ-plot.\nA QQ-plot plots the standardized sample quantiles against the theoretical quantiles of a N(0; 1) distribution. If they fall on a straight line, then we would say that there is evidence that the data came from a normal distribution.\nEspecially for unimodal datasets, the points in the middle will fall close to the line. The value of a QQ-plot is in judging if the tails of the data are fatter or thinner than the tails of the Normal.\n\nsm.qqplot(r_s[r_s &lt; 6], line=\"q\");\n\n\n\n\n\n\n\n\n\nOverall, the residuals do indicate a lack of Normal behaviour. Non-normality in the residuals should lead us to view the hypothesis tests with caution. The estimated models are still valid, in the sense that they are optimal. Estimation of the models did not require the assumption of Normality. So far, we have only focused on inspecting the \\(R^2\\) to assess the model quality.\n\n\nScatterplots\nTo understand the model fit better, a set of scatterplots are typically made. These are plots of standardised residuals (on the \\(y\\)-axis) against\n\nfitted values\nexplanatory variables, one at a time.\npotential variables.\n\nResiduals are meant to contain only the information that our model cannot explain. Hence, if the model is good, the residuals should only contain random noise. There should be no apparent pattern to them. If we find such a pattern in one of the above plots, we would have some clue as to how we could improve the model.\nWe typically inspect the plots for the following patterns:\n\n\n\nResiduals\n\n\n\nA pattern like the one on the extreme left is ideal. Residuals are randomly distributed around zero; there is no pattern or trend in the plot.\nThe second plot is something rarely seen. It would probably appear if we were to plot residuals against a new variable that is not currently in the model. If we observe this plot, we should then include this variable in the model.\nThis plot indicates we should include a quadratic term in the model.\nThe wedge shape (or funnel shape) indicates that we do not have homoscedascity. The solution to this is either a transformation of the response, or weighted least squares. You will cover these in your linear models class.\n\n\nExample 5.9 (Example: Residual Plots for lm_age_mrt_2) \nLet us extract and create the residual plots for the second model that we had fit, earlier.\n\nplt.figure(figsize=(12,4));\nr_s = lm_age_mrt_2.resid_pearson\nax=plt.subplot(121)\nax.scatter(re2.dist_MRT, r_s, alpha=0.5)\nax.set_xlabel('dist_mrt')\nax.axhline(y=0, color='red', linestyle='--')\n\nax=plt.subplot(122)\nax.scatter(re2.house_age, r_s, alpha=0.5)\nax.set_xlabel('house age');\nax.axhline(y=0, color='red', linestyle='--');\n\n\n\n\n\n\n\n\nWhile the plot for house age looks acceptable (points are evenly scattered about the red dashed line), the one for distance shows some curvature. This is something we can try to fix, using a transformation of the x-variable.\n\n\n\nInfluential Points\nThe influence of a point on the inference can be judged by how much the inference changes with and without the point. For instance to assess if point \\(i\\) is influential on coefficient \\(j\\):\n\nEstimate the model coefficients with all the data points.\nLeave out the observations \\((Y_i , X_i)\\) one at a time and re-estimate the model coefficients.\nCompare the \\(\\beta\\)’s from step 2 with the original estimate from step 1.\n\nWhile the above method assesses influence on parameter estimates, Cook’s distance performs a similar iteration to assess the influence on the fitted values. Cook’s distance values greater than 1 indicate possibly influential points.\nThere are several ways to deal with influential points. First, we can remove the influential point (or sets of points) and asssess how much the model changes. Based on our understanding of the domain, we can then decide to keep or remove those points. A second approach is to create a dummy variable that identifies those points (individually). Fitting the subsequent model allows all points to be used in estimating standard errors, but quantifies an adjustment for those points. A third approach is to use a robust linear model. This is a model that automatically reduces the influence of aberrant points. This is a good topic to know about - do read up on it if you are keen!\n\nExample 5.10 (Example: Influential Points for lm_age_mrt_2) \nThe influence of a point on the inference can be judged by how much the inference changes with and without the point. For instance to assess if point \\(i\\) is influential on coefficient \\(j\\):\n\nEstimate the model coefficients with all the data points.\nLeave out the observations \\((Y_i , X_i)\\) one at a time and re-estimate the model coefficients.\nCompare the \\(\\beta\\)’s from step 2 with the original estimate from step 1.\n\nWhile the above method assesses influence on parameter estimates, Cook’s distance performs a similar iteration to assess the influence on the fitted values. Cook’s distance values greater than 1 indicate possibly influential points.\n\ninfl = lm_age_mrt_2.get_influence()\ninfl_df = infl.summary_frame()\n\n\nprint(infl_df.head())\n\n   dfb_Intercept  dfb_house_age    dfb_x3  dfb_dist_MRT   cooks_d  \\\n0       0.004477      -0.015423  0.004746      0.015011  0.000249   \n1      -0.004122       0.022294 -0.024375     -0.017434  0.000236   \n2       0.016558       0.009214 -0.017881     -0.018001  0.000405   \n3       0.037359       0.020789 -0.040346     -0.040616  0.002054   \n4      -0.037629       0.024643 -0.013610      0.006609  0.000375   \n\n   standard_resid  hat_diag  dffits_internal  student_resid    dffits  \n0       -0.350312  0.008050        -0.031558      -0.349937 -0.031524  \n1        0.319229  0.009195         0.030754       0.318879  0.030720  \n2        0.638887  0.003955         0.040257       0.638426  0.040228  \n3        1.438604  0.003955         0.090648       1.440488  0.090767  \n4       -0.463313  0.006946        -0.038748      -0.462869 -0.038711",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#transformation",
    "href": "05-regression.html#transformation",
    "title": "5  Linear Regression",
    "section": "5.6 Transformation",
    "text": "5.6 Transformation\n\nExample 5.11 (Example: Log-transformation) \nAs we observed in the residual plots, the distance-to-MRT variable displays a slight curvature. We can fix this by taking a log-transformation of the variable before fitting the model.\nBelow, we include the code to perform this fitting.\n\nre2['ldist'] = np.log(re2.dist_MRT)\n\nlm_age_mrt_3 = ols('price ~ house_age + x3 + num_stores + ldist', data=re2).fit()\nprint(lm_age_mrt_3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.597\nModel:                            OLS   Adj. R-squared:                  0.593\nMethod:                 Least Squares   F-statistic:                     151.6\nDate:                Tue, 23 Sep 2025   Prob (F-statistic):           2.07e-79\nTime:                        09:16:25   Log-Likelihood:                -1479.4\nNo. Observations:                 414   AIC:                             2969.\nDf Residuals:                     409   BIC:                             2989.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     84.0709      4.009     20.971      0.000      76.190      91.951\nhouse_age     -0.4941      0.075     -6.578      0.000      -0.642      -0.346\nx3             0.8599      0.197      4.355      0.000       0.472       1.248\nnum_stores     0.7815      0.201      3.886      0.000       0.386       1.177\nldist         -6.6595      0.559    -11.910      0.000      -7.759      -5.560\n==============================================================================\nOmnibus:                      233.692   Durbin-Watson:                   2.059\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             3847.453\nSkew:                           2.030   Prob(JB):                         0.00\nKurtosis:                      17.372   Cond. No.                         213.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nNow, take some to investigate the following issues:\n\nInterpret the coefficient for dist_MRT.\nHave the issues with the residuals been fixed?\nWhat is the difference between how this model uses num_stores, and how lm_cat_1 uses it?\nHow did we choose 25 as the breakpoint for house-age? Is it the ideal one?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#summary-further-topics",
    "href": "05-regression.html#summary-further-topics",
    "title": "5  Linear Regression",
    "section": "5.7 Summary, Further topics",
    "text": "5.7 Summary, Further topics\nLinear regression is a very flexible model. It is quick to fit, easily generalisable and much more interpretable than other models. These are some of the reasons why it is still one of the most widely used models in industry.\nIn our short session, we have touched on several practical tips for using regression models. However, take note that regression models can be generalised in many other ways. Here are some models you may want to read up on:\n\nAssuming correlated errors instead of independent error terms\nUsing splines to include non-linear functions of explanatory variables.\nKernel regression is an even more modern method for including higher-order terms, but at this point we start to lose interpretability\nConstrained regression, when we know certain coefficients should be positive, for instance.\nRobust linear models to automagically take care of wild outliers.\n\nGood reference textbooks for this topic are Draper (1998) and Hastie, Tibshirani, and Friedman (2009).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-regression.html#references",
    "href": "05-regression.html#references",
    "title": "5  Linear Regression",
    "section": "5.8 References",
    "text": "5.8 References\n\nWebsite References\n\nTaiwan dataset from UCI machine learning repository\nStats models documentation\nDiagnostics\nOn residual plots\n\n\n\n\n\nDraper, NR. 1998. Applied Regression Analysis. McGraw-Hill. Inc.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. “An Introduction to Statistical Learning.”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "06-ts.html",
    "href": "06-ts.html",
    "title": "6  Time Series Analysis",
    "section": "",
    "text": "6.1 Exploring Time Series Data\nOne of the first things that we do when we are given a time series is visualise it. The visualisation is meant to give us an indication of what kinds of techniques would be suitable for forecasting it. In this section, we shall learn several methods to visualise a time series dataset.\nWhen we visualise a time series, we look out for the following features:\nimport pandas as pd\nimport numpy as np\nimport datetime, calendar\nfrom statsmodels.tsa.seasonal import seasonal_decompose,STL\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.tsa.forecasting.theta import ThetaModel\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace import exponential_smoothing\n\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt, STLForecast\nimport pmdarima as pm\n\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist,squareform\n\nfrom ind5003 import ts\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nWith the resample() function, we can perform both downsampling (reducing the frequency of observations) or upsampling (via interpolation or nearest neighbour imputation).\nhsales.resample('14D').interpolate().head()\n#hsales.head()\n\n\n\n\n\n\n\n\nhsales\n\n\ndate\n\n\n\n\n\n1973-01-01\n55.000000\n\n\n1973-01-15\n54.820513\n\n\n1973-01-29\n54.641026\n\n\n1973-02-12\n54.461538\n\n\n1973-02-26\n54.282051\nMost time series models are autoregressive in nature. This means that they attempt to predict future observations based on previous ones. The observations from the past could be from the time series that we are interested in, or they could be from other time series that we believe are related.\nWhen beginning with model fitting, we would want to have some idea about the extent to which past observations affect the current one. In other words, how many of the past observations should we include when forecasting new observations? Lag plots and autocorrelation functions (acf) are the tools that we use to answer this question.\nIf we denote the observation of a time series as \\(y_t\\), then a lag plot at lag \\(k, k &gt; 0\\) is a scatter plot of \\(y_t\\) against \\(y_{t-k}\\). It allows us to visually assess if the observations that are \\(k\\) units of time apart are associated with one another. We typically plot several lags at once to observe this relationship.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#exploring-time-series-data",
    "href": "06-ts.html#exploring-time-series-data",
    "title": "6  Time Series Analysis",
    "section": "",
    "text": "Trend: A trend exists when there is a long-term increase or decrease in the data.\nLevel: The level of a series refers to its height on the ordinate axis.\nSeasonal: A seasonal pattern exists when a series is influenced by factors such as quarters of the year, the month, the day of the week, or time of day. Seasonality is always of a fixed and known period.\nCyclic: A cyclic pattern exists when there are rises and falls that are not of a fixed period.\n\n\n\nExample 6.1 (Example: Basic Plots of Housing Data) \n\nhsales = pd.read_csv('data/hsales.csv', parse_dates=[0])\nhsales.set_index('date', inplace=True)\nhsales.index.freq = 'MS'\nhsales.plot(title='Sales of One-Family Houses, USA', legend=False, figsize=(12,5))\nplt.xlabel('Month-Year'); plt.ylabel('Sales');\n\n\n\n\n\n\n\n\nWe observe a strong seasonality within each year. There is some indication of cyclic behaviour every 6 – 10 years. There is no apparent monotonic trend over this period. With pandas objects, we can resample the series. This allows us to compute summaries over time windows that could be of business importance. For instance, we might be interested in a breakdown by quarters instead of months.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSee the page on date-offset objects for details and options on the strings that you can put within the resample method.\n\n\n\nExample 6.2 (Example: Season Plots of Housing Data) \nA season plot allows us to visualise what happens within a season. In this case, each line in the graph below traces the behaviour of the series from the beginning of the season till the end (from Jan to Dec).\n\nhsales.loc[:, 'year'] = hsales.index.year\nhsales.loc[:, 'month'] = hsales.index.month\n\nyrs = np.sort(hsales.year.unique())\ncolor_ids = np.linspace(0, 1, num=len(yrs))\ncolors_to_use = plt.cm.YlOrRd(color_ids)\n\nplt.figure(figsize=(12, 8))\n\nfor i,yr in enumerate(yrs):\n    df_tmp = hsales.loc[hsales.year == yr, :]\n    plt.plot(df_tmp.month, df_tmp.hsales, color=colors_to_use[i]);\n    plt.text(12.1, df_tmp.hsales.iloc[-1], str(yr), color=colors_to_use[i])\nplt.title('Season Plot: House Sales')\nplt.xlim(0, 13)\nplt.xticks(np.arange(1, 13), calendar.month_abbr[1:13]);\n\n\n\n\n\n\n\n\n\n\nExample 6.3 (Example: Basic Time Plot, Australian Quarterly Electricity Production) \n\nqau = pd.read_csv('data/qauselec.csv', parse_dates=[0])\nqau.set_index('date', inplace=True)\nqau.index.freq = 'QS'\n\nqau.plot(figsize=(8,5), title='Electrcity Production by Quarter', legend=False)\nplt.xlabel('Qtr-Year'); plt.ylabel('Billion kWh');\n\n\n\n\n\n\n\n\nThere is a strong increasing trend. There is strong seasonality, and there is no evidence of cyclic behaviour.\nIn general, it looks like there is a peak around Feb to March, after which sales descend until the next January. Using a colour map with colours that we can remember would allow us to identify a trend across seasons. In this case, there isn’t one, but if you re-do this plot for the Electricity series, you would see a clear association between the colour of line and level of each series within a year.\n\nqau2 = qau.copy()\n\nqau2.loc[:, 'year'] = qau2.index.year\nqau2.loc[:, 'qtr'] = qau2.index.quarter\n\n\nyrs = np.sort(qau2.year.unique())\ncolor_ids = np.linspace(0, 1, num=len(yrs))\ncolors_to_use = plt.cm.YlOrRd(color_ids)\n\nplt.figure(figsize=(12, 8))\n\nfor i,yr in enumerate(yrs):\n    df_tmp = qau2.loc[qau2.year == yr, :]\n    plt.plot(df_tmp.qtr, df_tmp.kWh, color=colors_to_use[i]);\n    plt.text(4.1, df_tmp.kWh.iloc[-1], str(yr), color=colors_to_use[i])\nplt.title('Season Plot: House Sales')\nplt.xlim(0, 5)\nplt.xticks(np.arange(1, 5), ['Q1', 'Q2', 'Q3', 'Q4']);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.4 (Example: Lag plots of Housing Sales Data) \n\nf, aa = plt.subplots(nrows=3,ncols=4, sharex=True, sharey=True)\nf.set_figheight(10)\nf.set_figwidth(12)\n\ny = hsales.hsales.values\nfor i in np.arange(0, 3):\n    for j in np.arange(0, 4):\n        lag = i*4 + j + 1\n        aa[i,j].scatter(y[:-lag], y[lag:], alpha=0.4)\n        aa[i,j].set_xlabel(\"lag \" + str(lag))\nf.suptitle('Lag plots');",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#decomposing-time-series-data",
    "href": "06-ts.html#decomposing-time-series-data",
    "title": "6  Time Series Analysis",
    "section": "6.2 Decomposing Time Series Data",
    "text": "6.2 Decomposing Time Series Data\nIn this section, our goal is still primarily exploratory, but it will also return information that we can use to model our data later on. We aim to break the time series readings into:\n\na trend-cycle component,\na seasonal component, and\na remainder component.\n\nIf we can forecast these components individually, we can combine them to provide forecasts for the original series. There are two basic methods of decomposition into the above components: an additive one, and a multiplicative one. In the additive decomposition, we assume that they contribute in an additive manner to \\(y_t\\):\n\\[\\begin{equation}\ny_t = S_t + T_t + R_t\n\\end{equation}\\]\nIn the multiplicative decomposition, we assume the following relationship holds: \\[\\begin{equation}\ny_t = S_t \\times T_t \\times R_t\n\\end{equation}\\]\nWhen we are in the multiplicative case, we sometimes apply the logarithm transform to the data, which returns to an additive model: \\[\\begin{equation}\n\\log y_t = \\log S_t + \\log T_t + \\log R_t\n\\end{equation}\\]\nA typical algorithm to decompose time series data would work like this:\n\nEstimate the trend component. Let us call it \\(\\hat{T}_t\\).\nObtain the de-trended data \\(y_t - \\hat{T}_t\\) or \\(y_t / \\hat{T}_t\\) as appropriate.\nEstimate the seasonal component. For instance, we could simply average the values in each month. Let us denote this estimate as \\(\\hat{S}_t\\).\nEstimate the remainder component. For instance, in the additive model, it would be \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t\\).\n\n\nExample 6.5 (Example: Additive Decomposition, Housing Sales) \nHere is the naive additive decomposition of the housing sales data.\n\nhsales_add = seasonal_decompose(hsales.loc[:, 'hsales'], \n                                model='additive', extrapolate_trend='freq')\nhsales_add.plot();\n\n\n\n\n\n\n\n\n\n\nExample 6.6 (Example: Multiplicative Decomposition, Aus Electricity Data) \nHere is the *multiplicative decomposition for the Australian electric data.\n\nqau_mult = seasonal_decompose(qau.loc[:, 'kWh'], model='multiplicative',\n                              extrapolate_trend='freq')\nqau_mult.plot();\n\n\n\n\n\n\n\n\n\nAs you may have noticed, there are some issues with the classical seasonal decomposition algorithms. These include:\n\nan inability to estimate the trend at the ends of the series.\nan inability to account for changing seasonal components.\nit is not robust to outliers.\n\n\nExample 6.7 (Example: STL decomposition, Aus Electricity) \nNewer algorithms such as STL decomposition, X11 and others attempt to address these issues. They use locally weighted regression models to obtain the trend. These algorithms iterate over the time series several times, so as to ensure that outliers are not affecting the outcome.\n\nqau_stl = STL(qau.kWh).fit()\nqau_stl.plot();",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#forecasting",
    "href": "06-ts.html#forecasting",
    "title": "6  Time Series Analysis",
    "section": "6.3 Forecasting",
    "text": "6.3 Forecasting\n\nBenchmark methods\nAs in all forecasting methods, it is useful to obtain a baseline forecast before proceeding to more sophisticated techniques. Baseline forecasts are usually obtained from simple, intuitive methods. Here are some such methods:\nA. The simple mean forecast:\n\\[\\begin{equation}\n\\hat{y}_{T+h | T} = \\frac{y_1 + y_2 + y_3 + \\cdots + y_T}{T}\n\\end{equation}\\]\nB. The naive forecast: \\[\\begin{equation}\n\\hat{y}_{T+h | T} = y_T\n\\end{equation}\\]\nC. The seasonal naive forecast.\n\nExample 6.8 (Example: Benchmark Forecasts Housing Sales) \nSuppose we apply some of the above forecasts to the housing sales dataset. We withhold the most recent 2 years of data and foreacst those. Here is a plot depicting the forecasts, and the true values.\n\n# Set aside the last two years as the test set.\n#hsales = hsales.drop(columns=['year', 'month'])\ntrain_set = hsales.iloc[:-24,]\ntest_set = hsales.iloc[-24:, ]\n\n# Obtain the forecast from the training set\nmean_forecast = ts.meanf(train_set.hsales, 24)\nsnaive_forecast = ts.snaive(train_set.hsales, 24, 12)\n\n# Plot the predictions and true values\nax = train_set.hsales.plot(title='Benchmarks', legend=False, figsize=(12,4.5))\ntest_set.hsales.plot(ax=ax, legend=False, style='--')\nmean_forecast.plot(ax=ax, legend=True, style='-')\nsnaive_forecast.plot(ax=ax, legend=False, style='-')\nplt.legend(labels=['train', 'test', 'mean', 'snaive'], loc='lower right');\n\n\n\n\n\n\n\n\n\nWhen assessing forecasts, there are a few different metrics that are typically applied. Each of them has it’s own set of pros and cons. If we denote the predicted value with \\(\\hat{y}_t\\), then these are the formulas for three of the most common error metrics used in time series forecasting\nA. RMSE: \\[\\begin{equation}\n\\sqrt{\\frac{1}{h} \\sum_{i=1}^h (y_{t+i} - \\hat{y}_{t+i})^2 }\n\\end{equation}\\]\nB. MAE \\[\\begin{equation}\n\\frac{1}{h} \\sum_{i=1}^h |y_{t+i} - \\hat{y}_{t+i}|\n\\end{equation}\\]\nC. Mean Absolute Scaled Error\n\\[\\begin{equation}\n\\frac{1}{h} \\sum_{i=1}^h \\frac{|y_{t+i} - \\hat{y}_{t+i}|}{ \\frac{1}{T-1} \\sum_{t=2}^T |y_t - y_{t-1}|}\n\\end{equation}\\]\nThe RMSE and MAE are scale dependent errors. It is difficult to compare the errors across, or to aggregate errors across different time series with it. Due to the square in the formula, the RMSE is quite sensitive to outliers. The MAE is more robust to outliers. The MASE is a scaled error - it allows us to compare the forecasting performance across time series. The other two metrics depend on the scale of the time series and hence do not allow us to make such comparisons.\n\nfor x in [ts.rmse, ts.mae]:\n    print(f\"{x.__name__},mean: {x(test_set.hsales.values, mean_forecast.values):.3f}\")\n    print(f\"{x.__name__},snaive: {x(test_set.hsales.values, snaive_forecast.values):.3f}\")\n    print('---')\n\nrmse,mean: 9.023\nrmse,snaive: 5.906\n---\nmae,mean: 7.562\nmae,snaive: 4.792\n---\n\n\nThe MASE for the seasonal naive forecast is as follows.\n\nts.mase(test_set.hsales.values,  snaive_forecast.values, train_set.values, \n        seasonality=12)\n\n1.5154940449933834\n\n\nIn our simple example, the seasonal naive model outperforms the simple mean forecast according to all the metrics but in general, things are not always this clear-cut.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#arima-models",
    "href": "06-ts.html#arima-models",
    "title": "6  Time Series Analysis",
    "section": "6.4 ARIMA Models",
    "text": "6.4 ARIMA Models\nNow let us turn to a huge class of models that have been utilised in time series forecasting since the 1960’s. They are known as ARIMA models. ARIMA stands for AutoRegressive Integrated Moving Average models. These models are appropriate for stationary processes. Stationarity is a technical term that refers to processes\n\nthat have a constant mean. This means that the process merely fluctuates about a fixed level over time.\nwhose covariance function does not change over time. This means that, for a fixed \\(h\\), the covariance between \\(y_t\\) and \\(y_{t+h}\\) is the same for all \\(t\\).\nwhose variance is constant over time.\n\nHow can we tell if a process is stationary or not? The following time series is not stationary. Why?\n\nExample 6.9 (Example: Dow Jones Index) \nConsider the following time series of the Dow Jones index.\n\ndj = pd.read_csv('data/dj.csv')\ndj.plot(legend=False, title='Dow Jones Index', figsize=(15,4));\n\n\n\n\n\n\n\n\nHowever, the following differenced version of the same series is: \\[\\begin{equation}\n\\Delta y_t = y_t - y_{t-1}\n\\end{equation}\\]\n\nddj = diff(dj)\nddj.plot(legend=False, title='Differenced Dow Jones', figsize=(15,4));\n\n\n\n\n\n\n\n\n\nARIMA models revolve around the idea that if we have a non-stationary series, we can transform it into a stationary one with a suitable number of differencing. A more general way of studying if a series is stationary is to plot it’s AutoCorrelation Function (ACF). The ARIMA method directly models the ACF. That is why it is so important for this class of models. The ACF of a stationary process should “die down” quickly. Here is the ACF of the Dow Jones data, before and after differencing.\n\nplt.subplot(211)\nplt.stem(acf(dj, fft=False))\nplt.title(\"Non-differenced\")\nplt.subplot(212)\nplt.stem(acf(ddj, fft=False));\nplt.title(\"Differenced Series\");\n\n\n\n\n\n\n\n\nNow suppose that, starting from our original series \\(y_t\\), we difference it a sufficient number of times and obtain a stationary series. Let’s call this \\(y'_t\\). The ARIMA model assumes that\n\\[\\begin{equation}\ny'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + \\cdots + \\phi_p y'_{t-p} + \\theta_1 e_{t-1} + \\cdots + \\theta_q e_{t-q}\n\\end{equation}\\]\n\nThe \\(e_j\\) correspond to unobserved innovations. They are typically assumed to be independent across time with a common variance.\nThe \\(\\phi\\) and \\(\\theta\\) terms are unknown coefficients to be estimated.\nIf \\(y'_t\\) was obtained by performing \\(d\\) successive differencings, then the above ARIMA model is referred to as an \\(\\text{ARIMA}(p,d,q)\\) model.\n\nIn olden days, the \\(p\\), \\(d\\) and \\(q\\) parameters were picked by the analyst after inspecting the ACF, PACF, and time plots of the differenced series. Today, we can iterate through a large number of them and pick the best according to a well-established criteria (AIC).\n\nExample 6.10 (Example: Auto ARIMA on Aus Electricity) \nLet us try out the ARIMA auto-fitting algorithm on the qau electricity usage dataset. We shall set aside the last three years of data as the test set. Recall that this is quarterly data. First, we establish the seasonal naive benchmark error for this dataset.\n\ntrain2 = qau.kWh[:-12]\ntest2 = qau.kWh[-12:]\n\nsnaive_f = ts.snaive(train2, 12, 4)\nprint(f\"The RMSE is approximately {ts.rmse(test2.values, snaive_f.values):.3f}.\")\n\nThe RMSE is approximately 2.545.\n\n\nLet us see if the automatically fitted ARIMA models can do better than this.\nHere is the summary of the final chosen model.\n\n#arima_m1 = pm.auto_arima(train2.values, seasonal=True, m=4, test='adf', \n#                         trace=False, suppress_warnings=True)\narima_m1.summary()\n\n\nSARIMAX Results\n\n\nDep. Variable:\ny\nNo. Observations:\n206\n\n\nModel:\nSARIMAX(2, 1, 2)x(1, 0, [1], 4)\nLog Likelihood\n-175.320\n\n\nDate:\nMon, 22 Sep 2025\nAIC\n364.640\n\n\nTime:\n23:34:09\nBIC\n387.901\n\n\nSample:\n0\nHQIC\n374.049\n\n\n\n- 206\n\n\n\n\nCovariance Type:\nopg\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nar.L1\n-0.1675\n0.613\n-0.273\n0.785\n-1.369\n1.034\n\n\nar.L2\n0.3262\n0.418\n0.781\n0.435\n-0.492\n1.145\n\n\nma.L1\n-0.1472\n0.604\n-0.244\n0.807\n-1.330\n1.036\n\n\nma.L2\n-0.6210\n0.589\n-1.054\n0.292\n-1.776\n0.534\n\n\nar.S.L4\n0.9889\n0.009\n116.030\n0.000\n0.972\n1.006\n\n\nma.S.L4\n-0.6195\n0.085\n-7.310\n0.000\n-0.786\n-0.453\n\n\nsigma2\n0.3110\n0.026\n12.139\n0.000\n0.261\n0.361\n\n\n\n\n\n\n\n\nLjung-Box (L1) (Q):\n0.19\nJarque-Bera (JB):\n21.89\n\n\nProb(Q):\n0.67\nProb(JB):\n0.00\n\n\nHeteroskedasticity (H):\n9.53\nSkew:\n-0.09\n\n\nProb(H) (two-sided):\n0.00\nKurtosis:\n4.59\n\n\n\nWarnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nOnce we have found the best-fitting model, we do what we do with any other model in data analytics: we have to inspect the residuals. The residuals should look like trash to us - there should be no clues about the data in them. The standardized residual in the top left should be consistently wide; it isn’t. This suggests some sort of transformation of the data before modeling might be appropriate.\nThe two plots on the off-diagonal are meant for us to assess if the residuals are Normally distributed with mean 0. They do indeed look like it.\nThe final plot, in the bottom right, displays an ACF with no spikes, indicating that the residuals are uncorrelated. This is precisely what we wished to see.\n\narima_m1.plot_diagnostics(figsize=(12,6));\n\n\n\n\n\n\n\n\nFinally, we assess the error on the test set. The out-of-sample performance is better than the naive methods.\n\nprint(f\"The RMSE on the test set is {ts.rmse(test2.values, arima_m1.predict(n_periods=12)):.3f}.\")\nprint(f\"The MASE on the test set is {ts.mase(test2.values, arima_m1.predict(n_periods=12), train2.values, seasonality=4):.3f}.\")\n\nThe RMSE on the test set is 1.929.\nThe MASE on the test set is 1.245.\n\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning:\n\n'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning:\n\n'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n\n\n\nLet us proceed to make a plot of the predictions.\n\nn_periods = 12\nfc, confint = arima_m1.predict(n_periods=n_periods, return_conf_int=True)\n\nff = pd.Series(fc, index=test2.index)\nlower_series = pd.Series(confint[:, 0], index=test2.index)\nupper_series = pd.Series(confint[:, 1], index=test2.index)\n\nplt.figure(figsize=(14, 5))\nplt.plot(train2[-36:])\nplt.plot(ff, color='red', label='Forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, color='gray', alpha=.15)\nplt.plot(test2, 'g--', label='True')\nplt.legend();\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning:\n\n'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning:\n\n'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#ets",
    "href": "06-ts.html#ets",
    "title": "6  Time Series Analysis",
    "section": "6.5 ETS",
    "text": "6.5 ETS\nETS models are a new class of models, that define a time series process according to a set of state space equations:\n\\[\\begin{eqnarray}\ny_t &=& w' x_{t-1} + e_t \\\\\nx_t &=& F x_{t-1} + g e_t\n\\end{eqnarray}\\]\nThe unobserved state \\(x_t\\) is usually a vector, and it typically contains information about * the level of the process at time \\(t\\). * the seasonal effect at time \\(t\\) * the trend or growth factor at time \\(t\\).\nAll these parameters are updated at every point in time. For instance, the simplest ETS model (A,N,N), is one that assumes that there is only a level parameter, and that it is updated as a weighted average of the most recent levels:\n\\[\\begin{eqnarray}\nl_t &=& \\alpha y_t + (1- \\alpha)l_{t-1} \\\\\n\\hat{y}_{t+1} &=& l_t\n\\end{eqnarray}\\]\nThe definition of the model is very flexible, and allows multiplicative growth, linear growth and damped terms in the model. All this, in addition to not requiring the assumption of stationarity!\nHere is a full table of the additive models, followed by the multiplicative models:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting: Principles and Practice\n\n\n\n\n\n\nExample 6.11 (Example: ETS model on Aus Electricity Dataset) \nLet us try one of the basic models, with just a slope, level and seasonal effect ETS(A,A,A), on the qau dataset.\n\nets_m1 = ExponentialSmoothing(train2.values, seasonal_periods=4, trend='add', seasonal='add')\nets_fit = ets_m1.fit()\n\n\nets_fit.summary()\n\n\nExponentialSmoothing Model Results\n\n\nDep. Variable:\nendog\nNo. Observations:\n206\n\n\nModel:\nExponentialSmoothing\nSSE\n64.907\n\n\nOptimized:\nTrue\nAIC\n-221.915\n\n\nTrend:\nAdditive\nBIC\n-195.292\n\n\nSeasonal:\nAdditive\nAICC\n-220.787\n\n\nSeasonal Periods:\n4\nDate:\nMon, 22 Sep 2025\n\n\nBox-Cox:\nFalse\nTime:\n23:34:09\n\n\nBox-Cox Coeff.:\nNone\n\n\n\n\n\n\n\n\n\n\n\ncoeff\ncode\noptimized\n\n\nsmoothing_level\n0.6205891\nalpha\nTrue\n\n\nsmoothing_trend\n1.4395e-11\nbeta\nTrue\n\n\nsmoothing_seasonal\n0.2690339\ngamma\nTrue\n\n\ninitial_level\n4.0245281\nl.0\nTrue\n\n\ninitial_trend\n0.2509921\nb.0\nTrue\n\n\ninitial_seasons.0\n-0.3901302\ns.0\nTrue\n\n\ninitial_seasons.1\n0.1114788\ns.1\nTrue\n\n\ninitial_seasons.2\n0.2913054\ns.2\nTrue\n\n\ninitial_seasons.3\n-0.3722268\ns.3\nTrue\n\n\n\n\n\n\nplt.subplot(211)\nplt.plot(train2.index, ets_fit.trend, scaley=False)\nplt.ylim(0.245, 0.255)\nplt.ylabel('slope')\n\nplt.subplot(212)\nplt.plot(train2.index, ets_fit.level)\nplt.ylabel('level');\n\n\n\n\n\n\n\n\nFinally, let’s take a look if the predictions on the test set are competitive with the automatic ARIMA model that was selected earlier. The forecasts are better than the naive ones, but not as good as the ARIMA. However, keep in mind that we are working with a single model; we should iterate over several ETS models and pick the best one in order to compare with the ARIMA model found.\n\nets_fc = ets_fit.forecast(steps=12)\nprint(f\"The RMSE on th tests set is {ts.rmse(test2.values, ets_fc):.3f}.\")\n\nThe RMSE on th tests set is 2.237.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#theta-method",
    "href": "06-ts.html#theta-method",
    "title": "6  Time Series Analysis",
    "section": "6.6 Theta Method",
    "text": "6.6 Theta Method\nThe Theta method is a forecasting approach that has proved to be quite effective in competitions. It regularly appears as one of the best-performing models. Instead of decomposing a time series into Trend, Season and Remainder terms, the theta method decomposes a seasonally adjusted time series into short- and long-term components. The full reference for this method is Assimakopoulos and Nikolopoulos (2000).\nSuppose we are interested in forecasting a time series \\(\\{y_t\\}\\). The theta decomposition is based on the concept of amplifying/diminishing the local curvatures of the time series. This is brought about through a coefficient \\(\\theta\\); hence the name of the model.\nAs a preview, we are going to find series \\(\\{ x_{\\theta,t}\\}\\) and \\(\\{ x_{1 - \\theta,t}\\}\\) such that\n\\[\ny_t = \\frac{1}{2}(x_{\\theta,t} + x_{1 - \\theta,t})\n\\]\nIn order to forecast \\(y_t\\), we shall forecast \\(\\{ x_{\\theta,t}\\}\\) and \\(\\{ x_{1\n- \\theta,t}\\}\\) separately and then combine them. There are numerous possible pairs of \\(\\{ x_{\\theta,t}\\}\\) and \\(\\{ x_{1 - \\theta,t}\\}\\) that we can use. However the most common one is where \\(x_\\theta\\) corresponds to a straight line OLS fit, and \\(x_{1 - \\theta}\\) corresponds to a version of the time series with its curvature amplified.\n\nInternational Tourism Arrivals to Singapore\nThe Singapore Department of Statistics shares information on tourist arrivals to Singapore at the monthly level. Here is a time series plot of arrivals from South East Asian Countries:\n\ntourism = pd.read_excel(\"data/international_visitor_arrivals_sg.xlsx\", \n                        parse_dates=[0], date_format=\"%Y %b\", \n                        na_values='na')\nsea_tourism = tourism.iloc[:, 0:9]\nsea_tourism.columns = ['Date', 'SEA', 'Brunei', 'Indonesia' ,'Malaysia', \n                       'Myanmar', 'Philippines', 'Thailand', 'Vietnam']\nsea_tourism.Date = sea_tourism.Date.str.replace('  ', '')\nsea_tourism.Date = pd.to_datetime(sea_tourism.Date, format=\"%Y %b\")\nsea_tourism.set_index('Date', inplace=True)\nsea_tourism.sort_index(inplace=True)\nsea_tourism.index.freq = 'MS'\n\nsea2 = sea_tourism.SEA[sea_tourism.index &lt; datetime.datetime(2020, 1, 1)]\nfig = sea2.plot(figsize=(12,4));\nfig.set_title('Monthly Arrivals from SEA Countries before COVID-19');\n\n\n\n\n\n\n\n\n\ntheta_mod = ThetaModel(sea2)\ntheta_mod_fit = theta_mod.fit()\ntheta_mod_fit.summary()\n\n\nThetaModel Results\n\n\nDep. Variable:\nSEA\nNo. Observations:\n504\n\n\nMethod:\nOLS/SES\nDeseasonalized:\nTrue\n\n\nDate:\nMon, 22 Sep 2025\nDeseas. Method:\nMultiplicative\n\n\nTime:\n23:34:11\nPeriod:\n12\n\n\nSample:\n01-01-1978\n\n\n\n\n\n- 12-01-2019\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameters\n\n\nb0\n2628.8595906871597\n\n\nalpha\n0.8383848338839772\n\n\n\n\n\nHere is what the two components from the model look like, in comparison to the seasonally adjusted series (based on a multiplicative decomposition).\n\n\n\n\n\n\nNote\n\n\n\nThe calculations in the next cell are based on formulas for the theta decomposition that we won’t get into. They are just for illustration of how the decomposition is in terms of long and short term trends, rather than the cross-sectional decompositions we have been dealing with so far.\n\n\n\n# carry out mutliplicative decomposition in order to obtain seasonally adjusted series\nsea2_mult = seasonal_decompose(sea2, model='multiplicative', extrapolate_trend='freq')\n\n# get long-term trend slope and intercept\na0 = (sea2.mean() - theta_mod_fit.params['b0']*(503)/2)/1e6\nxvals = np.arange(1, 505)\nyvals1 = a0 + theta_mod_fit.params['b0']*(xvals - 1)\nyvals1 = pd.Series(data=yvals1, index=sea2.index)\n\n# get short term trend series\na2 = -1*a0\nb2 = -1*theta_mod_fit.params['b0']\nyvals2 = a2 + b2*(xvals - 1) + 2*(sea2_mult.resid + sea2_mult.trend)\nyvals2 = pd.Series(data=yvals2, index=sea2.index)\n\n\nfig = (sea2_mult.resid + sea2_mult.trend).plot(label='Seasonally adjusted', \n                                               legend=True, figsize=(12,4));\nyvals1.plot(legend=True, label='Long-term trend', style=\"--\")\nyvals2.plot(legend=True, label='Short-term trend', style=\"--\");\nfig.set_title(\"Theta decomposition\");\n\n\n\n\n\n\n\n\nFinally, we visualise the forecasts from this theta decomposition model.\n\ntheta_forecasts = pd.DataFrame(\n    {\n        \"original\": sea2,\n        \"theta forecast\": theta_mod_fit.forecast(24)\n    }\n)\n\n\ntheta_forecasts.tail(360).plot(figsize=(12,4));",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#forecasting-with-seasonal-decomposition",
    "href": "06-ts.html#forecasting-with-seasonal-decomposition",
    "title": "6  Time Series Analysis",
    "section": "6.7 Forecasting with Seasonal Decomposition",
    "text": "6.7 Forecasting with Seasonal Decomposition\nContinuing with the tourism data, we demonstrate how we can utilise a seasonal decomposition (not theta decomposition) model to make forecasts. Recall that a seasonal decomposition breaks the series down into the trend, seasonal and residual components. Seasonal decompositions are typically used to study a time series, but they can also be used to make forecasts. One approach is to use either ARIMA or ETS to predict the seasonally adjusted component, and then to use a seasonal naive model to predict the seasonal component. These two forecasts will then be combined to create the final forecast.\n\nstlf = STLForecast(sea2, ARIMA, model_kwargs={\"order\": (3, 1, 2)})\nres = stlf.fit( )\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning:\n\nNon-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n\n\n\n\nforecasts2 = pd.DataFrame(\n    {\n        \"original\": sea2,\n        \"dcmp forecast\": res.forecast(24)\n    }\n)\n\n\nforecasts2.tail(360).plot(figsize=(12,4));",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#miscellaneous-topics",
    "href": "06-ts.html#miscellaneous-topics",
    "title": "6  Time Series Analysis",
    "section": "6.8 Miscellaneous Topics",
    "text": "6.8 Miscellaneous Topics\n\nTime Series Clustering\n\nExample 6.12 (Example: Time Series Clustering, US Employment Data) \nThe Forecasting: Principles and Practice contains time series data on employment in various sectors in the US. There are 148 unique series in the dataset.\n\nus_employment = pd.read_csv(\"data/us_employment.csv\", parse_dates=[0], date_format=\"%Y %b\")\n\nseries_ids = us_employment.Series_ID.unique()\nprint(f\"There are {len(series_ids)} unique series in the dataset.\")\n#us_employment.Month.set_index\n\nus_employment.sample(n=8)\n\nThere are 148 unique series in the dataset.\n\n\n\n\n\n\n\n\n\nMonth\nSeries_ID\nTitle\nEmployed\n\n\n\n\n3498\n1988-04-01\nCEU1000000001\nMining and Logging\n764.0\n\n\n25500\n1964-07-01\nCEU3133410001\nDurable Goods: Computer and Peripheral Equipment\nNaN\n\n\n94614\n1990-10-01\nCEU6000000001\nProfessional and Business Services\n10945.0\n\n\n8065\n1965-02-01\nCEU1021210001\nMining and Logging: Coal Mining\nNaN\n\n\n13135\n1983-11-01\nCEU2023620001\nConstruction: Nonresidential Building\nNaN\n\n\n37974\n1954-04-01\nCEU3231500001\nNondurable Goods: Apparel\nNaN\n\n\n137888\n1963-03-01\nCEU9092200001\nGovernment: State Government, Excluding Education\n1164.3\n\n\n85433\n1952-06-01\nCEU5552200001\nFinancial Activities: Credit Intermediation an...\nNaN\n\n\n\n\n\n\n\nHere is a plot of one of the time series in the dataset.\n\nus_employment[us_employment.Title == \"Total Private\"].plot(x='Month', \n                                                           y='Employed', \n                                                           figsize=(12,4));\n\n\n\n\n\n\n\n\nSince we are going to perform clustering on the time series data, our first step is to remove the missing values.\n\nus_full = us_employment[(us_employment.Month &gt;= datetime.datetime(2002, 9, 30)) & (us_employment.Month &lt; datetime.datetime(2018, 1, 1) )   ]\nus2 = us_full.pivot(index='Series_ID', columns='Month', values=\"Employed\")\nus2_array = us2.to_numpy()\n\nNow we can begin the clustering procedure.\n\n# This already converts the correlation into a distance (see the help page)\nout = pdist(us2_array, metric='correlation')\n#corr_mat = np.corrcoef(X, rowvar=False)\n#dist_mat = (1 - corr_mat)/2\n\n\nlm1 = hierarchy.linkage(out, method='average', optimal_ordering=True)\n\nplt.figure(figsize=(12,5))\nhierarchy.dendrogram(lm1, p=3, truncate_mode='level',color_threshold=True);\n\n\n\n\n\n\n\n\nWe reorder the columns and rows in the matrix so that similar time series appear next to one another.\n\nX_ord = us2_array[hierarchy.leaves_list(lm1)]\ncorr_mat_ord = np.corrcoef(X_ord)\n#corr_mat_ord.shape\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr_mat_ord, vmin=-1, vmax=1, cmap='coolwarm_r', center=0);\n#, annot=True, cmap='coolwarm', center=0)\n\n\n\n\n\n\n\n\n\nIn the next few plots, we pick out series that are close to one another in blue segments of the matrix above to inspect how similar they are, and in what ways they are similar to one another.\n\nus2_series = us2.index.to_list()\n\n\n# Similar series set 1\n# ss = [us2_series[x] for x in [22, 24, 32, 33, 34]]\nss = [us2_series[x] for x in [143, 144, 145]]\nus2.T.loc[:, ss].plot(figsize=(12,4));\n\n# Similar series set 2\n#ss = us2_series[65:68]\n#us2.T.loc[:, ss].plot(logy=False, figsize=(12,4)); # try with and without log\n\n# Simular series set 3:\n\n#ss = [us2_series[x] for x in [92, 94, 96, 97, 98]]\n#us2.T.loc[:, ss].plot(logy=False, figsize=(12,4));",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#summary",
    "href": "06-ts.html#summary",
    "title": "6  Time Series Analysis",
    "section": "6.9 Summary",
    "text": "6.9 Summary\nWe have briefly covered the following time series concepts: Exploratory data analysis, assessing point forecasts and a few commonly used models. To finish up the section, we also applied clustering to time series. With today’s code, the model-fitting routines are easier to run. As always, the onus is on us as analysts to inspect the residuals from the models to understand the time series we are working with.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "06-ts.html#references",
    "href": "06-ts.html#references",
    "title": "6  Time Series Analysis",
    "section": "6.10 References",
    "text": "6.10 References\n\nStats models pages\n\nMain page\nForecasting with statsmodels\nTheta model\n\n\n\nForecasting principles and practice\nAlthough the code for this textboook uses R, the concepts are very well explained. It is written by one of the foremost experts in time series forecasting methods. The reference is Hyndman and Athanasopoulos (2018).\n\nForecasting: Principles and Practice\n\n\n\n\n\nAssimakopoulos, Vassilis, and Konstantinos Nikolopoulos. 2000. “The Theta Model: A Decomposition Approach to Forecasting.” International Journal of Forecasting 16 (4): 521–30.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting: Principles and Practice. OTexts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "07-simulation.html",
    "href": "07-simulation.html",
    "title": "7  Simulation",
    "section": "",
    "text": "7.1 Random Variables\nIn statistics, we use random variables to describe the probabilistic behaviour of phenomenon. Random variables are real numbers that represent the phenomena we observe. For instance, we might let \\(X\\) represent a coin toss, with \\(X=1\\) representing Heads and \\(X=0\\) representing Tails.\nRandom variables come with a rule (or function) that prescribes the probabilities with which it takes on particular values. For instance, if we had a fair coin, then the rule would be that \\[\nP(X=1) = P(X=0) = \\frac{1}{2}\n\\]\nOn the other hand, a biased coin might follow the rule \\[\nP(X=1) = \\frac{2}{3},\\quad P(X=0) = \\frac{1}{3}\n\\]\nThis rule tells us which events are more likely, and which are less likely.\nfrom math import pi \nfrom IPython.display import IFrame,Video\n\nimport mesa\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk import word_tokenize\n\nfrom numpy.random import default_rng\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import binom, bernoulli, norm, expon, uniform",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#random-variables",
    "href": "07-simulation.html#random-variables",
    "title": "7  Simulation",
    "section": "",
    "text": "Discrete vs. Continuous Random Variables.\nDiscrete random variables take on only a countable number of values. Examples are:\n\nA coin toss (\\(\\{0, 1\\}\\))\nThe number of taxis passing by a particular junction between 12noon and 1pm. (\\(\\{0, 1, 2, \\ldots, \\}\\))\nThe number of coin tosses until we observe Heads. (\\(\\{1, 2, 3, \\ldots, \\}\\))\n\nDiscrete random variables are defined by their probability mass function (pmf), which is just a table or a function describing \\(P(X=i)\\) for all possible \\(i\\) values. Once we know the pmf of a random variable, we know everything about it - the mean, variance, quantiles, maximum values, etc.\nWe can visualise a pmf using a bar-chart. Here is the pmf for the above biased coin.\n\nplt.bar([0,1], [1/3, 2/3], tick_label=['0', '1'], width=0.3);\nplt.xlim(-1,2); \nplt.ylim(0,1); \nplt.title('pmf of \\'X\\' random variable' );\n\n\n\n\n\n\n\n\nHere is the pmf for a random variable representing the total number of Heads after 10 tosses of that same coin. Suppose we call that new random variable \\(Y\\).\n\nprobs = binom.pmf(np.arange(0, 11), n=10, p=2/3)\nplt.bar(np.arange(0, 11), probs);plt.ylim(0,1);\nplt.title('pmf of no. of coin tosses');\n\n\n\n\n\n\n\n\nIf we wish to find the probability of events, for instance, \\(P(Y \\le 4)\\), we sum up the heights of the bars.\nContinuous random variables are defined by what is known as a probability density function. Continuous random variables can take on an uncountable number of values, for instance, all real numbers in the interval \\([0,1]\\).\n\nx = np.linspace(-3, 3, num=100)\ny = norm.pdf(x)\nplt.figure(figsize=(12,4), facecolor='0.9')\nplt.subplot(131)\nplt.plot(x,y)\nplt.ylim(0,1)\nplt.title('Normal pdf')\n\ny = expon.pdf(x[x&gt;0])\nplt.subplot(132)\nplt.plot(x[x&gt;0],y)\nplt.ylim(0,1)\nplt.title('Exponential pdf');\n\nx = np.linspace(0, 1, num=100)\ny = uniform.pdf(x)\nplt.subplot(133)\nplt.plot(x,y)\nplt.ylim(0,1.1)\nplt.title('Uniform pdf');\n\n\n\n\n\n\n\n\n\n\nGenerating Random Variates\nWith a computer, we can generate random variables from almost any distribution. There are built-in routines to generate from the ‘named’ distributions. For instance,\n\nrng = default_rng(5003)\n\nyvals = np.zeros((3,300))\nyvals[0,:] = rng.normal(size=300)\nyvals[1,:] = rng.exponential(size=300)\nyvals[2, :] = rng.uniform(size=300)\npdfs = ['Normal', 'Exponential', 'Uniform']\nplt.figure(figsize=(12,4), facecolor='0.9')\n\nfor i in np.arange(0,3):\n    plt.subplot(1, 3, i+1)\n    plt.hist(yvals[i,:], density=True, histtype='step')\n    plt.title(pdfs[i])\n    plt.ylim(0, 1.05)\n\n\n\n\n\n\n\n\nNotice how the (normalised) histograms look a lot like the corresponding pdfs.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#general-principles-in-simulation-studies",
    "href": "07-simulation.html#general-principles-in-simulation-studies",
    "title": "7  Simulation",
    "section": "7.2 General Principles in Simulation Studies",
    "text": "7.2 General Principles in Simulation Studies\n\nIntroduction\nThe objective of any simulation study is to estimate an expectation \\(E(X)\\). Simulation studies involve the use of a computer to generate independent copies of the random variable of interest \\(X\\). Here are a couple of examples where simulation studies would be applicable.\nThrough the estimation of \\(E(X)\\), simulation studies allow us to:\n\nConsider different planning scenarios and estimate the throughput from them.\nStudy the emergent behaviour of complex situations.\n\n\nExample 7.1 (Example: Insurance Claims) \nBefore the financial year begins, an insurance company has to decide how much cash to keep, in order to pay out the claims for that year. Suppose that claims are independent of each other and are distributed as \\(Exp(1/200)\\) dollars. This means that the probability density function (pdf) of the random variable is \\(f_X(x) = \\frac{1}{200} \\exp(-x/200),\\; x &gt; 0\\)\nAlso suppose that the number of claims in a year is a Poisson random variable with mean 8.2.\nAn actuary has been asked to determine the size of the reserve fund that should be set up, and he recommends $12,000. We might consider answering the following question using simulation:\n\nWhat is the probability that the total claims will exceed the reserve fund?\n\nIf we let \\(Y\\) be the random variable representing the total sum of claims, we are interested in estimating \\(P(Y &gt; 12000)\\). Since probabilities are expectations, we can use simulation to estimate this value.\n\n\nExample 7.2 (Example: Sandwich Shop Closing Time) \nHere is a slightly more sophisticated example.\nSuppose that you run a sandwich shop, which is open from 9am till 5pm. Your philosophy has always been to serve every customer who has entered before 5pm, even if that requires you to stay back until they have been served. You would like to estimate the mean amount of overtime you have to work.\nIf you are willing to assume that the inter-arrival times of customers is \\(Exp(3)\\) hours, then it is possible to simulate this process to estimate the mean time that you would have to remain open, beyond 5pm.\n\n\n\nSteps in a Simulation Study\nThe two examples above are known as Discrete Event Simulations. It is one type of simulation study. Another type of simulations are Agent-Based Models. No matter the type of simulation, the basic steps in a simulation study are:\n\nIdentify the random variable of interest and write a program to simulate it.\nGenerate an iid sample \\(X_1, X_2, \\ldots, X_n\\) using this program.\nEstimate \\(E(X)\\) using \\(\\bar{X}\\).\n\nBefore proceeding, let us refresh our knowledge of the properties of the sample mean.\n\n\nTheory\nThere are two important theorems that simulation studies rely on. The first is the Strong Law of Large Numbers (SLLN).\n\nStrong Law of Large Numbers\nIf \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed with \\(E(X) &lt; \\infty\\), then \\[\n\\bar{X} =\\frac{1}{n} \\sum_{i=1}^n X_i\\rightarrow E(X) \\quad \\text{with probability 1.}\n\\]\nIn the simulation context, it means that as we generate more and more samples (i.e. increase \\(n\\)), our sample mean \\(\\bar{X}\\) converges to the desired value \\(E(X)\\), no matter what the distribution of \\(X\\) is.\nThe second theorem that aids us is the Central Limit Theorem (CLT).\n\n\nCentral Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be i.i.d., and suppose that\n\n\\(-\\infty &lt; E(X_1) = \\mu &lt; \\infty\\).\n\\(Var(X_1) = \\sigma^2 &lt; \\infty\\).\n\nThen \\[\n\\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sigma} \\Rightarrow N(0,1)\n\\] where \\(\\Rightarrow\\) denotes convergence in distribution.\nThis is sometimes informally interpreted to mean that when \\(n\\) is large, \\(\\bar{X}\\) is approximately Normal with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). In the simulation context, we can use this theorem to obtain a confidence interval for the expectation that we are estimating.\nAlso take note of the following properties of the sample mean and variance:\n\n\nSample Estimates\nIt can be shown that both the sample mean and sample standard deviation are unbiased estimators. \\[\nE(\\bar{X}) = E(X), \\quad E(s^2) = \\sigma^2\n\\] where \\(s^2 = \\frac{\\sum (X_i - \\bar{X})^2}{n-1}\\). :::\nTo obtain a \\((1-\\alpha)100%\\) confidence interval for \\(\\mu\\), we use the following formula, from the CLT:\n\\[\n\\bar{X} \\pm z_{1-\\alpha/2} \\frac{s}{\\sqrt{n}}\n\\]\nWhen our goal is to estimate a probability \\(p\\), we have to introduce a corresponding indicator variable \\(X\\) such that\n\\[\nX =\n\\begin{cases}\n1 & \\text{with probability $p$} \\\\\n0 & \\text{with probability $1- p$}\n\\end{cases}\n\\]\nIn this case, the formula for the CI becomes \\[\n\\bar{X} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\bar{X}(1-\\bar{X})}{n}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#object-oriented-programming-in-python",
    "href": "07-simulation.html#object-oriented-programming-in-python",
    "title": "7  Simulation",
    "section": "7.3 Object-Oriented Programming in Python",
    "text": "7.3 Object-Oriented Programming in Python\nPython has been developed as both a functional and object-oriented programming language. Much of the code we will soon use for the mesa package involves creation of an instance, and then accessing the attributes (data or methods) of that instance.\nThe syntax for defining a class is:\nclass ClassName:\n    &lt;statement-1&gt;\n    .\n    .\n    &lt;statement-N&gt;\nA class definition typically consists of function definitions and attributes. A special function within a class definition, __init__(), can be used to set the initial state of instances of a class. Think of a class definition as a template, and instances as copies made using that template.\nConsider the following class Circle.\n\nclass Circle:\n    \"\"\" A simple class definition \n    \n    c0 = Circle()\n    c0.radius\n    \n    \"\"\"\n    def __init__(self, radius = 1.0):\n        self.radius = radius\n        \n    def area(self):\n        \"\"\" Compute area of circle\"\"\"\n        return pi*(self.radius**2)\n\nWe can create multiple circles using this template. Note the use of the keyword self to refer to attributes of the instance.\n\nc1 = Circle(3.2)\n\n\nc2 = Circle(4.0)\nc2.area()\n\n50.26548245743669\n\n\n\nprint(f\"\"\"\nThe radius of c1 is {c1.radius} and the radius of c2 is {c2.radius}.\nThe area of c1 is {c1.area():.3f} and the area of c2 is {c2.area():.3f}.\n\"\"\")\n\n\nThe radius of c1 is 3.2 and the radius of c2 is 4.0.\nThe area of c1 is 32.170 and the area of c2 is 50.265.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#introduction-to-agent-based-models",
    "href": "07-simulation.html#introduction-to-agent-based-models",
    "title": "7  Simulation",
    "section": "7.4 Introduction to Agent Based Models",
    "text": "7.4 Introduction to Agent Based Models\nAgent-based models (ABM) consist of individual elements (agents) interacting with one another. One of the most common uses of an agent-based model is to understand how the behaviour of the system, as a whole, emerges from simple rules for the agents’ and their interactions. A very famous example is from the simulation software used to create battle scenes in The Lord of The Rings movies from the early 2000s. The following video was from an article on the MASSIVE software.\n\n#IFrame(src='https://house-fastly-signed-us-east-1-prod.brightcovecdn.com/media/v1/pmp4/static/clear/6415844878001/e4f4a5c9-0393-4fe5-9986-d5c5b237070a/c276cef7-9eba-4498-a802-9f7867c41334/main.mp4?fastly_token=NjZlYjgxYzlfYWYzYzhmNDQ0ODhmM2Y4NmE1ODIzNzU1ZmE3ODhiNmM4NGZiNjAxNTc2YzZhYmI1NWNlMzYxNzliOGNjY2QxZF9odHRwczovL2hvdXNlLWZhc3RseS1zaWduZWQtdXMtZWFzdC0xLXByb2QuYnJpZ2h0Y292ZWNkbi5jb20vbWVkaWEvdjEvcG1wNC9zdGF0aWMvY2xlYXIvNjQxNTg0NDg3ODAwMS9lNGY0YTVjOS0wMzkzLTRmZTUtOTk4Ni1kNWM1YjIzNzA3MGEvYzI3NmNlZjctOWViYS00NDk4LWE4MDItOWY3ODY3YzQxMzM0L21haW4ubXA0', \n#       width=960, height=480)\nVideo(\"../data/lotr.mp4\", width=960, height=480)\n\nToday, ABM are used in scenarios where the overall behaviour of a system is to complicated to predict by modeling the stages of interaction. Examples of these scenarios are:\n\nCOVID-19 modeling,\nModeling of driver behaviour on roads for the purpose of understanding traffic,\nModeling of swarms, e.g. locusts, fish, etc, and\nModeling of effects of networks in marketing and in organisations.\n\nThe essential characteristics of an ABM are the following:\n\nAgents at various scales,\nAgents make decisions, based on defined behaviours\nAgents interact with one another in time and possibly space.\n\nRemember that our role as analysts is to imbue the agents with the appropriate traits (behaviours), interactions and probabilities. Otherwise, our model is not going to yield useful results.\n\nIntroduction to Mesa\nMesa is a Python package for running simulations using Agent-Based Models (ABM). Before we proceed, here are a few overarching concepts about this framework:\n\nAgents are defined using classes. Within the class definition for an agent, we need to define:\n\nhow the agent is initialised,\nwhat the agent does at each time-step.\n\nThe model is defined as a class as well. The model constitutes agents (defined through the class(es) above) and a scheduler that determines the sequence in which agents act, and the space in which they act.\n\nMesa also comes equipped with data collection tools to extract information from agents and/or the model at each step of the scheduler. Both the agent and the model classes should define a step() method. In ABM, a step is the smallest unit of time. It is also sometimes referred to as a “tick”.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#boltzmann-model-overview",
    "href": "07-simulation.html#boltzmann-model-overview",
    "title": "7  Simulation",
    "section": "7.5 Boltzmann Model Overview",
    "text": "7.5 Boltzmann Model Overview\nThis is a simple model from social sciences, to demonstrate the use of mesa for agent-based modeling. In this agent-based economy, the assumptions are:\n\nThere are a certain number of agents.\nAll agents begin with 1 unit of money.\nAt every step of the model, an agent gives 1 unit of money (if they have it), to another, randomly chosen, agent.\n\nThe reference for this paper is Dragulescu and Yakovenko (2002). Before we begin the model, think about what you expect to see after some time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#agent-and-model-classes-v1",
    "href": "07-simulation.html#agent-and-model-classes-v1",
    "title": "7  Simulation",
    "section": "7.6 Agent and Model Classes (v1)",
    "text": "7.6 Agent and Model Classes (v1)\nThe code below contains our initial definition for the agent class. The initialisation simply consists of setting the wealth for the agent. The step method defines that the agent will pick an agent (possibly itself) and transfer 1 unit of money to that agent.\n\nclass MoneyAgent(mesa.Agent):\n    \"\"\"An agent with fixed initial wealth.\"\"\"\n\n    def __init__(self, model):\n        # Pass the parameters to the parent class.\n        super().__init__(model)\n\n        # Create the agent's variable and set the initial values.\n        self.wealth = 1\n\n    def step(self):\n        # Verify agent has some wealth\n        if self.wealth &gt; 0:\n            other_agent = self.random.choice(self.model.agents)\n            other_agent.wealth += 1\n            self.wealth -= 1\n\nThe following flowchart depicts the flow of events for each agent. It is useful to sketch such a flowchart when implementing agent-based models, since it allows for easier debugging and information sharing with colleagues.\n\n\n\nAgent trajectory\n\n\nThe next class defines the model itself. The initialisation of a model typically consists of populating it with various agents. However, it is also important to define the scheduler. In this case, the scheduler is based on RandomActivation. This means that at every step, the agents are shuffled, and then each of their step methods are executed in that order. The order changes at every time step. It is necessary to avoid running all agents’ steps at the same time to avoid clashes.\n\nclass MoneyModel(mesa.Model):\n    \"\"\"A model with some number of agents.\"\"\"\n\n    def __init__(self, N):\n        super().__init__()\n        self.num_agents = N\n        # Create scheduler and assign it to the model\n        # self.schedule = mesa.time.RandomActivation(self)\n        # self.agents.shuffle_do(\"step\")\n\n        # Create agents\n        for i in range(self.num_agents):\n            a = MoneyAgent(self)\n            # Add the agent to the scheduler\n            #self.schedule.add(a)\n\n    def step(self):\n        \"\"\"Advance the model by one step.\"\"\"\n\n        # The model's step will go here for now this will call the step method of each agent and print the agent's unique_id\n        # self.schedule.step()\n        self.agents.shuffle_do(\"step\")\n\nThe next cell creates a model with 10 agents, and runs the model for 100 steps. At the end of this run, we compute the proportion of customers with wealth 0. This is a simple initial metric that we define on the model. Once again, just as with the parameters of the model, the metrics we extract determine how well we can use the model.\nRemember that, to begin with, all agents had equal wealth. If this proportion is high, it indicates a severe imbalance in wealth distribution after 100 time steps.\n\nall_wealth = []\nmodel = MoneyModel(10)\n\n# allows replicability of results; useful for debugging\nmodel.reset_randomizer(seed=5003) \n\nfor i in range(100):\n    model.step()\n\n# Extract the results\n# for agent in model.schedule.agents:\nfor agent in model.agents:\n    all_wealth.append(agent.wealth)\n\n\nprop = (np.array(all_wealth) == 0).mean()\nprint(f\"The proportion of agents with zero wealth is {prop:.3f}.\")\n\nThe proportion of agents with zero wealth is 0.300.\n\n\n\nData Collection\nHowever, remember that when we perform simulations, we need to repeat the run multiple times, take the average, and then form a confidence interval. To prepare the mesa model to perform data collection, we write a function that, given a model, will compute the proportion of zero-wealth agents.\n\ndef compute_zero_prop(mesa_model):\n    all_wealth = []\n    # Extract the results\n    for agent in mesa_model.agents:\n        all_wealth.append(agent.wealth)\n    prop = (np.array(all_wealth) == 0).mean()\n    return prop\n\ncompute_zero_prop(model)\n\n0.3\n\n\nNext, we update the model class to define a data collector that calls the compute_zero_prop() function that we just defined. With Mesa, the data collector can be based on the model, or it could be a function that operates on individual agents.\n\nclass MoneyModel(mesa.Model):\n    \"\"\"A model with some number of agents.\"\"\"\n\n    def __init__(self, N):\n        super().__init__()\n        self.num_agents = N\n        # Create scheduler and assign it to the model\n        # self.schedule = mesa.time.RandomActivation(self)\n\n        # Create agents\n        for i in range(self.num_agents):\n            a = MoneyAgent(self)\n            # Add the agent to the scheduler\n            # self.schedule.add(a)\n\n        # initialise the data collector, telling it to use the compute_zero_prop() function\n        # on the model.\n        self.datacollector = mesa.DataCollector(\n            model_reporters={\"zero_prop\": compute_zero_prop}\n        )\n\n    def step(self):\n        # self.schedule.step()\n        self.agents.shuffle_do(\"step\")\n        # Collect data at every step\n        self.datacollector.collect(self)\n\nOnce again, we reset the seed and re-run the model. We can then retrieve the zero-wealth proprortion at every step of the model.\n\nmodel = MoneyModel(10)\nmodel.reset_randomizer(seed=5003)\n\nfor i in range(100):\n    model.step()\n\n\nmodel.datacollector.get_model_vars_dataframe()\n\n\n\n\n\n\n\n\nzero_prop\n\n\n\n\n0\n0.4\n\n\n1\n0.5\n\n\n2\n0.6\n\n\n3\n0.6\n\n\n4\n0.5\n\n\n...\n...\n\n\n95\n0.1\n\n\n96\n0.5\n\n\n97\n0.5\n\n\n98\n0.3\n\n\n99\n0.3\n\n\n\n\n100 rows × 1 columns\n\n\n\nTake note that this is considered a single simulation. Any simulation model requires time to reach an equilibrium phase. In this case, we are interested in measuring the average proportion of zeros after 100 time steps. To do so, we need to run several simulations, compute the zero-wealth proportion at each step and take the average.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#multiple-iterations",
    "href": "07-simulation.html#multiple-iterations",
    "title": "7  Simulation",
    "section": "7.7 Multiple Iterations",
    "text": "7.7 Multiple Iterations\nThe mesa package has features to help us to run multiple simulations and then to collate the results. The primary method is .batch_run(). Now we shall run the model for 50 iterations. Remember that each iteration consists of a fresh set of 100 time steps.\n\nparams = {\"N\": 10}\n#params = {\"N\": [10, 50, 100]}\n\nresults = mesa.batch_run(\n    MoneyModel,\n    parameters=params,\n    iterations=50,\n    max_steps=100,\n    number_processes=1,\n    data_collection_period=-1,\n    display_progress=True,\n)\n\n\n\n\n\nresults_df = pd.DataFrame(results)\n\nresults_df.zero_prop.hist(grid=False, bins=np.arange(0.10, 1.00, 0.05));\n# results_df.zero_prop.groupby(results_df.N).describe()\n\n\n\n\n\n\n\n\n\nstats.ttest_1samp(results_df.zero_prop, 0.0).confidence_interval()\n#results_df.zero_prop.groupby(results_df.N).apply(lambda x: \n#                                                 stats.ttest_1samp(x, 0.0).confidence_interval())\n\nConfidenceInterval(low=0.3931118578144421, high=0.4588881421855579)\n\n\nWe can see that the proportion of agents with zero income is approximately 0.42. This is considerably higher than the 0.0 that the population began with.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#agent-and-model-classes-v2",
    "href": "07-simulation.html#agent-and-model-classes-v2",
    "title": "7  Simulation",
    "section": "7.8 Agent and Model Classes (v2)",
    "text": "7.8 Agent and Model Classes (v2)\n\nAssessment of Income Inequality\nThe Gini coefficient is a common measure of the income inequality of a population. The Gini coefficient can be defined using a Lorenz curve for a population:\n\n\n\nLorenz curve\n\n\nThe x- and y-axes both run from 0 to 100%. The x-axis depicts the cumulative proportion of a population (increasing from left to right), ranked by income. The y-axis depicts the cumulative proportion of total income earned by the population. If all members earned an equal amount, the Lorenz curve would be a straight line, from bottom left to top right. Gini coefficient measures deviation from this equality as:\n\\[\n\\frac{A}{A+B}\n\\]\nThe Gini coefficient is a measure between 0 and 1. A value of 0 indicates income equality, while a value of 1 indicates that only 1 member of the population earns all the income (extreme inequality). Here is an interactive visualisation from Our World in Data on Gini coefficient values around the world, across time.\n\nfrom IPython.display import IFrame\n\nIFrame(src='https://ourworldindata.org/grapher/economic-inequality-gini-index?tab=map', width=1024, height=600)\n\n\n        \n        \n\n\nThe wikipedia page on Gini coefficient contains a formula for computing Gini coefficient, given the income of all agents. After implementing it, we include it in the model class, along with our earlier metric.\n\ndef compute_gini(model):\n    agent_wealths = [agent.wealth for agent in model.agents]\n    x = sorted(agent_wealths)\n    N = model.num_agents\n    B = sum(xi * (N - i) for i, xi in enumerate(x)) / (N * sum(x))\n    return 1 + (1 / N) - 2 * B\n\nclass MoneyModel(mesa.Model):\n    \"\"\"A model with some number of agents.\"\"\"\n\n    def __init__(self, N):\n        super().__init__()\n        self.num_agents = N\n        # Create scheduler and assign it to the model\n        # self.schedule = mesa.time.RandomActivation(self)\n\n        # Create agents\n        for i in range(self.num_agents):\n            a = MoneyAgent(self)\n            # Add the agent to the scheduler\n            # self.schedule.add(a)\n\n        # initialise the data collector, telling it to use the gini() and compute_zero_prop() function\n        # on the model.\n        self.datacollector = mesa.DataCollector(\n            model_reporters={\"Gini\": compute_gini, \"zero_prop\": compute_zero_prop}\n        )\n\n    def step(self):\n        # self.schedule.step()\n        self.agents.shuffle_do(\"step\")\n        # Collect data at every step\n        self.datacollector.collect(self)\n\n\n\nData Collection\nSuppose we are interested to study how the Gini coefficient of our little population evolves over time. We shall use the batch_run to run 50 iterations, and then create a summary visualisation of the evolution of Gini coefficient for each iteration.\n\nparams = {\"N\": 10}\n\nresults = mesa.batch_run(\n    MoneyModel,\n    parameters=params,\n    iterations=50,\n    max_steps=100,\n    number_processes=1,\n    data_collection_period=1,\n    display_progress=True,\n)\n\n\n\n\nWe would like to inspect how the inequality varies over the time steps. Hence, at each time step, we obtain the lower and upper quantiles along with the median and plot them.\n\nresults_df = pd.DataFrame(results)\ngrp_by_step = results_df.Gini.groupby(results_df.Step).describe()\n#grp_by_step.head()\n\ngrp_by_step[['25%', '50%', '75%']].plot(style=['--', '-', '--'])\nplt.ylim([0, 1]);\n\n\n\n\n\n\n\n\nWe can see that the Gini index quickly achieves one of the highest inequality values that we observe in the world, and remains there.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#agent-and-model-classes-v3",
    "href": "07-simulation.html#agent-and-model-classes-v3",
    "title": "7  Simulation",
    "section": "7.9 Agent and Model Classes (v3)",
    "text": "7.9 Agent and Model Classes (v3)\n\nAdding a spatial component\nNext we introduce another critical component of agent-based modeling - the spatial component. We retain the Gini coefficient computation, but we are going to modify the model behaviour such that agents only exchange money if the other agent is nearby. The MultiGrid defines a rectangular grid, indexed by [x, y], where [0, 0] is assumed to be at bottom-left and [width - 1, height - 1] is the top-right. If a grid is toroidal, the top and bottom, and left and right, edges wrap to each other.\nIn the loop to create agents, the agents are added to a random x- and y- cell of the grid.\n\nclass MoneyModel(mesa.Model):\n    \"\"\"A model with some number of agents.\"\"\"\n\n    def __init__(self, N, width, height):\n        super().__init__()\n        self.num_agents = N\n        self.grid = mesa.space.MultiGrid(width, height, True)\n        # self.schedule = mesa.time.RandomActivation(self)\n\n        # Create agents\n        for i in range(self.num_agents):\n            a = MoneyAgent(self)\n            # self.schedule.add(a)\n            # Add the agent to a random grid cell\n            x = self.random.randrange(self.grid.width)\n            y = self.random.randrange(self.grid.height)\n            self.grid.place_agent(a, (x, y))\n            \n        # Data collectors for the model\n        self.datacollector = mesa.DataCollector(\n            model_reporters={\"Gini\": compute_gini}, \n            agent_reporters={\"Wealth\": \"wealth\"}\n        )\n\n    def step(self):\n        self.datacollector.collect(self)\n        self.agents.shuffle_do(\"step\")\n        # self.schedule.step()\n\nNow we turn to the agent class. The new version defines methods to move() and to give_money(). Within the move method, we define that the agent could move to a neighbouring cell. With mesa, it is possible to define a couple of types of neighbourhoods: Moore and Von Neumann. In this case, we use the Moore neighbourghood.\n\n\n\n\n\nIn the give_money method, the logic proceeds as follows:\n\nRetrieve the agents in neighbouring cells.\nRemove the agent itself from the list of neighbours, so that an agent will not give money to itself.\nChoose one of the neighbouring agents at random and give money to it.\n\n\nclass MoneyAgent(mesa.Agent):\n    \"\"\"An agent with fixed initial wealth.\"\"\"\n\n    def __init__(self, model):\n        super().__init__(model)\n        self.wealth = 1\n\n    def move(self):\n        possible_steps = self.model.grid.get_neighborhood(\n            self.pos, moore=True, include_center=False\n        )\n        new_position = self.random.choice(possible_steps)\n        self.model.grid.move_agent(self, new_position)\n\n    def give_money(self):\n        cellmates = self.model.grid.get_cell_list_contents([self.pos])\n        cellmates.pop(\n            cellmates.index(self)\n        )  # Ensure agent is not giving money to itself\n        if len(cellmates) &gt;= 1:\n            other = self.random.choice(cellmates)\n            other.wealth += 1\n            self.wealth -= 1\n                \n    # First, the agent moves. Then, it might give money away to another agent in the neighbouring cell.\n    def step(self):\n        self.move()\n        if self.wealth &gt; 0:\n            self.give_money()\n\n\n\nData Collection\nThe next code cell runs a single iteration with 100 agents in a 10x10 grid.\n\nmodel = MoneyModel(100, 10, 10)\nfor i in range(100):\n    model.step()\n\ngini = model.datacollector.get_model_vars_dataframe()\n# Plot the Gini coefficient over time\ng = sns.lineplot(data=gini)\ng.set(title=\"Gini Coefficient over Time\", ylabel=\"Gini Coefficient\");\n\n\n\n\n\n\n\n\n\nparams = {\"width\": 10, \"height\": 10, \"N\": range(5, 20, 5)}\n\nresults = mesa.batch_run(\n    MoneyModel,\n    parameters=params,\n    iterations=50,\n    max_steps=100,\n    number_processes=1,\n    data_collection_period=1,\n    display_progress=True,\n)\n\n\n\n\nFor a visualisation of the results, we shall compute the mean Gini coefficient at each time step, grouped by N.\n\nresults_df = pd.DataFrame(results)\ngrp_by_step = results_df.Gini.groupby([results_df.Step, results_df.N]).describe()\ngrp_by_step.reset_index(inplace=True)\n\n#grp_by_step.head()\n\n\ngrouped = grp_by_step.groupby('N')\nplt.figure(figsize=(10, 6))\n\nfor group in grouped:\n    plt.plot(group[1]['Step'], group[1]['mean'], label=group[0], alpha=0.5)\n\nplt.xlabel('Step')\nplt.ylabel('Gini')\nplt.title('Gini vs Step by N (number of agents)');\nplt.legend();\n\n\n\n\n\n\n\n\nAs we can see, the value at which the Gini coefficient converges to differs for varying N. For smaller N, the final Gini value seems to be lower. For all the settings, the convergence does appear to happen at approximately the same time step.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#visualisations",
    "href": "07-simulation.html#visualisations",
    "title": "7  Simulation",
    "section": "7.10 Visualisations",
    "text": "7.10 Visualisations\nAs with most ABM software, it is possible to visualise the steps. This is not to be used for conveying results to management; it is more for debugging, and for study for the purpose of understanding the behaviour and for generating hypothesis.\n\ndef agent_portrayal(agent):\n    #return {\n    #    \"color\": \"tab:blue\",\n    #    \"size\": 50,\n    #}\n    size = 10\n    color = \"tab:red\"\n    if agent.wealth &gt; 0:\n        size = 50\n        color = \"tab:blue\"\n    return {\"size\": size, \"color\": color}\n\nmodel_params = {\n    \"N\": {\n        \"type\": \"SliderInt\",\n        \"value\": 50,\n        \"label\": \"Number of agents:\",\n        \"min\": 10,\n        \"max\": 100,\n        \"step\": 1,\n    },\n    \"width\": 10,\n    \"height\": 10,\n}\n\n\n#from mesa.experimental import JupyterViz\n\n#page = JupyterViz(\n#    MoneyModel,\n#    model_params,\n#    measures=[\"Gini\"],\n#    name=\"Money Model\",\n#    agent_portrayal=agent_portrayal,\n#)\n# This is required to render the visualization in the Jupyter notebook\n#page\n\nRemember that simulations are a risk-free approach to trying out different scenarios. They are especially useful when it is difficult to guage what interactions the output depends on. In this case, here are some modifications you can try to understand the Boltzmann Wealth Distribution model better.\n\nVary the number of agents, to understand the Gini coefficient after 100 steps.\nWhen there are N agents, what is the proportion of agents that own 80% of the total wealth after a certain number of steps?\nHow can we measure social mobility in this population?\nWhat if we change the starting income distribution?\nWhat if we want to know when Gini coefficient first crosses a threshold?\nWhat if we want to use Palma Ratio instead?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#simpler-simulation-models",
    "href": "07-simulation.html#simpler-simulation-models",
    "title": "7  Simulation",
    "section": "7.11 Simpler Simulation Models",
    "text": "7.11 Simpler Simulation Models\nIn this section, we provide two examples of “simpler” simulation scenarios. These are not agent-based models, but they use probability distributions to model language, and then to represent the scenario where we intend to perform a 2-sample \\(t\\)-test.\n\nN-gram models\nHere is an example of how we can simulate from distributions to generate random (nonsensical) sentences that are “similar” to actual sentences from a text.\n\n#nltk.download('genesis')\n\nkjv = nltk.corpus.genesis.words('english-kjv.txt')\nbg = nltk.bigrams(kjv)\ncfd = nltk.ConditionalFreqDist(bg)\n\nThe object cfd contains information on the distribution of words that come after. For instance, these are the words that come after “she”:\n\ncfd['she']\n\nFreqDist({'said': 22, 'was': 14, 'bare': 14, 'called': 13, 'conceived': 11, 'had': 11, 'saw': 6, 'shall': 5, 'is': 5, 'went': 5, ...})\n\n\nThe object kjv contains all the words and punctuations from the King James Bible. Here are the first 20 words:\n\n' '.join(kjv[:20])\n\n'In the beginning God created the heaven and the earth . And the earth was without form , and void'\n\n\nNext, we extract all bigrams and create a dictionary of frequency distributions. Each frequency distribution tabulates the occurences of the next word.\nSuppose we are interested in what words/tokens come after the word ‘And’ in the bible:\n\ncfd['And'].keys()\n\ndict_keys(['the', 'God', 'let', 'to', 'on', 'every', 'out', 'a', 'Adam', 'they', 'he', 'when', 'I', 'unto', 'she', 'Abel', 'in', 'Cain', 'now', 'Lamech', 'Adah', 'his', 'Zillah', 'all', 'Seth', 'Enos', 'Cainan', 'Mahalaleel', 'Jared', 'Enoch', 'Methuselah', 'Noah', 'it', 'this', ',', 'of', 'take', 'surely', 'you', 'with', 'Ham', 'Shem', 'Cush', 'Resen', 'Mizraim', 'Pathrusim', 'Canaan', 'Arphaxad', 'Joktan', 'Hadoram', 'Obal', 'Ophir', 'their', 'Salah', 'Eber', 'Peleg', 'Reu', 'Serug', 'Nahor', 'Terah', 'Haran', 'Abram', 'there', 'Pharaoh', 'Lot', 'Melchizedek', 'blessed', 'also', 'thou', 'Sarai', 'Hagar', 'ye', 'Abraham', 'as', 'Ishmael', 'said', 'Sarah', 'while', 'Abimelech', 'yet', 'Isaac', 'Chesed', 'Bethuel', 'Ephron', 'after', 'if', 'Rebekah', 'my', 'before', 'her', 'Jokshan', 'these', 'Mishma', 'Jacob', 'Esau', 'make', 'by', 'tarry', 'give', 'that', 'thy', 'thither', 'Laban', 'Leah', 'Bilhah', 'Rachel', 'Zilpah', 'Reuben', 'afterwards', 'your', 'hast', 'Mizpah', 'early', 'so', 'say', 'Dinah', 'Shechem', 'Hamor', 'Israel', 'Bashemath', 'Aholibamah', 'Timna', 'Dishon', 'Bela', 'Jobab', 'Husham', 'Hadad', 'Samlah', 'Saul', 'Baalhanan', 'Joseph', 'Judah', 'Er', 'Onan', 'Tamar', 'afterward', 'we', 'for', 'bring', 'other', 'put', 'our', 'told', 'one', 'forty'])\n\n\nThus ‘the’ is the most likely word after ‘And’. The second most likely pairing would be ‘And the’, and so on. We can convert this into a pmf, just by normalising to ensure that it sums to 1. Then we are ready to generate a pairing once we know that the first word is ‘And’. With the second word, we turn to its frequency distribution and simulate the third word, and so on.\n\nrng = default_rng(1361)\n\ndef generate_model(cfdist, word, num=15):\n    for i in range(num):\n        print(word, end=' ')\n        \n        choices = np.array(list(cfdist[word].keys()))\n        pp = np.array(list(cfdist[word].values()))\n        pp = pp / np.sum(pp)\n        word = rng.choice(choices, size=1, p = pp)[0]\n\n\ngenerate_model(cfd, 'God', 10)\n\nGod amongst the daughter of his hands , to Jacob \n\n\n\n\nPower Analysis\nRecall that this is the general form of a 2-sample \\(t\\)-test: \\[\\begin{eqnarray}\nH_0 :& \\mu = \\mu_0 \\\\\nH_1 :& \\mu \\neq \\mu_0\n\\end{eqnarray}\\]\nInstead of using the \\(p\\)-value alone to assess the strength of evidence against the null hypothesis, an alternative method is to specify a significance level and then compare the \\(p\\)-value to it.\nThis returns the possible four outcomes of a test: * \\(H_0\\) was true, but we rejected it (Type I error). * \\(H_0\\) was true, and we did not reject it. * \\(H_0\\) was false, and we rejected it. * \\(H_0\\) was false, but we did not reject it (Type II error).\nWe want the Type I and II errors to be small. 1 minus the Type II error is known as the power of a test. Typically, we have to specify a particular value of the parameter in order to compute the power of a test. We cannot simply leave it as \\(\\mu \\neq \\mu_0\\). These ideas can be used to compute a sample size for an experiment that we wish to conduct. Let’s think about this for a minute.\nSuppose we wish to perform some simple A/B testing: we have two groups, and we wish to be able to detect a difference. What sample size do I need?\nThe question is actually a little more complex than that. Let’s think about what we need to consider: * How does the true difference affect my sample size? * How does the variability within each group affect my sample size? * How does the significance level affect the sample size I need? Recall that the significance level determines my Type I error.\nSuppose now, that we fix the significance level to be 5%, with power of at least 0.9 and we are interested in detecting a difference between the means of the value of 1, when the standard deviation of observed values is 1.2. What sample size do I need?\n\ndef generate_one_sample(alpha, delta_m, sd1, n):\n    X = np.random.randn(n)*sd1\n    Y = np.random.randn(n)*sd1 + delta_m\n    \n    ts2 = stats.ttest_ind(X, Y)\n    if ts2.pvalue &lt; alpha:\n        return 1 # 1 means reject H0\n    else:\n        return 0\n\n\nn_vals = np.arange(5, 50, step=4)\n\npower_est = []\nfor n_ in n_vals:\n    x = [generate_one_sample(0.05, 1, 1.2, n_) for ii in np.arange(0, 2000)]\n    power_est.append(np.mean(x))\n    print(\"Done with sample size \" + str(n_))\n\nDone with sample size 5\nDone with sample size 9\nDone with sample size 13\nDone with sample size 17\nDone with sample size 21\nDone with sample size 25\nDone with sample size 29\nDone with sample size 33\nDone with sample size 37\nDone with sample size 41\nDone with sample size 45\nDone with sample size 49\n\n\n\nax = plt.plot(n_vals, power_est, 'go-')\nplt.hlines(0.9, n_vals[0], n_vals[-1], colors='b', linestyles='dotted')\nplt.title('Power Estimates');",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#summary-simulation-based-modeling",
    "href": "07-simulation.html#summary-simulation-based-modeling",
    "title": "7  Simulation",
    "section": "7.12 Summary: Simulation-Based Modeling",
    "text": "7.12 Summary: Simulation-Based Modeling\nIn simulation-based modeling, we have several things to consider.\n\nWhat input distributions should I use?\nHow complicated does my simulation need to be?\nHow do I know it is giving the right results?\nHow many simulations should I run?\n\nHere are some general guidelines to begin with your simulation model:\n\nStart simple.\nUse your own data to decide what distributions should be used.\nTry with different distributions to see how sensitive your results are to those choices.\nAdd more and more realistic layers to our simulation as you proceed.\nRemember that simulation is used in estimating mean values. You will be able to estimate standard deviations around your estimate too.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "07-simulation.html#references",
    "href": "07-simulation.html#references",
    "title": "7  Simulation",
    "section": "7.13 References",
    "text": "7.13 References\n\nMesa Links\n\nMesa documentation: The Boltzmann tutorial in our lecture was modified from this page.\nMesa examples on github\n\n\n\nOther ABM Software\n\nNetlogo: Netlogo is an open-source software for ABM. There are a host of examples that you can take a look. However NetLogo uses it’s own programming language.\n\nhttps://ccl.northwestern.edu/netlogo/models/index.cgi : Several of the examples can be run directly on the web. The mesa examples repository has several mesa versions of these NetLogo models.\n\nAnyLogic: AnyLogic is a commercial software from a Russian company that combines ABM with DES. It is very powerful and allows for the creation of impressive animations.\n\n\n\nReference Papers and Websites\n\nStatistical Mechanics of Money, Income, and Wealth: A Short Survey: This paper contains more information about more complex versions of the Boltzmann model.\nExplanation of Boids model\nHotelling’s Law\nSegregation in polygons\n\n\n\nOther Simulation Software\n\nSimpy documentation Simpy is a Python package for Discrete Event Simulations (DES). It is appropriate when there are arrivals to a set of queues.\nArena ARENA is a commercial DES software. There is a free license for academic use.\n\n\n\nOther Links\n\nPython documentation on classes\n\n\n\n\n\nDragulescu, Adrian A, and Victor M Yakovenko. 2002. “Statistical Mechanics of Money, Income, and Wealth: A Short Survey.” arXiv Preprint Cond-Mat/0211175.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "08-supervised.html",
    "href": "08-supervised.html",
    "title": "8  Supervised Learning",
    "section": "",
    "text": "8.1 Introduction\nIn supervised learning, our goal is to develop a model that can predict a quantity of interest from a set of features. In this process,\nHere are some examples:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#introduction",
    "href": "08-supervised.html#introduction",
    "title": "8  Supervised Learning",
    "section": "",
    "text": "Algorithms learn from a training set of labelled examples.\nThis training set is meant to be representative of the set of all possible inputs.\nExample algorithms include logistic regression, support vector machines, decision trees and random forests. Regression models can also be used for supervised learning.\n\n\n\nWe wish to predict if a student will graduate from university or not, based on his/her ‘A’ level results.\nWe wish to predict tomorrow’s stock price based on today’s price.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#classification-versus-regression",
    "href": "08-supervised.html#classification-versus-regression",
    "title": "8  Supervised Learning",
    "section": "8.2 Classification versus Regression",
    "text": "8.2 Classification versus Regression\nIf the answer to the question (supervised learning problem) we are facing is either YES or NO, then what we have is a classification problem. Here are some examples:\n\nGiven the results of a clinical test, does this patient suffer from diabetes?\nGiven an MRI, is there a tumor?\n\nOn the other hand, if we are trying to predict a real-valued quantity, then we are faced with a regression problem.\n\nGiven the details about an apartment, what will the rental be?\nGiven historical transactions of a customer, how much is he likely to spend on his next purchase?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#supervised-learning-workflow",
    "href": "08-supervised.html#supervised-learning-workflow",
    "title": "8  Supervised Learning",
    "section": "8.3 Supervised Learning Workflow",
    "text": "8.3 Supervised Learning Workflow\nThe overall workflow in supervised learning is as follows:\n\n\n\nFigure from sklearn documentation\n\n\nHere are the specific details:\n\nSplit up a dataset into training and test datasets, typically along a 80/20 or 75/25 split. Do not touch the test data again until the end.\nPreprocess/clean the training data and store the parameters for later use on the test data.\n\nExample preprocessings are scaling, one-hot encoding, PC decomposition, etc.\n\nDecide on what models you wish to try. Each model has parameters to be fit (from the data), and hyperparameters to be chosen by you.\n\nExample models are k-nearest neighbours (KNN) and random forests.\n\nA hyperparameter for KNN is the number of neighbours to use.\nA hyperparameter for random forests is the number of trees.\n\nHyperparameters usually control the complexity of a model. If a model is too complex, it will over-fit to the training data but fare poorly on the test data.\n\nUse cross-validation or a set-aside validation set to decide on the hyperparameters for your chosen estimator. To fit the parameters for a particular hyperparameter configuration, we typically minimise a loss function or error metric.\nOnce you are satisfied with your choice(s) or model, evaluate the selected model on the test set to obtain an estimate of your generalisation error.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#scikit-learn",
    "href": "08-supervised.html#scikit-learn",
    "title": "8  Supervised Learning",
    "section": "8.4 Scikit-learn",
    "text": "8.4 Scikit-learn\nScikit-learn is a library in Python which has several useful functions used in machine learning. The library has many algorithms for classification, regression, clustering and other machine learning methods. It uses other libraries like NumPy and matplotlib which are also used in this course. The website for scikit-learn is an excellent source of examples and tips on using the functions within this package (see the references).\nAll objects in scikit-learn have common access points. The three main interfaces are:\n\nEstimator interface - fit() method.\n\nThis function allows us to build and fit models.\nAny object that can estimate some parameters based on a dataset is an estimator.\nEstimation is performed by the fit() method. This method takes in two datasets as arguments (the input data, and the corresponding output/labels).\n\nPredictor interface - predict() method.\n\nThis function allows us to make predictions.\nEstimators capable of making predictions when given a dataset are called predictors.\nA predictor has a predict() method. It takes in a dataset of new instances and returns a dataset of corresponding predictions.\n\nTransformer interface - transform() method.\n\nThis function is for converting data.\nEstimators which can also transform a dataset are called transformers.\nTransformations are carried out by the transform() method.\nThis method takes in the dataset to transform as a parameter and returns the transformed dataset.\nWe will not have too much time to spend on the transformer interface in this course.\n\n\n\nInput Data Structure\nFor supervised learning problems in scikit-learn, the input data has to be structured in NumPy-like arrays.\nThe feature matrix X, of shape \\(n\\) by \\(d\\) contains features: * \\(n\\) rows: the number of samples * \\(d\\) columns: the number of features or distinct traits used to describe each item in a quantitative manner\nEach row in the feature matrix is referred to as a sample, example or an instance.\n\\[\n\\text{feature matrix:} \\quad \\mathbf{X}_{n\\times d} =\n\\begin{bmatrix}\nx_{1,1} & x_{1,2} & \\ldots & x_{1,d}\\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nx_{n,1} & x_{n,2} & \\ldots & x_{n,d}\n\\end{bmatrix}\n\\]\nA label vector y stores the target values. This vector stores the true output value for each corresponding sample (row) in matrix X.\n\\[\n\\text{label vector:} \\quad \\mathbf{y} =\n\\begin{bmatrix}\ny_1 & y_2 & \\ldots & y_n\n\\end{bmatrix}^T\n\\]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom itables import show\n\nfrom lime import lime_tabular\n\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, r2_score\nfrom sklearn.model_selection import cross_val_score, cross_validate, ShuffleSplit, learning_curve, validation_curve, GridSearchCV, train_test_split\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import PartialDependenceDisplay, partial_dependence",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#measures-of-performance",
    "href": "08-supervised.html#measures-of-performance",
    "title": "8  Supervised Learning",
    "section": "8.5 Measures of Performance",
    "text": "8.5 Measures of Performance\n\nFor Classification\nBefore we head into creating classifiers which will help us predict heart failure, let’s understand what determines the usefulness of a classifier. For now, we focus on the case where the outcome is binary (only two possible values for \\(y_i\\)).\n\nAccuracy\nA basic measure of performance would be the accuracy of predictions.\n\\[\n\\text{accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n\\]\nWhen more detailed analysis is needed, partial performance metrics can be presented in a confusion matrix. A confusion matrix is a contingency table that arises from cross-classification of predictions and the actual outcomes.\n\n\n\n\n\n\n\n\n\nPositive prediction\nNegative prediction\n\n\nPositive truth\nTP\nFN\n\n\nNegative truth\nFP\nTN\n\n\n\nIn the confusion matrix, there are 4 possible cases: * True positives (TP) * Classifier predicts sample as positive and it really is so. * False positives (FP) * Classifier predicts sample as positive but in truth, it is negative. Incorrect prediction * True negatives (TN) * Classifier predicts sample as negative and it really is so. * False negatives (FN) * Classifier predicts sample as negative but in truth, it is positive. Incorrect prediction\n\n\nPrecision and Recall\nWith the confusion matrix, more performance metrics can be defined besides the accuracy of a classifier.\nThe recall of a classifier is the proportion of TP correctly identified:\n\\[\n\\text{recall}=  \\frac{\\text{TP}}{\\text{TP + FN}}\n\\]\nThe precision of a classifier is the proportion of predicted positives that are truly positive:\n\\[\n\\text{precision} =  \\frac{\\text{TP}}{\\text{TP + FP}}\n\\]\nDepending on the context of the problem, it may be more important to have better recall than precision. In the above, we have defined recall and precision for the positive category outcome. There are analogous definitions for the negative outcome.\nNote that recall is also sometimes referred to as the True Positive Rate (TPR), while \\((1 - \\text{precision})\\) is also referred to as the False Positive Rate (FPR).\n\n\nF1 score\nThe harmonic mean of two numbers \\(x_1\\) and \\(x_2\\) is \\[\n\\left( \\frac{1/x_1 + 1/x_2}{2} \\right)^{-1}\n\\]\nWe can combine precision and recall into one score using their harmonic mean: \\[\nF1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} +\n\\text{receall}}\n\\]\nRoughly, the F1 score is a summary of how good the classifier is in terms of both precision and recall. The F1 score is preferable to the simple arithmetic mean of precision and recall, because it ensures that both are high; the F1 score will be significantly lower than the mean if one of precision or recall is very low.\n\n\n\nFor Regression\n\nRoot Mean Squared Error\nFor regression problems, the typical measure of accuracy is RMSE. Let * \\(y_i\\) be the observed quantity (that we wish to predict), for \\(i=1,\\ldots, n\\), and * \\(\\hat{y}_i\\) be the predicted quantity for observation \\(i\\).\nThe RMSE is defined to be:\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\]\n\n\nMean Absolute Error\nUsing the RMSE amplifies the effect of outliers because of the square-term in the equation. Hence, in order to be resistant to outliers, one alternative is to the mean absolute error.\n\\[\nMAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#classification",
    "href": "08-supervised.html#classification",
    "title": "8  Supervised Learning",
    "section": "8.6 Classification",
    "text": "8.6 Classification\nIn this section, we demonstrate how we can use scikit-learn to perform supervised learning on a classification problem.\n\nExample 8.1 (Example: Heart Failure) \nThis dataset, from the UCI machine learning repository contains records on 299 patients who had heart failure. The data was collected during the follow-up period; each patient had 13 clinical features recorded. The primary variable of interest (\\(y\\)) was whether they died or not. Here are more details about each column:\n\nage: age of the patient (years)\nanaemia: decrease of red blood cells or hemoglobin (boolean)\ncreatinine phosphokinase (CPK): level of the CPK enzyme in the blood (mcg/L)\ndiabetes: if the patient has diabetes (boolean)\nejection_fraction: percentage of blood leaving the heart at each contraction (percentage)\nhigh_blood_pressure: if the patient has hypertension (boolean)\nplatelets: platelets in the blood (kiloplatelets/mL)\nsex: woman or man (binary)\nserum_creatinine: level of serum creatinine in the blood (mg/dL)\nserum_sodium: level of serum sodium in the blood (mEq/L)\nsmoking: if the patient smokes or not (boolean)\ntime: follow-up period (days)\nDEATH_EVENT: if the patient died during the follow-up period (boolean). This is the categorical outcome that we wish to predict.\n\n\nhf = pd.read_csv(\"data/heart+failure+clinical+records/\"+\n                 \"heart_failure_clinical_records_dataset.csv\")\nprint(hf.head())\n\n    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n0  75.0        0                       582         0                 20   \n1  55.0        0                      7861         0                 38   \n2  65.0        0                       146         0                 20   \n3  50.0        1                       111         0                 20   \n4  65.0        1                       160         1                 20   \n\n   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n0                    1  265000.00               1.9           130    1   \n1                    0  263358.03               1.1           136    1   \n2                    0  162000.00               1.3           129    1   \n3                    0  210000.00               1.9           137    1   \n4                    0  327000.00               2.7           116    0   \n\n   smoking  time  DEATH_EVENT  \n0        0     4            1  \n1        0     6            1  \n2        1     7            1  \n3        0     7            1  \n4        0     8            1  \n\n\n\n\nDecision Tree\nTo begin our journey into supervised learning, we shall fit a decision tree to this dataset. A decision tree consists of a hierarchal set of rules, that when followed, will return a prediction for an individual observation. Each is a (typically binary) split of one of the features in the observation. One of the main advantages of decision tree classifiers is that they are easy to interpret. However, some disadvantages are that: They tend to overfit to a dataset, and they have high variability. This latter point means that a small change in the training data could lead to vastly different predictions. Although in this example we focus on classification, a decision tree can also be used for regression.\nIn this first example, we shall only split the data into a test and training set. We shall fit the tree using the training set, and then apply the model to the test set.\n\ny = hf.DEATH_EVENT\nX = hf.iloc[:, 0:12]\n\nclf = tree.DecisionTreeClassifier(max_depth=4)\n\nThe max_depth of a decision is the maximum number of splits down each branch of a tree. If it is not specified, it is possible that the splits continue until the terminal nodes are homogeneous. This could result in overfitting of the tree to this particular dataset.\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25, \n                                                 random_state=41, stratify=y)\n\nThe train_test_split divides the data into a training and test set. When doing so, the function will attempt to ensure that the proportion of classes in both the training and test sets are roughly equal to the proportion in the overall dataset.\n\nprint(f\"The proportion of 1's in the overall data is {y.mean():.3f}.\")\nprint(f\"The proportion of 1's in the training data is {y_train.mean():.3f}.\")\nprint(f\"The proportion of 1's in the test data is {y_test.mean():.3f}.\")\n\nThe proportion of 1's in the overall data is 0.321.\nThe proportion of 1's in the training data is 0.321.\nThe proportion of 1's in the test data is 0.320.\n\n\n\nclf.fit(X_train, y_train,)\n\nDecisionTreeClassifier(max_depth=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \n4\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nNotice that when we call the .fit() method, there is no output object returned; it is just that the parameters in the clf object are updated.\n\nclf.predict_proba(X_train.sample(random_state=3005))\n\narray([[0.66666667, 0.33333333]])\n\n\n\nplt.figure(figsize =(18, 6))\ntree.plot_tree(clf,feature_names=X.columns, filled=True, max_depth=2);\n\n\n\n\n\n\n\n\nThe figure above visualises the rules in the first two layers (depth 2) of the tree. First off, notice that the tree is upside down, with the root on top; the terminal nodes (or boxes) at the bottom are referred to as the leaves. To understand the tree, consider using it to obtain a new prediction, with the following two features:\n\ntime=70, ejection_fraction = 40.5\n\nSince the value for the time variable is less than or equal to 73.5, we go down the left branch. Next, since time is more than 52.0, we go down the right branch. Finally, since ejection_fraction is more than 30, we would go down the right branch, and so on.\nThe information in the node summarises the splitting rule at that node. samples refers to the number of observations from the training set that would reach that node. The value vector indicates the count of each output class that has reached that node. For instance, for the blue node at the second level on the extreme left, out of the 44 observations that reached it, 4 were class 0 and 40 were class 1. The Gini impurity index is what is used to decide the split (not the same Gini as the Gini coefficient for income inequality!). For our binary classification problem, the formula for Gini impurity at each node reduces to:\n\\[\np_0(1 - p_0)\n\\]\nwhere \\(p_0\\) is the proportion of class 0 at that node. If a node consists all 0’s or all 1’s, then the impurity index would be 0. The fitting algorithm tries all features, and all split points for each feature, to obtain the feature and split point that yields the largest drop in impurity at that node.\nBefore proceeding with the calculation of error metrics, notice the non-linearity of the splits. Both the initial split and the subsequent split on the left are on time. This indicates a non-linear relationship with time, and is not a property of classical models such as out-of-the-box linear regression.\n\nExample 8.2 (Example: Heart Failure Classification Scores) \nPerformance of classification algorithms are usually presented in the form of a confusion matrix. First we display the results for the training set.\n\ny_pred_train = clf.predict(X_train)\nConfusionMatrixDisplay.from_predictions(y_train, y_pred_train, \n                                        labels=clf.classes_, cmap='bone');\nprint(f\"\"\"\n##: For training set:\n----\nThe precision (for cat. 1) is {precision_score(y_train, y_pred_train):.3f}\nThe recall (for cat. 1) is {recall_score(y_train, y_pred_train):.3f}\nThe accuracy (for cat. 1) is {accuracy_score(y_train, y_pred_train):.3f}\nThe f1-score (for cat. 1) is {f1_score(y_train, y_pred_train):.3f}\n\"\"\")\n\n\n##: For training set:\n----\nThe precision (for cat. 1) is 0.938\nThe recall (for cat. 1) is 0.833\nThe accuracy (for cat. 1) is 0.929\nThe f1-score (for cat. 1) is 0.882\n\n\n\n\n\n\n\n\n\n\nNext, we display the results for the test set.\n\ny_pred_test = clf.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_test, \n                                        labels=clf.classes_, cmap='bone');\nprint(f\"\"\"\n##: For test set:\n----\nThe accuracy (for cat. 1) is {accuracy_score(y_test, y_pred_test):.3f}\nThe precision (for cat. 1) is {precision_score(y_test, y_pred_test):.3f}\nThe recall (for cat. 1) is {recall_score(y_test, y_pred_test):.3f}\nThe f1-score (for cat. 1) is {f1_score(y_test, y_pred_test):.3f}\n\"\"\")\n\n\n##: For test set:\n----\nThe accuracy (for cat. 1) is 0.760\nThe precision (for cat. 1) is 0.636\nThe recall (for cat. 1) is 0.583\nThe f1-score (for cat. 1) is 0.609\n\n\n\n\n\n\n\n\n\n\n\nIt is normal to observe poorer performance on the test set than on the training set. However, such a large difference in the scores usually indicates that there has been overfitting by the model; it is too tuned to the data in the training set. In supervised learning, we aim for a classifier that performs almost as well on the test data as it does on the training data. That would indicate that the model, with this particular hyperparameter configuration, will perform well on new data that has yet to be seen.\n\n\nVariable importance\nAs we shall discuss in a later section of this topic, it is important to be able to identify which features are important to the model. This allows non-technical folk in your team (who could be end-user domain experts, or upper management) to have faith in your models. If the features that are identified to be important align with what the domain experts know or intuit, that provides more buy-in for your analyses and recommendations.\nWe shall demonstrate two types of feature importance:\n\nPermutation based feature importance. It works by permuting one of the features at a time, and measuring the drop in accuracy on the test set. The intuition is that, if a variable is highly important, randomising it will cause a sharp drop in predictive value.\nPartial dependence plots (PDP). A PDP can be generated for each feature in the model, or a pair of features. Each PDP is generated by:\n\nVary the feature over a range of values\nFor each feature value in that range, average the predictions over all values of other variables occuring in the training data.\n\n\nThere are certain caveats with using the above measures. One is that these measures are indicative of how important a feature is to a particular model, not to predictive value. Hence an unimportant feature for a poor model (in terms of accuracy) could in fact be an important feature for a good model! Second, in the permutation approach, the importance of a variable may not show up if certain features are highly correlated (dropping one of them may have no effect because the other feature is still present in the model).\n\nPermutation Based\n\nresult = permutation_importance(\n    clf, X_test, y_test, n_repeats=30, random_state=42,\n    # clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (test set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n\n\n\n\n\n\nIn the plot above, we can see that time, ejection_fraction and serum_sodium are the top 3 in terms of importance when assessing generalisability to test set.\n\n\nPartial Dependence Plots\n\n_, ax = plt.subplots(ncols=3, nrows=2, figsize=(12, 6), constrained_layout=True)\nfeatures_info = {\n    \"features\": [\"age\", \"creatinine_phosphokinase\", \"platelets\", \n                 \"serum_creatinine\", \"serum_sodium\", (\"age\", \"serum_creatinine\"),\n                ],\n    \"kind\": \"average\",\n}\ndisplay = PartialDependenceDisplay.from_estimator(\n    clf,\n    X_train,\n    **features_info,\n    ax=ax,\n    contour_kw = {'cmap': \"Reds\"}\n)\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:717: FutureWarning:\n\nThe column 2 contains integer data. Partial dependence plots are not supported for integer data: this can lead to implicit rounding with NumPy arrays or even errors with newer pandas versions. Please convert numerical featuresto floating point dtypes ahead of time to avoid problems. This will raise ValueError in scikit-learn 1.9.\n\n/home/viknesh/NUS/coursesTaught/ind5003-book/env/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:717: FutureWarning:\n\nThe column 8 contains integer data. Partial dependence plots are not supported for integer data: this can lead to implicit rounding with NumPy arrays or even errors with newer pandas versions. Please convert numerical featuresto floating point dtypes ahead of time to avoid problems. This will raise ValueError in scikit-learn 1.9.\n\n\n\n\n\n\n\n\n\n\nPDP are meant to visualise the relationship that has been learned in the training set; hence it is not usually created with the test set. From the plots, we can interpret that when other variables are already in the model, age returns a visible bump in the probability of death. serum_creatinine is associated with a similar increase, but at low levels. The final plot, at the lower right, darker reds indicate higher probability of death.\n\n\n\nRandom Forest\nA random forest model is an example of an ensemble model. It aims to fix the weakness of decision trees by introducing a little noise in the fitting process. A random forest is in fact a collection of decision trees, with some added modifications. First of all, each tree in a random forest is fit using a bootstrapped version of the training data. Second, at each split, not all features are considered - only a sample of all available features is considered. When a classification prediction is eventually made, it is made by averaging the predictions from all the individual trees. Through the introduction of these perturbations, the eventual model has lower variance (less susceptible to changes in the data), thus generalising to new data better.\nClearly an important property of the random forest is the number of trees to be fitted. This is known as a hyperparameter. More trees leads to a more complex model, which could overfit to the data. scikit-learn provides convenient grid search utilities for identifying the optimal number of trees through. This process, known as hyperparameter tuning, uses cross-validation on the training dataset.\nCross-validation begins with a split of the training data into \\(k\\) blocks (typically \\(k=5\\)). Then, for each block \\(i\\),\n\nSet aside block \\(i\\), and use the remaining data to fit the model.\nUse the fitted model to predict on block \\(i\\), thus obtaining one estimate of generalised error.\n\nThe process yields \\(k\\) estimates of error - one for each block that was set aside. This can be used to compare between hyperparamter settings. In this case below, we try to identify the optimal max_depth parameter for the trees. Too large a depth would overfit to the data, and we wish to avoid that.\n\np_range = range(1, 11, 1)\n#list(p_range)\ncv_search = GridSearchCV(RandomForestClassifier(n_estimators=20, random_state=21), \n                         return_train_score=True,\n                         param_grid ={'max_depth': p_range},\n                         scoring = 'accuracy', cv= 5, verbose=1)\ncv_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n\n\nGridSearchCV(cv=5,\n             estimator=RandomForestClassifier(n_estimators=20, random_state=21),\n             param_grid={'max_depth': range(1, 11)}, return_train_score=True,\n             scoring='accuracy', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nRandomForestC...ndom_state=21)\n\n\n\nparam_grid \n{'max_depth': range(1, 11)}\n\n\n\nscoring \n'accuracy'\n\n\n\nn_jobs \nNone\n\n\n\nrefit \nTrue\n\n\n\ncv \n5\n\n\n\nverbose \n1\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nerror_score \nnan\n\n\n\nreturn_train_score \nTrue\n\n\n\n\n            \n        \n    best_estimator_: RandomForestClassifierRandomForestClassifier(max_depth=4, n_estimators=20, random_state=21)RandomForestClassifier?Documentation for RandomForestClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n20\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n4\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n21\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nAfter calling the .fit() method, the optimal parameters are available for persual:\n\ncv_search.best_estimator_\n\nRandomForestClassifier(max_depth=4, n_estimators=20, random_state=21)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n20\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n4\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n21\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nIt is also possible, at this point, to extract the training and test scores for each cross-validation set, and to then plot the validation curve.\n\ntrain_means = cv_search.cv_results_['mean_train_score']\ntrain_sd = cv_search.cv_results_['std_train_score']\n\ntest_means = cv_search.cv_results_['mean_test_score']\ntest_sd = cv_search.cv_results_['std_test_score']\nplt.plot(p_range, train_means, 'o-', label='Training', color='blue')\nplt.fill_between(p_range, train_means-train_sd, train_means+train_sd, \n                 color='blue', alpha=0.2)\n\nplt.plot(p_range, test_means, 'o-', label='CV (Test)', color='red')\nplt.fill_between(p_range, test_means-test_sd, test_means+test_sd, \n                 color='red', alpha=0.2)\n\nplt.legend(loc='lower right');plt.ylabel('Accuracy');\nplt.xlabel('Complexity');plt.title('Validation Curve');\n\n\n\n\n\n\n\n\nFrom the above chart, we can see that as the max. depth increases, both the training and test errors increase sharply, and then plateau. It even appears that the test error appears to come down. As we discussed, it is normal for the test error to be lower than the training set. Observe that with too high a complexity, accuracy in the training set approaches 1. From the curve, a good choice for the max_depth parameter is 3:\n\nThe test error for this value is close to the training error.\nAny max_depth larger than this would lead to overfitting (fitting the noise patterns in the data to the model).\nThe convention is to take the smallest complexity that is not more than 1 standard error away from the best performing model.\n\n\nExample 8.3 (Example: Heart Failure Classification Scores II) \n\nrf = RandomForestClassifier(n_estimators=20, max_depth=3, random_state=40)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(max_depth=3, n_estimators=20, random_state=40)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n20\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n3\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n40\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred_train = rf.predict(X_train)\nprint(f\"\"\"##: For training set:\n----\nThe precision (for cat. 1) is {precision_score(y_train, y_pred_train):.3f}\nThe recall (for cat. 1) is {recall_score(y_train, y_pred_train):.3f}\nThe accuracy (for cat. 1) is {accuracy_score(y_train, y_pred_train):.3f}\nThe f1-score (for cat. 1) is {f1_score(y_train, y_pred_train):.3f}\n\"\"\")\n\n##: For training set:\n----\nThe precision (for cat. 1) is 0.950\nThe recall (for cat. 1) is 0.792\nThe accuracy (for cat. 1) is 0.920\nThe f1-score (for cat. 1) is 0.864\n\n\n\n\ny_pred_test = rf.predict(X_test)\nprint(f\"\"\"##: For test set:\n----\")\nThe precision (for cat. 1) is {precision_score(y_test, y_pred_test):.3f}\nThe recall (for cat. 1) is {recall_score(y_test, y_pred_test):.3f}\nThe accuracy (for cat. 1) is {accuracy_score(y_test, y_pred_test):.3f}\nThe f1-score (for cat. 1) is {f1_score(y_test, y_pred_test):.3f}\n\"\"\")\n\n##: For test set:\n----\")\nThe precision (for cat. 1) is 0.762\nThe recall (for cat. 1) is 0.667\nThe accuracy (for cat. 1) is 0.827\nThe f1-score (for cat. 1) is 0.711\n\n\n\nCompared with the earlier single decision tree, we have obtained an improved test accuracy (from 0.760 to 0.827).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#regression",
    "href": "08-supervised.html#regression",
    "title": "8  Supervised Learning",
    "section": "8.7 Regression",
    "text": "8.7 Regression\nJust to demonstrate the use of random forests for regression, we return to the Taiwan Data. In our scoring, instead of using RMSE or MAE, we shall use the \\(R^2\\).\nFirst, we read the Taiwan data into Python and transform each column to have mean 0 and sd 1.\n\nre2 = pd.read_csv(\"data/taiwan_dataset.csv\")\n\nX_re = re2.loc[:, ['trans_date', 'house_age', 'dist_MRT', \n                   'num_stores', 'Xs', 'Ys']]\nre_scaler = StandardScaler().fit(X_re)\nX_re_scaled = re_scaler.transform(X_re)\ny_re = re2.price\n\nXre_train,Xre_test, yre_train,yre_test = train_test_split(X_re_scaled, \n                                                          y_re, test_size=0.2, \n                                                          random_state=41)\n\n\nRandom Forest Regressor\nNext, we set up a grid search to set up a random forest regressor.\n\np_range = range(1, 11, 1)\nrf_search = GridSearchCV(RandomForestRegressor(n_estimators=10, random_state=43), \n                         {'max_depth': p_range}, \n                         scoring='r2', cv=5, verbose=1, return_train_score=True)\nrf_search.fit(Xre_train, yre_train,)\nrf_search.best_estimator_\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n\n\nRandomForestRegressor(max_depth=5, n_estimators=10, random_state=43)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n10\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n5\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n43\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nHere is the validation curve, that will allow us to decide on the optimal number of trees.\n\ntrain_means = rf_search.cv_results_['mean_train_score']\ntrain_sd = rf_search.cv_results_['std_train_score']\n\ntest_means = rf_search.cv_results_['mean_test_score']\ntest_sd = rf_search.cv_results_['std_test_score']\n\nplt.plot(p_range, train_means, 'o-', label='Training', color='blue')\nplt.fill_between(p_range, train_means-train_sd, train_means+train_sd, \n                 color='blue', alpha=0.2)\n\nplt.plot(p_range, test_means, 'o-', label='CV (Test)', color='red')\nplt.fill_between(p_range, test_means-test_sd, test_means+test_sd, \n                 color='red', alpha=0.2)\n\nplt.legend(loc='lower right');plt.ylabel('R2');\nplt.xlabel('Complexity');plt.title('Validation Curve');\n\n\n\n\n\n\n\n\n\nExample 8.4 (Example: Taiwan Data Regression) \nWe shall fit a model with 10 trees, and max_depth 2, and assess the test error.\n\nrf1 = RandomForestRegressor(n_estimators=10, max_depth = 2, random_state=89)\nrf1.fit(Xre_train, yre_train)\n\nRandomForestRegressor(max_depth=2, n_estimators=10, random_state=89)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n10\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n2\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n89\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\nyrf_pred = rf1.predict(Xre_test)\n\n\nr2_score(yre_test, yrf_pred)\n\n0.5487486857968205",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#interpretability-of-models",
    "href": "08-supervised.html#interpretability-of-models",
    "title": "8  Supervised Learning",
    "section": "8.8 Interpretability of Models",
    "text": "8.8 Interpretability of Models\n\nLIME\nWhile the earlier methods indicated how we can understand the importance of features to the overall model, we have better tools to understand how features play a role in particular predictions. One such approach is known as LIME (Local Interpretable Model-Agnostic Explanations). A local model is one that explains why the model is making a particular prediction for a particular combination of feature values.\nIt works by training a new model that is inherently explainable, like a decision tree or a linear regression model, using instances that are weighted according to their proximity to the instance of interest. The new model’s prediction for this instance should be as close as possible to the prediction using the original model. To be precise, the steps are:\n\nSelect your instance of interest for which you want to have an explanation of its black box prediction.\nPerturb your dataset and get the black box predictions for these new points.\nWeight the new samples according to their proximity to the instance of interest.\nTrain a weighted, interpretable model on the dataset with the variations.\n\nThe following diagram visualises the process when making predictions (light-blue vs. gray) using two features (\\(x_1\\) and \\(x_2\\)). The instance of interest is the yellow point.\n\n\n\nLIME\n\n\n\nExample 8.5 (Example: Taiwan Data LIME) \nNow we return to our dataset on real estate transations and attempt to understand the prediction made for a particular instance:\n\nre_scaler.inverse_transform(Xre_test[4, :].reshape(1, -1)).round()\n#X_re.mean(axis=0).round(3)\n#Xre_test[4, :].round(4)\n\narray([[2013.,   12., 1144.,    4.,    0.,    3.]])\n\n\nFirst we instantiate an explainer object, and then provide it with the instance we wish to explain.\n\nexplainer = lime_tabular.LimeTabularExplainer(\n    training_data=np.array(Xre_train),\n    feature_names=X_re.columns,\n    mode='regression',\n)\n\n\nexp = explainer.explain_instance(\n    data_row=Xre_test[4, :], \n    predict_fn=rf1.predict,\n)\n\n\nexp.as_pyplot_figure();\n\n\n\n\n\n\n\n\n\nexp.show_in_notebook(show_table=True)\n\n\n        \n        \n        \n        \n        \n        \n\n\nHere is how we can interpret the plots above. In terms of importance to the prediction of this particular instance, dist_MRT is the highest. The local model that has been fit has been constrained to make as close a prediction as possible to the original model. However, it may fall short. In this case, the original predicted value was 29.58. However, the value predicted by the local approximation was:\n\nprint(f\"The value predicted by the local approximation was: {exp.local_pred[0]:.3f}\")\n\nThe value predicted by the local approximation was: 33.209\n\n\nThe numbers in the bar chart essentially decompose this prediction from the local model:\n\ns = 0.0\nfor x,y in exp.local_exp[1]:\n    s += y\n\ns + exp.intercept[0]\n\n33.20919695063819\n\n\nThis means that, from the values, we can see that the dist_MRT, which was close to the average distance, had a negative impact on the price. The longitude had a positive impact on the price.\n\n\n\nICE plots\nICE plots are conditional versions of PDP. Instead of averaging over all instances in the training set, a single line is generated for each instance corresponding to features other than the one under study. This allows us to inspect if the relationship between the response and the feature is consistent across the data.\n\n_, ax = plt.subplots(ncols=2, nrows=1, figsize=(12, 4), constrained_layout=True)\nfeatures_info = {\n    \"features\": [1,2], # no names in the array; 1 and 2 correspond to house_age and dist_MRT\n    \"kind\": \"both\",\n}\ndisplay = PartialDependenceDisplay.from_estimator(\n    rf1,\n    Xre_train,\n    **features_info,\n    ax=ax\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#summary",
    "href": "08-supervised.html#summary",
    "title": "8  Supervised Learning",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nWe have only touched on a random forests and decision trees. However, there are numerous other (non-deep learning) machine learning models. Examples are Support Vector Machines, Nearest Neighbours, and Linear Models. It would be good to skim through these models in the sklearn documentation to be aware of their existence. Instead of concentrating on learning a variety of shallow learning models, our topic has focused on the workflow when using these models, and how we should assess them. Nonetheless, it will be good if you can read up on your own on some of these models. The textbook ISL below is very good for learning about these models.\nWhen working on your project, if you intend to perform supervised learning, please ensure that you also interpret the models, and do not leave them as a black box.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "08-supervised.html#references",
    "href": "08-supervised.html#references",
    "title": "8  Supervised Learning",
    "section": "8.10 References",
    "text": "8.10 References\nOne of the best textbook references for this topic is Hastie, Tibshirani, and Friedman (2009). For more information on interpretable machine learning, refer to Molnar (2020). It is very comprehensive, with much more details on LIME and Shaply values.\n\nWebsite and video references\n\nDecision trees, clearly explained: This is from a popular YouTube channel that explains stats and data science concepts.\nsklearn documentation: Documentation on available supervised learning models in sklearn.\n\n\n\nDocumentation references\n\nInterpretable Machine Learning: A comprehensive textbook on interpreting machine learning models. See this book for more information about LIME and SHAPLY values.\nLIME: Full documentation on LIME\n\nVideo introduction to LIME\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. “An Introduction to Statistical Learning.”\n\n\nMolnar, Christoph. 2020. Interpretable Machine Learning. Lulu. com.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "09-vision.html",
    "href": "09-vision.html",
    "title": "9  Computer Vision",
    "section": "",
    "text": "9.1 Introduction\nData science has a number of applications in computer vision. These have increased in number and accuray in the past 5 years or so due to the advancements in deep learning (both theory and computational feasibility). In this topic, we shall experiment with some of the applications, utilising existing deep learning models.\nHere is a short list of applications of computer vision techniques:\nHere is an old page with a more comprehensive list of applications.\nIn computer vision, the goal is to get a computer to perceive the world as we see it. This is not easy even for us to do - we can get fooled by optical illusions such as the ones below.\nIn general though, for us, it is possible to do things like pick out faces that we recognise from a photograph of a crowd. But how can we get a computer to do it?\nimport cv2\nimport sys\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import YouTubeVideo, display, HTML, Image, Video\n\nopencv_path = \"opencv/samples/data/\"\nopencvxtra_path = \"opencv_extra/testdata/dnn/\"",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "09-vision.html#introduction",
    "href": "09-vision.html#introduction",
    "title": "9  Computer Vision",
    "section": "",
    "text": "Optical Character Recognition: Reading handwritten documents, car license plate numbers from images.\n\nSurveillance and traffic monitoring: Monitoring cars on a highway, tracking humans in security cameras for suspicious activity.\nMachine inspection: Automatically detecting damage on components manufactured, such as silicon wafers, etc.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "09-vision.html#image-processing",
    "href": "09-vision.html#image-processing",
    "title": "9  Computer Vision",
    "section": "9.2 Image Processing",
    "text": "9.2 Image Processing\n\nReading Images\nThe opencv package contains routines for reading images into Python. Images are typically represented using the RGB colorspace. Each image is a (W x H x 3) numpy array. Each layer corresponds to one of these channels. However, opencv uses the order BGR. When you use plt.imshow to plot an image that has been read in using opencv, take note of this, as it might not appear “correct”.\n\nImage(\"data/starry_night.jpg\", width=240)\n\nfrom matplotlib import colormaps\n#list(colormaps)\n\n\nstarry_night = cv2.imread('data/starry_night.jpg')\n\n# Reversed (not correct)\nplt.imshow(starry_night);\n#plt.imshow(starry_night[:, :, 0], cmap='Blues');\n\n\n\n\n\n\n\n\nThe following code should open up the starry night image in a new window on your computer.\n\n# plotting with cv2.imshow returns the correct colours.\ncv2.namedWindow('dst_rt', cv2.WINDOW_NORMAL)\ncv2.resizeWindow('dst_rt', starry_night.shape[1], starry_night.shape[0])\n\n# hit \"q\" to close window (do not hit the \"X\" button)\ncv2.imshow('dst_rt', starry_night)\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "09-vision.html#working-with-masks",
    "href": "09-vision.html#working-with-masks",
    "title": "9  Computer Vision",
    "section": "9.3 Working with Masks",
    "text": "9.3 Working with Masks\nWhen working in a computer vision project, we often need to extract parts of the image to work with, or to modify. This is typically done using masks. Masks are black-and-white images that identify the foreground (white) and the background (black).\nLet us work with the following image of Lionel Messi, and see how we can use masks and colour selection to outline the ball.\n\nImage('data/messi5.jpg')\n\n\n\n\n\n\n\n\nOur first task is to read the data into Python, and use a colour picker to identify the colour of the ball. The following website provides a useful tool for us to upload an image, and isolate the HSV (Hue-Saturation-Value) representaion of the colour we wish to pick out:\nhttps://redketchup.io/color-picker\nUsing this website, we learn that the RGB representation we want is RGB=(244,251,95). We the convert this to HSV using a colour conversion function.\n\nmessi = cv2.imread('data/messi5.jpg')\nhsv = cv2.cvtColor(messi, cv2.COLOR_BGR2HSV)\n\n\n# 244, 251, 95\ncv2.cvtColor(np.uint8([[[95, 251, 244 ]]]), cv2.COLOR_BGR2HSV)\n\narray([[[ 31, 158, 251]]], dtype=uint8)\n\n\n\n# define range of yellow color in HSV \nlower_yellow = np.array([21,  100, 100])\nupper_yellow = np.array([41, 255, 255])\n\n\n# Threshold the HSV image to get only blue colors\nmask1 = cv2.inRange(hsv, lower_yellow, upper_yellow)\n\nCompare the black-and-white image below with the original one. The white regions below correspond to yellow coloured sections in the original image. The white pixels take the value 255, while the black pixels are 0. We are going to modify the numpy array corresponding to this mask to correspond to only the ball.\nTake note of where the origin is: at the top-left corner. The x-values increase to the right from there; y-values increase downward from there.\n\nplt.imshow(mask1, cmap='gray');\n\n\n\n\n\n\n\n\n\nmask1[:, :330] = 0\nmask1[:, 405:] = 0\n\nThe next section of code identifies contours around the ball region, computes the convex hull around these points and then draws this on the original image.\n\n#im2, contours= cv2.findContours(mask1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\ncontours, _ = cv2.findContours(mask1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\nlargest_contour = max(contours, key=cv2.contourArea)\nhull = cv2.convexHull(largest_contour)\ncv2.drawContours(messi, [hull], -1, (0, 0, 255), thickness=2)\n\nplt.imshow(cv2.cvtColor(messi, cv2.COLOR_BGR2RGB));",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "09-vision.html#modifying-perspective-of-images",
    "href": "09-vision.html#modifying-perspective-of-images",
    "title": "9  Computer Vision",
    "section": "9.4 Modifying Perspective of Images",
    "text": "9.4 Modifying Perspective of Images\nThere are situations where we need to transform the perspective of an image, in order to identify objects better, or to perform OCR better. To do so, we need to provide a map of four points from the original image (in the same plane), and the corresponding four points in the transformed image. Here are a couple of examples.\n\nExample 9.1 (Example 1: Sudoku) \nHere is an example of transforming the perspective.\n\nsudoku = cv2.imread('data/sudoku.png')\nrows,cols,ch = sudoku.shape\n\npts1 = np.float32([[73,85], [350, 88], [355, 360], [48, 357]])\n\npts = np.array(pts1, np.int32)\nimg2 = cv2.polylines(sudoku, [pts], True, (255, 0, 0), 2 )\nplt.imshow(img2);\n\n\n\n\n\n\n\n\n\npts2 = np.float32([[0,0],[300,0],[300,300],[0,300]])\nM = cv2.getPerspectiveTransform(pts1,pts2)\ndst = cv2.warpPerspective(sudoku ,M,(300,300))\n \nplt.subplot(121),plt.imshow(sudoku),plt.title('Input')\nplt.subplot(122),plt.imshow(dst),plt.title('Output');\n\n\n\n\n\n\n\n\n\n\nExample 9.2 (Example 2: Football) \n\nImage('data/football1.png')\n\n\n\n\n\n\n\n\n\nfootball1 = cv2.imread('data/football1.png')\npts1 = np.float32([[45,121],[48,238],[206,230], [155,118]])\npts2 = np.float32([[45,100], [45,220], [77,220], [77,100]])\n\nM = cv2.getPerspectiveTransform(pts1,pts2)\ndst = cv2.warpPerspective(football1, M, (504, 360))\n\npts = np.array([[45,121],[48,238],[206,230], [155,119]], np.int32)\npts = pts.reshape((-1,1,2))\nimg2 = cv2.polylines(football1, [pts], True, (255, 0, 0), 1 )\n\n\nfig = plt.figure(figsize=(10, 5)) \ngs = fig.add_gridspec(1, 2, width_ratios=[2, 1]) \n\n# First subplot\nax1 = fig.add_subplot(gs[0])\nax1.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\nax1.set_title('Original Screen capture')\n\n# Second subplot\nax2 = fig.add_subplot(gs[1])\nax2.imshow(cv2.cvtColor(dst[:300, :200], cv2.COLOR_BGR2RGB))\nax2.set_title('Transformed Image');",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "09-vision.html#computer-vision-tasks",
    "href": "09-vision.html#computer-vision-tasks",
    "title": "9  Computer Vision",
    "section": "9.5 Computer Vision Tasks",
    "text": "9.5 Computer Vision Tasks\nJust as we observed there are numerous NLP tasks, the field of computer vision has made great strides in several tasks. Among them are:\n\nObject detection\nObject classification\nObject tracking\nFace detection\nPose estimation\nQR/Bar code detection\nText detection/extraction\n\nAlmost all the models that perform well in the above tasks are deep learning models. In the next few sections, we are going to practice running/configuring some of the above models. Although these models are already trained, it is a little tricky to find the resources and parameters to get them up and running.\nThere are three repositories of interest:\n\nopencv: This contains example images and videos, and configuration files for several models.\nopencv_extra: This contains more configuration details for the models, along with routines to download model weights.\nopencv_zoo: This contains a smaller range of models, along with the model weights.\n\nFor our course, we have pre-downloaded some models for us to play around with, models. However, the general approach to start working on one of these models is as follows:\n\nDownload the model weights using opencv_extra/testdata/dnn/download_models.py\nFigure out what other configuration files/parameters are needed for this model. Sometimes information is in opencv/samples/dnn/models.yml. At other times, you may need to figure it out from the github repository for the particular model.\nTest it out using one of the sample scripts from opencv/samples/dnn/*.py\nOnce you have that working, inspect the script to obtain details on how to call the model programmatically, and proceed from there.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "09-vision.html#references",
    "href": "09-vision.html#references",
    "title": "9  Computer Vision",
    "section": "9.6 References",
    "text": "9.6 References\n\nOpencv documentation\n\nPython tutorials\nChanging colour spaces\nBackground subtraction\nOpencv bootcamp: This is a very useful course on opencv techniques. It will also provide you several more notebooks with template code for object tracking, etc.\n\n\n\nBooks\nA very comprehensive book on vision techniques is by Szeliski (2022). An online version can be found here: Computer Vision: Applications and Algorithms\n\n\nGithub repositories\n\nopencv\nopencv-extra\nopencv model zoo\n\n\n\n\n\nSzeliski, Richard. 2022. Computer Vision: Algorithms and Applications. Springer Nature.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Computer Vision</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Academic References",
    "section": "",
    "text": "Agresti, Alan. 2012. Categorical Data Analysis. Vol. 792. John\nWiley & Sons.\n\n\nAssimakopoulos, Vassilis, and Konstantinos Nikolopoulos. 2000.\n“The Theta Model: A Decomposition Approach to Forecasting.”\nInternational Journal of Forecasting 16 (4): 521–30.\n\n\nClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D Manning.\n2019. “What Does Bert Look at? An Analysis of Bert’s\nAttention.” arXiv Preprint arXiv:1906.04341.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“Bert: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding.” In Proceedings of the 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 4171–86.\n\n\nDragulescu, Adrian A, and Victor M Yakovenko. 2002. “Statistical\nMechanics of Money, Income, and Wealth: A Short Survey.”\narXiv Preprint Cond-Mat/0211175.\n\n\nDraper, NR. 1998. Applied Regression Analysis. McGraw-Hill.\nInc.\n\n\nHan, Xiaochuang, Byron C Wallace, and Yulia Tsvetkov. 2020.\n“Explaining Black Box Predictions and Unveiling Data Artifacts\nThrough Influence Functions.” arXiv Preprint\narXiv:2005.06676.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. “An\nIntroduction to Statistical Learning.”\n\n\nHyndman, Rob J, and George Athanasopoulos. 2018. Forecasting:\nPrinciples and Practice. OTexts.\n\n\nJurafsky, Daniel, and James H. Martin. 2025. Speech and Language\nProcessing: An Introduction to Natural Language Processing,\nComputational Linguistics, and Speech Recognition, with Language\nModels. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.\n2013. “Distributed Representations of Words and Phrases and Their\nCompositionality.” Advances in Neural Information Processing\nSystems 26.\n\n\nMolnar, Christoph. 2020. Interpretable Machine Learning. Lulu.\ncom.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014.\n“Glove: Global Vectors for Word Representation.” In\nProceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 1532–43.\n\n\nSun, Xiaofei, Diyi Yang, Xiaoya Li, Tianwei Zhang, Yuxian Meng, Han Qiu,\nGuoyin Wang, Eduard Hovy, and Jiwei Li. 2021. “Interpreting Deep\nLearning Models in Natural Language Processing: A Review.”\narXiv Preprint arXiv:2110.10470.\n\n\nSzeliski, Richard. 2022. Computer Vision: Algorithms and\nApplications. Springer Nature.",
    "crumbs": [
      "Academic References"
    ]
  }
]